\documentclass[preprint,12pt]{elsarticle}
    \usepackage{algorithm}
    \usepackage{algorithmic}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \usepackage{amssymb}
    \usepackage{amsmath}
    \usepackage[hidelinks]{hyperref}
    \usepackage[capitalize]{cleveref}
    \usepackage{xspace} 
    \usepackage{ifthen} 
    \usepackage{csvsimple}
    \setlength {\marginparwidth }{2cm}
    \usepackage{todonotes}
    \usepackage{float}
    \newcommand*{\M}[1]{\ensuremath{#1}\xspace} 
    \newcommand*{\tr}[1]{\M{#1}}
    \newcommand*{\x}{\times}
    \newcommand*{\mi}[1]{\mathbf{#1}} 
    \newcommand*{\st}[1]{\mathbb{#1}} 
    \newcommand*{\rv}[1]{\mathsf{#1}} 
    \newcommand*{\te}[2][]{\left\lbrack{#2}\right\rbrack_{#1}}
    \newcommand*{\tte}[2][]{\lbrack{#2}\rbrack_{#1}}
    \newcommand*{\tse}[2][]{\mi{\lbrack#2\rbrack}_{#1}}
    \newcommand*{\tme}[3][]{\lbrack{#3}\rbrack_{\tse[#1]{#2}}}
    \newcommand*{\diag}[2][]{\left\langle{#2}\right\rangle_{#1}}
    \newcommand*{\prob}[3]{\M{\mathrm{p}\!\left(\left.{#1}\right\vert{#2,#3}\right)}} 
    \newcommand*{\deq}{\M{\mathrel{\mathop:}=}} 
    \newcommand*{\deqr}{\M{=\mathrel{\mathop:}}} 
    \newcommand{\T}[1]{\text{#1}} 
    \newcommand*{\QT}[2][]{\M{\quad\T{#2}\ifthenelse{\equal{#1}{}}{\quad}{#1}}} 
    \newcommand*{\ev}[3][]{\mathbb{E}_{#3}^{#1}\!\left\lbrack{#2}\right\rbrack}
    \newcommand*{\evt}[3][]{\mathbb{E}_{#3}^{#1}\!#2}
    \newcommand*{\cov}[3][]{\ifthenelse{\equal{#1}{}}{\mathbb{V}_{#3}\!\left\lbrack{#2}\right\rbrack}{\mathbb{V}_{#3}\!\left\lbrack{#2,#1}\right\rbrack}}
    \newcommand*{\covt}[2]{\mathbb{V}_{#2}\!{#1}}
    \newcommand*{\gauss}[2]{\mathsf{N}\!\left({#1,#2}\right)}
    \newcommand*{\uni}[2]{\mathsf{U}\!\left({#1,#2}\right)}
    \newcommand*{\tgauss}[2]{\mathsf{N}({#1,#2})}
    \newcommand*{\gaussd}[2]{\mathsf{N}^{\dagger}\!\left({#1,#2}\right)}
    \newcommand*{\modulus}[1]{\M{\left\lvert{#1}\right\rvert}} 
    \newcommand*{\norm}[1]{\M{\left\lVert{#1}\right\rVert}} 
    \newcommand*{\ceil}[1]{\M{\left\lceil{#1}\right\rceil}} 
    \newcommand*{\set}[1]{\M{\left\lbrace{#1}\right\rbrace}} 
    \newcommand*{\setbuilder}[2]{\M{\left\lbrace{#1}\: \big\vert \:{#2}\right\rbrace}}
    \newcommand*{\uniti}{\lbrack 0,1\rbrack}
    \DeclareMathOperator*{\argmax}{argmax}
    \DeclareMathOperator*{\argmin}{argmin}
    \DeclareMathOperator*{\trace}{tr\!}

\journal{Reliability Engineering and System Safety}

\begin{document}
\begin{frontmatter}

    \title{The Coefficient of Determination of a Reduced Order Model}

    \author{Robert A. Milton}
    \ead{r.a.milton@sheffield.ac.uk}

    \author{Solomon F. Brown}
    \ead{s.f.brown@sheffield.ac.uk}

    \author{Aaron S. Yeardley}
    \ead{asyeardley1@sheffield.ac.uk}

    \address{Department of Chemical and Biological Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom}       

    \begin{abstract}
        %% Text of abstract
    \end{abstract}

    \begin{keyword}
        Global Sensitivity Analysis, Sobol' Index, Surrogate Model, Active Subspace
    \end{keyword}

\end{frontmatter}

\section{Introduction}\label{sec:Intro}


\section{Coefficient of Determination}\label{sec:COD}
    Given a regression model
    \begin{equation*}
        \T{Integrable } y \colon \uniti^{M+1} \mapsto \st{R}^{L}
    \end{equation*}
    begin by taking a uniformly distributed random variable (RV)
    \begin{equation*}
        \rv{u} \sim \uni{\te[\mi{M+1}]{0}}{\te[\mi{M+1}]{1}} \deq \uni{0}{1}^{M+1}
    \end{equation*}
    Exponentiation is categorical -- repeated cartesian $\x$ or tensor $\otimes$ -- unless otherwise specified. Square bracketed quantities are tensors, carrying their dimensions as a von Neumann ordinal subscript, in this case
    \begin{equation*}
        \mi{M+1} \deq (0,\ldots,M) \supseteq \mi{m+1} \deq (0,\ldots,m \leq M)
    \end{equation*}
    with void $\mi{0}$ voiding any tensor it subscripts. Ordinals are concatenated by Cartesian $\times$ and may be subtracted like sets, as in $\mi{M-m} \deq (m,\ldots,M-1)$. 
    Subscripts refer to the tensor prior to any superscript operation, so $\te[\mi{L}]{y(\rv{u})}^{2}$ is an $\mi{L}^{2} \deq \mi{L\x L}$ tensor, for example.
    The preference throughout this work is for uppercase constants and lowercase variables, in case of ordinals the lowercase ranging over the uppercase. We prefer $o$ for an unbounded positive integer, avoiding O.

    Expectations and variances will be subscripted by the dimensions of $\rv{u}$ marginalized. Conditioning on the remaining dimensions is left implicit after \cref{def:Theory:y_m}, to lighten notation.
    Now, Construct $M+1$ stochastic processes (SPs)
    \begin{equation}\label{def:Theory:y_m}
        \te[\mi{L}]{\rv{y_m}} \deq \ev{y(\rv{u})}{\mi{M-m}} \deq \ev{y(\rv{u}) \big\vert \te[\mi{m}]{u}}{\mi{M-m}}
    \end{equation}
    ranging from $\tte[\mi{L}]{\rv{y_0}}$ to $\tte[\mi{L}]{\rv{y_M}}$. Every SP quietly depends on the ungovernable noise dimension $\tte[M]{\rv{u}} \perp \tte[\mi{M}]{\rv{u}}$, while ignoring governed input dimensions $\tte[\mi{M-m}]{\rv{u}}$. 
    Sans serif symbols such as $\rv{u,y}$ generally refer to RVs and SPs, italic $u,y$ being reserved for (tensor) functions and variables.
    
    Following Daniell-Kolmogorov \cite{Rogers.Williams2000} pp.124 we may regard an SP as a random function, from which we shall freely extract finite dimensional distributions generated by a design matrix $\tte[\mi{M\x o}]{u}$ of $o \in \st{Z}^{+}$ samples.
    Daniell-Kolmogorov incidentally secures $\rv{u}$. 
    Because $y$ is (Lebesgue) integrable it must be measurable, guaranteeing $\tte[\mi{L}]{\rv{y_0}}$.
    Because all probability measures are finite, integrability of $y$ implies integrability of $y^n$ for all $n \in \st{Z}^{+}$ \cite{Villani1985}. 
    So Fubini's Theorem \cite{Williams1991} pp.77 allows all expectations to be taken in any order. 
    This is sufficient to ensure every object appearing in this work.

    Our aim is to compare predictions from a reduced model $\rv{y_m}$ with those of the full model $\rv{y_M}$. Correlation between these predictions is squared -- using element-wise (Hadamard) multiplication $\circ$ and division -- to form an RV called the coefficient of determination
    \begin{equation}
        \te[\mi{L}^2]{\rv{R_{m}^{2}}} \deq 
        \frac{\cov[\rv{y_M}]{\rv{y_m}}{\mi{M}} \circ \cov[\rv{y_M}]{\rv{y_m}}{\mi{M}}}
        {\cov{\rv{y_m}}{\mi{m}} \circ \cov{\rv{y_M}}{\mi{M}}} =
        \frac{\cov{\rv{y_m}}{\mi{m}}}{\cov{\rv{y_M}}{\mi{M}}} \deqr
        \te[\mi{L}^2]{\rv{S_m}}
    \end{equation}
    The closed Sobol' index is the complement of the commonplace total Sobol' index
    \begin{equation*}
        \te[\mi{L}^2]{\rv{S_m}} \deqr \te[\mi{L}^2]{1} - \te[\mi{L}^2]{\rv{S^{T}_{M-m}}}
    \end{equation*}
    It has mean value over the ungovernable noise dimension of
    \begin{align}\label{def:COD:mean}
        \te[\mi{L}^2]{S_{\mi{m}}} &\deq \evt{\te[]{\rv{S_m}}}{M} = \frac{V_{\mi{m}}}{V_{\mi{M}}} \\            
        \T{where }\te[\mi{L}^2]{V_{\mi{m}}} &\deq \evt{\; \cov{\rv{y_m}}{\mi{m}}}{M} \ \ \forall \mi{m}\subseteq \mi{M}
    \end{align}
    and variance due to ungovernable noise of
    \begin{align}\label{def:COD:variance}
        \te[\mi{L}^4]{T_\mi{m}} &\deq 
        \cov{\rv{S_m}}{M} = \frac{V_{\mi{m}}^{2}}{V_{\mi{M}}^{2}} \circ
        \left(
            \frac{W_{\mi{mm}}}{V_{\mi{m}}^{2}}
            -2\frac{W_{\mi{mM}}}{{V_{\mi{m}}\otimes V_{\mi{M}}}}
            +\frac{W_{\mi{MM}}}{V_{\mi{M}}^{2}}
        \right) \\                
        \T{where }\te[\mi{L}^4]{W_{\mi{mm^{\prime}}}} &\deq \cov[\cov{\rv{y_{m^{\prime}}}}{\mi{m^{\prime}}}]{\cov{\rv{y_{m}}}{\mi{m}}}{M} \ \ \forall \mi{m,m^{\prime}} \subseteq \mi{M}
    \end{align}
    The remainder of this paper is devoted to calculating these two quantities -- the coefficient of determination and its variance over noise (measurement error, squared).


\section{Stochastic Process Estimates}\label{sec:SPEst}
    Adopt as design matrix a triad of inputs to condition $\te[\mi{M+1\x 3}]{\rv{u}}$, eliciting the response
    \begin{equation}\label{def:SPEst:y}
        \te[\mi{L\x 3}]{\rv{y}} \deq 
        \evt{\;\evt{\;\ev{y(\tte[\mi{M+1\x 3}]{\rv{u}}) 
            \big\vert \te[]{\te[\mi{0}]{u}, \te[\mi{m^{\prime}}]{u}, \te[\mi{M^{\prime\prime}}]{u}}}{\mi{0^{\prime\prime}}}}
        {\mi{M^{\prime}-m^{\prime}}}}{\mi{M}}
    \end{equation}
    Primes mark independent input axes, otherwise expectations are shared by all three members of the triad. It is not always obvious whether axes are independent or shared by the triad, but this can be mechanically checked against the measure of integration behind an expectation. Repeated expectations over the same axis are rare here, usually indicating that apparent repetitions must be ``primed''. The purpose of the triad is to interrogate its response for moments in respect of ungovernable noise (which is shared)
    \begin{equation}\label{def:SPEst:mu}
            \te[(\mi{L\x 3})^{n}]{\mu_{n}} \deq \ev{\tte[\mi{L\x 3}]{\rv{y}}^{n}}{M} \ \ \forall n \in \st{Z}^{+}
    \end{equation}
    for these embody
    \begin{equation*}
        \te[\mi{L}^{n}]{\mu_{\mi{m^{\prime}\ldots m}^{n\prime}}} \deq \te[(\mi{L\x}i)^{n}]{\mu_{n}} = \ev{\tte[\mi{L}]{\rv{y}_{\mi{m}^{\prime}}}\otimes\cdots\otimes\tte[\mi{L}]{\rv{y}_{\mi{m}^{n\prime}}}}{M}
    \end{equation*}
    where $\mi{m}^{i\prime} \in \set{\mi{0},\mi{m},\mi{M}}$. This expression underpins both quantities we seek. The reduction which follows repeatedly realizes
    \begin{equation}\label{eq:SPEstimates:reduction}
        \te[\mi{L}^{n}]{\mu_{\mi{0\ldots 0}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}} \deq 
        \evt{\te[\mi{L}^{n}]{\mu_{\mi{M\ldots M}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}}}{\mi{M}} = 
        \evt{\te[\mi{L}^{n}]{\mu_{\mi{m\ldots m}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}}}{\mi{m}}
    \end{equation}

    Defining
    \begin{equation}\label{def:SPEst:e}
        \te[\mi{L\x 3}]{\rv{e}} \deq \rv{y} - \mu_{1}
    \end{equation}
    the expected conditional variance in \cref{def:COD:mean} amounts to
    \begin{equation}\label{eq:SPEst:V}
        \begin{aligned}
            \te[\mi{L}]{V_{\mi{m}}} 
            &= \evt{\;\ev{\te[\mi{L}]{\rv{e_m} + \mu_{\mi{m}}}^{2}}{M}}{\mi{m}}
            - \ev{\te[\mi{L}]{\rv{e_0} + \mu_{\mi{0}}}^{2}}{M} \\
            &= \ev{\te[\mi{L}]{\mu_{\mi{m}}}^{2}}{\mi{m}} - \te[\mi{L}]{\mu_{\mi{0}}}^{2} + 
            \evt{\te[\mi{L}^2]{\mu_{\mi{mm}}}}{\mi{m}} - \te[\mi{L}^2]{\mu_{\mi{00}}} \\
            &= \ev{\te[\mi{L}]{\mu_{\mi{m}}}^{2}}{\mi{m}} - \te[\mi{L}]{\mu_{\mi{0}}}^{2}
        \end{aligned}
    \end{equation}

    and the covariance between conditional variances in \cref{def:COD:variance} is
    \begin{equation}\label{eq:SPEst:W}
        \begin{aligned}
            \te[\mi{L}^4]{W_{\mi{mm^{\prime}}}} &\deq \cov[\cov{\rv{y_{m^{\prime}}}}{\mi{m^{\prime}}}]{\cov{\rv{y_{m}}}{\mi{m}}}{M} \\
            &\phantom{:}=
            \cov[\ev{\te[\mi{L}]{\rv{y_{m^{\prime}}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m^{\prime}}}]{\ev{\te[\mi{L}]{\rv{y_{m}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m}}}{M} \\
            &\phantom{:}=
            \ev{\ev{\te[\mi{L}]{\rv{y_{m}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m}} \otimes\ev{\te[\mi{L}]{\rv{y_{m^{\prime}}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m^{\prime}}}}{M}\\
            &\phantom{\deq}\  - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}} \\       
            &\phantom{:}= \te[\mi{L}^4]{A_{\mi{mm^{\prime}}}-A_{\mi{0m^{\prime}}}-A_{\mi{m0}}+A_{\mi{00}}}
        \end{aligned}
    \end{equation}
    Here, for any $\mi{m},\mi{m^{\prime}}\subseteq\mi{M}$
    \begin{equation*}
        \begin{aligned}
            \te[\mi{L}^4]{A_{\mi{mm^{\prime}}}}
            &\deq \evt{\;\evt{\;\ev{\te[\mi{L}]{\rv{y_{m}}}^{2} \otimes \te[\mi{L}]{\rv{y_{m^{\prime}}}}^{2}}{M}}{\mi{m^{\prime}}}}{\mi{m}} - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}} \\
            &\phantom{:}= \evt{\;\evt{\;\ev{\te[\mi{L}]{\rv{e_{m}}+\mu_{\mi{m}}}^{2} \otimes \te[\mi{L}]{\rv{e_{m^{\prime}}}+ \mu_{\mi{m^{\prime}}}}^{2}}{M}}{\mi{m^{\prime}}}}{\mi{m}}
            - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}}
        \end{aligned}
    \end{equation*}
    exploiting the fact that $V_{\mi{0}} = \te[\mi{L}^2]{0}$. \Cref{eq:SPEstimates:reduction} cancels all terms beginning with $\te[\mi{L}]{\rv{e_{m}}}^{2}$, first across $A_{\mi{m,m^{\prime}}}-A_{\mi{0,m^{\prime}}}$ then across $A_{\mi{m,0}}-A_{\mi{0,0}}$. All remaining terms ending in $\te[\mi{L}]{\mu_{\mi{m^{\prime}}}}^{2}$ are eliminated by error centralization $\mu_{1} = 0$ and
    \begin{multline*}
        \evt{\;\ev{\te[\mi{L}]{\mu_{\mi{m}}}^{2} \otimes \te[\mi{L}]{\mu_{\mi{m^{\prime}}}}^{2}}{\mi{m^{\prime}}}}{\mi{m}}
        - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}} \\
        = \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}]{\mu_{\mi{0}}}^{2}
        + \te[\mi{L}]{\mu_{\mi{0}}}^{2}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}}
        + \te[\mi{L}]{\mu_{\mi{0}}}^{4}
    \end{multline*}
    cancelling across $A_{\mi{m,m^{\prime}}}-A_{\mi{0,m^{\prime}}} - A_{\mi{m,0}}+A_{\mi{0,0}}$.
    Similar arguments eliminate $\te[\mi{L}]{\rv{e_{m^{\prime}}}}^{2}$ and $\te[\mi{L}]{\mu_{\mi{m}}}^{2}$.
    Effectively then
    \begin{equation}\label{eq:SPEst:A}
        \begin{aligned}
            \te[\mi{L}^4]{A_{\mi{mm^{\prime}}}} &= \sum_{\pi(\mi{L}^{2})} \sum_{\pi(\mi{L^{\prime}}^{2})}
            \evt{\;\evt{\te[\mi{L}^{2} \x \mi{L^{\prime}}^{2}]{\mu_{\mi{m}} \otimes \mu_{\mi{mm^{\prime}}} \otimes \mu_{\mi{m^{\prime}}}}}{\mi{m^{\prime}}}}{\mi{m}} \\
            &= 4 \;
            \evt{\;\evt{\te[\mi{L}^{4}]{\mu_{\mi{m}} \otimes \mu_{\mi{mm^{\prime}}} \otimes \mu_{\mi{m^{\prime}}}}}{\mi{m^{\prime}}}}{\mi{m}}
        \end{aligned}
    \end{equation}
    where each summation is over permutations of tensor axes
    \begin{equation*}
        \pi(\mi{L}^{2}) \deq \set{(\mi{L}\x\mi{L^{\prime\prime}}), (\mi{L^{\prime\prime}}\x\mi{L})} \QT{;} \pi(\mi{L^{\prime}}^{2}) \deq \set{(\mi{L^{\prime}}\x\mi{L^{\prime\prime\prime}}), (\mi{L^{\prime\prime\prime}}\x\mi{L^{\prime}})}
    \end{equation*}
    Primes on constants are for bookeeping purposes only ($\mi{L}^{i\prime} = \mi{L}$ always), they do not change the value of the constant -- unlike primes on variables ($\mi{m}^{i\prime}$ need not equal $\mi{m}$ in general). In this case the four permutations all give the same result, by symmetry of covariances, hence the last equality

    In order to further elucidate these estimates, we must fill in the details of the underlying stochastic processes, sufficiently identifying the regression $y$ by its first two moments $\mu_{1}, \mu_{2}$.


\section{Interlude: Gaussian Process (GP) Regression} \label{sec:GPR}
    A GP over $x$ is formally defined and specified by
    \begin{equation*}
        \te[\mi{L}]{\rv{y_M}} \big\vert \te[\mi{M}\x\mi{o}]{x} \sim 
        \gaussd{\te[\mi{L}\x\mi{o}]{\bar{y}(x)}}{\te[(\mi{L}\x\mi{o})^{2}]
        {k_{\rv{y}}(x,x)}} \quad \forall o \in \st{Z^{+}}
    \end{equation*}
    where tensor ranks concatenate into a multivariate normal distribution
    \begin{equation*}
        \begin{aligned}
            \te[\mi{L}\x\mi{o}]{} \sim \gaussd{\te[\mi{L}\x\mi{o}]{}}{\te[(\mi{L\x o})^{2}]{}}
            & \Longleftrightarrow
            \te[\mi{L}\x\mi{o}]{}^{\dagger} \sim \gauss{\te[\mi{L}\x\mi{o}]{}^{\dagger}}{\te[(\mi{L\x o})^{2}]{}^{\dagger}} \\
            \te[\mi{lo}-\mi{(l-1)o}]{\te[\mi{L}\x\mi{o}]{}^{\dagger}} 
            &\deq \te[l-1\x\mi{o}]{} \\
            \te[(\mi{lo}-(\mi{l-1})\mi{o}) \x (\mi{l^{\prime}o}-\mi{(l^{\prime}-1)o})]
            {\te[(\mi{L\x o})^{2}]{}^{\dagger}} 
            &\deq \te[l-1\x\mi{o} \x l^{\prime}-1\x\mi{o}]{} \\
        \end{aligned}
    \end{equation*}
    supporting the fundamental definition of the Gaussian process kernel, as a covariance over response space
    \begin{equation*}
        \te[l\x o\x l^{\prime}\x o^{\prime}]{k_{\rv{y}}(x,x)} 
        \deq \cov[{\te[l^{\prime}\x o^{\prime}]{\rv{y_M}\vert x}}]{\te[l\x o]{\rv{y_M}\vert x}}{\mi{Lo}}
    \end{equation*}

    \subsection{Tensor Gaussians} \label{sub:GPR:Tensor}
        A tensor Gaussian like $\prob{\te[\mi{m}\x\mi{o}]{x}}{\te[\mi{m}\x\mi{o^{\prime}}]{x^{\prime}}}
        {\te[\mi{L}^{2}\x\mi{m}^{2}]{\Sigma}}$ is defined element-wise
        \begin{multline} \label{def:Notation:p}
            \te[l\x o \x l^{\prime}\x o^{\prime}]{\prob{\te[\mi{m}\x\mi{o}]{x}}{\te[\mi{m}\x\mi{o^{\prime}}]{x^{\prime}}}
            {\te[\mi{L}^{2}\x\mi{m}^{2}]{\Sigma}}}
            \deq (2 \pi)^{-M/2} \modulus{\te[l\x l^{\prime}]{\Sigma}}^{-1/2} \\
            \exp\left(-\frac{
                \te[\mi{m}\x l\x o\x l^{\prime}\x o^{\prime}]{x-x^{\prime}}^{\intercal} 
            \te[l\x l^{\prime}]{\Sigma}^{-1} 
            \te[\mi{m}\x l\x o\x l^{\prime}\x o^{\prime}]{x-x^{\prime}}}
            {2}\right)
        \end{multline}
        in terms of the matrix
        \begin{equation*}
            \te[l\x l^{\prime}]{\Sigma} \deq
            \te[l\x l^{\prime}\x\mi{m}^{2}]{\Sigma}
        \end{equation*}
        and the transpose $^{\intercal}$ (moving first multi-index to last) of the broadcast difference between two tensors
        \begin{equation*}
            \te[\mi{m}\x l\x o\x l^{\prime}\x o^{\prime}]{x-x^{\prime}} \deq
            \te[\mi{m}\x o]{x}
            - \te[\mi{m}\x o^{\prime}]{x^{\prime}}
        \end{equation*}
        Remarkably, the algebraic development in the remainder of this paper relies almost exclusively on an invaluable product formula reported in \cite{Rasmussen2016}:
        \begin{multline} \label{eq:GPR:product}
            \prob{z}{a}{A} \circ \prob{\Theta^{\intercal}z}{\tr{b}}{\tr{B}}
            = \prob{0}{(b-\Theta^{\intercal}a)}{(B + \Theta^{\intercal}A\Theta)} \\
            \circ \prob{z}
            {(A^{-1}+\Theta B^{-1}\Theta^{\intercal})^{-1}(A^{-1}a+\Theta B^{-1}b)}
            {(A^{-1}+\Theta B^{-1}\Theta^{\intercal})^{-1}}
        \end{multline}
        This formula and the Gaussian tensors behind it will appear in a variety of guises.

    \subsection{Prior GP} \label{sub:GPR:Prior}
        GP regression decomposes output $\te[\mi{L}]{\rv{y_M}}$ into signal GP $\te[\mi{L}]{\rv{f_M}}$, and independent noise GP $\te[\mi{L}]{\rv{e}_M}$ with constant noise covariance $\te[\mi{L}^2]{E}$
        \begin{equation*}
            \begin{aligned}
                \te[\mi{L}]{\rv{y_M}\vert E} 
                &= \te[\mi{L}]{\rv{f_M}} + \te[\mi{L}]{\rv{e}_M\vert E} \\
                \te[\mi{L}]{\rv{e}_M\vert E} \big\vert \te[\mi{M}\x\mi{o}]{x}
                &\sim \gaussd{\te[\mi{L}\x\mi{o}]{0}}{\te[(\mi{L}\x 1)^2]{E} \circ \diag[(1\x\mi{o})^2]{1}}
            \end{aligned}
        \end{equation*}
        The RBF kernel is hyperparametrized by signal covariance $\te[\mi{L}^2]{F}$ and the matrix $\te[\mi{L}\x\mi{M}]{\Lambda}$ of characteristic lengthscales for each output/input combination. 
        Angle brackets denoting a (perhaps broadcast) diagonal tensor, such as the identity matrix $\diag[\mi{m}^2]{1} \deqr \diag[]{\tte[\mi{m}]{1}}$, are used to define
        \begin{equation*}
            \begin{aligned}
                \diag[l\x l^{\prime}\x\mi{M}^{2}]{\Lambda^{2} \pm I} 
                &\deq \diag{\te[l\x\mi{M}]{\Lambda} \circ \te[l^{\prime}\x\mi{M}]{\Lambda} \pm \te[\mi{M}]{I}} 
                \qquad I \in \set{0}\cup\st{Z}^{+} \\
                \diag[l\x l^{\prime}\x\mi{M}^{2}]{\Lambda^{2}} &\deq 
                \diag[l\x l^{\prime}]{\Lambda^{2} \pm 0} \\
                    \te[l\x l^{\prime}]{\pm F} 
                &\deq (2 \pi)^{M/2} \modulus{\diag[l\x l^{\prime}]{\Lambda^{2}}}^{1/2} \te[l\x l^{\prime}]{F}
            \end{aligned}
        \end{equation*}
        and implement the objective RBF prior using \cref{def:Notation:p}
        \begin{equation*}
            \te[\mi{L}]{\rv{f_M} \vert F,\Lambda}
            \big\vert \te[\mi{M}\x\mi{o}]{x} \sim \\
            \gaussd{\te[\mi{L}\x\mi{o}]{0}}{\te[(\mi{L}\x 1)^{2}]{\pm F} \circ 
            \prob{\te[\mi{M}\x\mi{o}]{x}}{\te[\mi{M}\x\mi{o}]{x}}
            {\diag[\mi{L}^{2}\x\mi{M}^{2}]{\Lambda^{2}}}} 
        \end{equation*}
        
    \subsection{Predictive GP} \label{sub:GPR:Predictive}
        Bayesian inference for GP regression further conditions the hyper-parametrized GP $\rv{y} \vert E,F,\Lambda$ on the observed realization of the random variable $\te{\rv{y}\vert X}$
        \begin{equation*}
            \te[\mi{L} \x \mi{N}]{Y}^{\dagger} \deq \te{\te[\mi{L}]{\rv{y_M}\vert E,F,\Lambda} \big\vert \te[\mi{M}\x\mi{N}]{X}}^{\dagger}\!(\omega) \in \st{R}^{LN}
        \end{equation*}
        To this end we define
        \begin{equation} \label{def:GPR:Kk}
            \begin{aligned}
                \te[\mi{Lo}\x\mi{Lo}]{K_{\rv{e}}} &\deq 
                \cov{\te{\te[\mi{L}]{\rv{e}_{M}\vert E} \big\vert \te[\mi{M}\x\mi{o}]{x}}^{\dagger}}{\mi{Lo}} \\
                &\phantom{:}= \te{\te[(\mi{L}\x 1)^2]{E} \circ \diag[(1\x\mi{o})^2]{1}}^{\dagger} \\
                \te[\mi{Lo}\x\mi{L^{\prime}o^{\prime}}]{k(x, x^{\prime})} &\deq
                \cov[{\te{\te[\mi{L^{\prime}}]{\rv{f_M}\vert F,\Lambda} \big\vert \te[\mi{M}\x\mi{o^{\prime}}]{x^{\prime}}}^{\dagger}}]
                {\te{\te[\mi{L}]{\rv{f_M}\vert F,\Lambda} \big\vert \te[\mi{M}\x\mi{o}]{x}}^{\dagger}}{\mi{Lo}} \\
                &\phantom{:}= \te{\te[\mi{L}^{2}]{\pm F} \circ 
                \prob{\te[\mi{M}\x\mi{o}]{x}}{\te[\mi{M}\x\mi{o^{\prime}}]{x^{\prime}}}
                {\diag[\mi{L}^{2}\x\mi{M}^{2}]{\Lambda^{2}}}}^{\dagger} \\
                %
                \te[\mi{LN}\x\mi{LN}]{K_{Y}} &\deq 
                \cov{\te{\te[\mi{L}]{\rv{y}\vert E,F,\Lambda} \big\vert \te[\mi{M}\x\mi{N}]{X}}^{\dagger}}{\mi{Lo}} \\
                &\phantom{:}= k(\te[\mi{M}\x\mi{N}]{X},\te[\mi{M}\x\mi{N}]{X}) + \te[\mi{LN}\x\mi{LN}]{K_{\rv{e}}})
            \end{aligned}
        \end{equation}
        Applying Bayes' rule
        \begin{equation*}
            \begin{aligned}
                \mathsf{p}(\rv{f_M}\vert Y)\mathsf{p}(Y) = \mathsf{p}(Y\vert \rv{f_M})\mathsf{p}(\rv{f_M})
                &= \prob{Y^{\dagger}}{\rv{f_M}^{\dagger}}{K_{\rv{e}_{M}}} \prob{\rv{f_M}^{\dagger}}{\te[\mi{LN}]{0}}{k(X,X)} \\
                &= \prob{\rv{f_M}^{\dagger}}{Y^{\dagger}}{K_{\rv{e}_{M}}} \prob{\rv{f_M}^{\dagger}}{\te[\mi{LN}]{0}}{k(X,X)}
            \end{aligned}
        \end{equation*}
        \cref{eq:GPR:product} immediately reveals the marginal likelihood
        \begin{equation} \label{eq:GPR:marginalLikelihood}
            \mathsf{p}\!\left(\te{Y \vert E,F,\Lambda} \big\vert X\right)
            = \prob{\te[\mi{L\x N}]{Y}^{\dagger}}{\te[\mi{LN}]{0}}{K_Y}
        \end{equation}
        and the posterior distribution
        \begin{multline*}
            \te[\mi{L\x N}]{\te{\rv{f_M} \vert Y \vert E,F,\Lambda} \big\vert X}^{\dagger} \sim \\
            \gauss{k(X,X) K_{Y}^{-1} Y^{\dagger}}{\ k(X,X) - k(X,X) K_{Y}^{-1} k(X,X)}
        \end{multline*}

        The ultimate goal is the posterior predictive GP which extends the posterior distribution to arbitrary -- usually unobserved -- $\te[\mi{M}\x\mi{o}]{x}$. This is traditionally derived from the definition of conditional probability, but this seems unnecessary, for the extension must recover the posterior distribution when $x=X$. There is only one way of selectively replacing $X$ with $x$ in the posterior formula which preserves the coherence of tensor ranks:
        \begin{multline} \label{def:GPR:Predictive}
            \te[\mi{L\x o}]{\te{\rv{f_M} \vert Y \vert E,F,\Lambda} \big\vert x}^{\dagger} \sim \\
            \gauss{k(x,X) K_{Y}^{-1} Y^{\dagger}}{\ k(x,x) - k(x,X) K_{Y}^{-1} k(X,x)}
        \end{multline}

    \subsection{GP Optimization} \label{sub:GPR:Optimization}
        Henceforth we implicitly condition on optimal hyperparameters, which maximise the marginal likelihood \cref{eq:GPR:marginalLikelihood}.
        \begin{equation} \label{eq:GPR:hyperparameters}
            \te[\mi{L}^{2}]{E},\te[\mi{L}^{2}]{F},\te[\mi{L}\x\mi{M}]{\Lambda} \deq \argmax \prob{\te[\mi{L\x N}]{Y}^{\dagger}}{\te[\mi{LN}]{0}}{K_Y}
        \end{equation}
        The lengthscale tensor could feasibly have been of maximal rank $\te[\mi{L}^{2}\x\mi{M}]{\Lambda}$. We have restricted this to $\te[\mi{L}\x\mi{M}]{\Lambda}$, as one set of ARD lengthscales per output is heuristically satisfying and enables effective optimization as follows.
        For each output $l \in \mi{L}$ construct a separate GP to optimize the diagonal hyperparameters
        \begin{equation*}
            \te[l^{2}]{E},\te[l^{2}]{F},\te[l\x\mi{M}]{\Lambda} = \\
            \argmax \prob{\te[l\x \mi{N}]{Y}^{\dagger}}{\te[\mi{N}]{0}}{\te[\mi{(l+1)N-N}\x\mi{(l+1)N-N}]{K_Y}}
        \end{equation*}
        From this starting point, $E,F$ may be optimized (off-diagonal elements in particular) in the full multi-output GP \cref{eq:GPR:hyperparameters}. One may then attempt to re-optimize lengthscales according to \cref{eq:GPR:hyperparameters}, and iterate, although this may be gilding the lily.


\section{Gaussian Process Moments}\label{sec:GPMom}
    This Section calculates the stochastic process moments of Gaussian Process Regression, absorbing \cref{sec:GPR} into the perspective of \cref{sec:SPEst}
    Let $c\colon \st{R} \to [0,1]$ be the (bijective) CDF of the standard, univariate normal distribution, and define the triads
    \begin{equation*}
        \begin{aligned}
            \te[\mi{M\x 3}]{\rv{z}} &\deq c^{-1}\!\left(\te[\mi{M\x 3}]
            {\rv{u}}\right) \sim \gauss{\te[\mi{M\x 3}]{0}}{\diag[\mi{M}^{2}]{1}} \\
            \te[\mi{M^{\prime}\x 3}]{\rv{x}} &\deq \te[\mi{M\x M^{\prime}}]{\Theta}^{\intercal} \te[\mi{M\x 3}]{\rv{z}}
        \end{aligned}
    \end{equation*}
    Here, the rotation matrix $\te[\mi{M\x M^{\prime}}]{\Theta}^{\intercal} = \te[\mi{M\x M^{\prime}}]{\Theta}^{-1}$ is broadcast to multiply the triad $\tte[\mi{M\x 3}]{\rv{z}}$. 
    The purpose of ths arbitrary rotation is to allow GPs whose input basis $\rv{x}$ is not aligned with the fundamental basis $\rv{u}$ of model reduction and coefficient of determination. The latter is aligned with $\rv{z}$ which is the input we must condition.
    
    Throughout the remainder of this paper, primed ordinal subscripts are used to specify Einstein sum (einsum) contraction of tensors. Whenever a subscript primed in a specific fashion appears in adjacent tensors (those not separated by algebraic operations $+,-,\circ,\,\otimes$), it is einsummed over and does not subscript the result.
    
    Adding shared gaussian noise $\te[\mi{L}]{\rv{e}_M\vert E}$ to \cref{def:GPR:Predictive} yields
    \begin{multline}\label{eq:GPMom:yDist}
        \te[\mi{L\x 3}]{y(\te[\mi{M+1\x 3}]{\rv{u}}) \big\vert \te[\mi{M\x 3}]{u}}^{\dagger} 
        = \te[\mi{L\x 3}]{\te{\rv{y_M} \vert Y \vert E,F,\Lambda} \big\vert \tte[\mi{M\x 3}]{z}}^{\dagger} \sim \\
        \gauss{k(x,X) K_{Y}^{-1} Y^{\dagger}}{\ k(x,x) - k(x,X) K_{Y}^{-1} k(X,x) + E^{\dagger}}
    \end{multline}
    using broadcast $\tte[\mi{L3\x L3}]{E^{\dagger}} \deq \tte[(\mi{L\x 3})^{2}]{\tte[(\mi{L}\x 1)^{2}]{E} \circ \tte[(1\x\mi{3})^{2}]{1}}^{\dagger}$. 
    To bring the GP estimate fully under the umbrella of the SP estimate we should identify its ungovernable noise, and ascribe it to $\tte[M]{\rv{u}}$ of the SP.
    Let $d\colon (0,1) \to (0,1)^{L}$ concatenate every $L^{\mathrm{th}}$ decimal place starting at $l$, for each output dimension $l\leq L$ of $(0,1)^{L}$, then \cref{eq:GPMom:yDist} can be written as
    \begin{multline}\label{eq:GPMom:yReveal}
        \te[\mi{L\x 3}]{y(\te[\mi{M+1\x 3}]{\rv{u}}) \big\vert \te[\mi{M\x 3}]{u}}^{\dagger} \\
        = \te[\mi{L\x 3}]{\mu_{1}}^{\dagger}
        + \te[\mi{L\x 3\x L^{\prime}\x 3^{\prime}}]{\mu_{2}}^{\dagger/2} \te[\mi{L^{\prime}\x 3^{\prime}}]{\te[\mi{L}\x 1]{c^{-1}\!\left(d\left(\te[M]{\rv{u}}\right)\right)} \circ \te[1\x\mi{3}]{1}}^{\dagger}
    \end{multline}
    where $\tte[(\mi{L\x 3})^{2}]{\mu_{2}}^{\dagger/2}$ denotes the Cholesky decomposition of the matrix $\tte[(\mi{L\x 3})^{2}]{\mu_{2}}^{\dagger}$.
    From the development in \cref{sec:SPEst}, the first two moments $\mu_{1},\mu_{2}$ are sufficient to compute the coefficient of determination and its variance. 
    
    The crucial moments $\mu_{1},\mu_{2}$ can be read straight off \cref{eq:GPMom:yDist,eq:GPMom:yReveal}, but still need conditioning. This is entirely a matter of repeatedly applying product formula \cref{eq:GPR:product}, together with the familiar Gaussian identities
    \begin{equation*}
        \begin{aligned}
            \te[\mi{M}]{\rv{z}} \sim \gauss{\te[\mi{M}]{Z}}{\te[\mi{M}\x\mi{M}]{\Sigma}} &\Rightarrow
            \te[\mi{m}]{\rv{z}} \sim \gauss{\te[\mi{m}]{Z}}{\te[\mi{m}\x\mi{m}]{\Sigma}} \\
            \te[\mi{m}]{\rv{z}} \sim \gauss{\te[\mi{m}]{Z}}{\te[\mi{m}\x\mi{m}]{\Sigma}} &\Rightarrow
            \te[\mi{m}\x\mi{m}]{\Theta}^{\intercal}\te[\mi{m}]{\rv{z}} \sim 
            \modulus{\Theta}^{-1}
            \gauss{\Theta^{\intercal}Z}{\Theta^{\intercal}\Sigma\Theta}                        
        \end{aligned}
    \end{equation*}

    \subsection{First Moments} \label{sub:GPMom:First}
        The first moment of the GP for any $\mi{m}\subseteq\mi{M}$ is given by
        \begin{equation*}
            \te[\mi{L}]{\mu_{\mi{m}}}
            = \ev{k\!\left(\te[\mi{M}]{\rv{x}},X\right) K_{Y}^{-1} Y^{\dagger} \big\vert \te[\mi{m}]{z}}{\mi{M-m}}
            = \te[\mi{L}\x\mi{L^{\prime}}\x\mi{N^{\prime}}]{g_{\mi{m}}}^{\dagger}
            \te[\mi{L^{\prime}N^{\prime}}]{K_{Y}^{-1} Y^{\dagger}}
        \end{equation*}
        where
        \begin{equation*}
            \begin{aligned}
                \te[l\x l^{\prime}\x\mi{N^{\prime}}]{g_{\mi{m}}} 
                &\deq \te[l\x l^{\prime}]{\pm F} \circ 
                \prob{\te[\mi{M}]{0}}{\te[\mi{M}\x\mi{N^{\prime}}]{X}}
                {\diag[l\x l^{\prime}]{\Lambda^{2}+1}} \\
                & \phantom{\deq\ } \circ \frac
                {\prob{\te[\mi{m}]{z}}{\te[\mi{m}\x l\x l^{\prime}\x\mi{N^{\prime}}]{G}}{\te[l\x l^{\prime}]{\Gamma}}}
                {\prob{\te[\mi{m}]{z}}{\te[\mi{m}]{0}}{\diag[\mi{m}^{2}]{1}}} \\
                &\phantom{:}= \te[l\x l^{\prime}]{\pm F} \circ 
                \frac
                {\prob{\te[\mi{M}]{0}}{\te[\mi{M}\x\mi{N^{\prime}}]{X}}
                {\diag[l\x l^{\prime}]{\Lambda^{2}+1}}} 
                {\prob{\te[\mi{m}]{0}}{\te[\mi{m}\x l\x l^{\prime}\x\mi{N^{\prime}}]{G}}
                {\te[l\x l^{\prime}]{\Phi}}} \\
                & \phantom{\deq\ } \circ
                {\prob{\te[l\x l^{\prime}\x\mi{m\x m^{\prime}}]{\Phi}\te[\mi{m^{\prime}}]{z}}{\te[\mi{m}\x l\x l^{\prime}\x\mi{N^{\prime}}]{G}}
                {\te[l\x l^{\prime}]{\Gamma}\te[l\x l^{\prime}\x \mi{m^{\prime}\x m}]{\Phi}}}
            \end{aligned}                    
        \end{equation*}
        and
        \begin{equation*}
            \begin{aligned}
                \te[\mi{m}\x l\x l^{\prime}\x\mi{N^{\prime}}]{G} &\deq 
                \te[\mi{m}\x\mi{M}]{\Theta} \diag[l\x l^{\prime}\x\mi{M}\x\mi{M^{\prime}}]{\Lambda^{2}+1}^{-1} \te[\mi{M^{\prime}}\x\mi{N^{\prime}}]{X} \\
                \te[l\x l^{\prime}\x\mi{m}\x\mi{m^{\prime}}]{\Phi} &\deq 
                \te[\mi{m}\x\mi{M}]{\Theta}
                \diag[l\x l^{\prime}\x\mi{M}\x\mi{M^{\prime}}]{\Lambda^{2}+1}^{-1} \te[\mi{m^{\prime}}\x\mi{M^{\prime}}]{\Theta}^{\intercal} \\
                \te[l\x l^{\prime}\x\mi{m}^{2}]{\Gamma} &\deq 
                \diag[\mi{m}^{2}]{1} -
                \te[l\x l^{\prime}\x\mi{m}^{2}]{\Phi}
            \end{aligned}
        \end{equation*}
        In particular, when $\mi{m} = \mi{M}$, $\Theta$ factors out entirely. On the other hand
        \begin{equation*}
                \te[\mi{L}^{2}\x\mi{N^{\prime}}]{g_{\mi{0}}} =
                \te[\mi{L}^{2}]{\pm F} \circ 
                \prob{\te[\mi{M}]{0}}{\te[\mi{M}\x\mi{N^{\prime}}]{X}}
                {\diag[\mi{L}^{2}\x\mi{M}^{2}]{\Lambda^{2}+1}}                  
        \end{equation*}
        Standardization of $X$ and $Y$ instills a totally marginal expectation of $\mu_{\mi{0}} \approx \te[\mi{L}]{0}$, but this is usually inexact.

        \subsection{Second Moments} \label{sub:GPMom:Second}
        The second moment of the GP for any $\mi{m,m^{\prime}}\subseteq\mi{M}$ is given by
            \begin{equation} \label{eq:GPMom:Second}
                \te[\mi{L}^2]{\mu_{\mi{mm^{\prime}}}} = 
                \te[\mi{L}^2]{F} \circ \te[\mi{L}^2]{\phi_{\mi{mm^{\prime}}}} - \te[\mi{L}^2]{\psi_{\mi{mm^{\prime}}}} + \te[\mi{L}^2]{E}                        
            \end{equation}
            where
            \begin{multline*}
                \te[l\x l^{\prime}]{\phi_{\mi{mm^{\prime}}}}
                \deq \frac{\evt{\;\evt{\te[l\x l^{\prime}]{k\!\left(\te[\mi{M}]{\rv{x}},\te[\mi{M^{\prime}}]{\rv{x}}\right) \big\vert \te[\mi{m}]{z},\te[\mi{m^{\prime}}]{z}}}{\mi{M^{\prime}-m^{\prime}}}}{\mi{M-m}}}{\te[l\x l^{\prime}]{F}} \\
                = \frac
                {\modulus{\diag[l\x l^{\prime}\x\mi{M}^{2}]{\Lambda^{2}}}^{1/2} \prob{\te[\mi{m}]{\rv{z}}}{\te[\mi{m}]{0}}{\te[l\x l^{\prime}\x\mi{m}^2]{\Upsilon}}\prob{\te[\mi{m^{\prime}}]{\rv{z}}}{\te[l\x l^{\prime}\x \mi{m^{\prime}}]{Z}}{\te[l\x l^{\prime}\x\mi{m^{\prime}}^{2}]{\Pi}}}
                {\modulus{\diag[l\x l^{\prime}\x\mi{M}^2]{\Lambda^{2}+2}}^{1/2}
                \prob{\te[\mi{m}]{\rv{z}}}{\te[\mi{m}]{0}}{\diag[\mi{m}^{2}]{1}}\prob{\te[\mi{m^{\prime}}]{\rv{z}}}{\te[\mi{m^{\prime}}]{0}}{\diag[\mi{m^{\prime}}^{2}]{1}}}
            \end{multline*}
            \begin{multline*}
                \te[l\x l^{\prime}]{\psi_{\mi{mm^{\prime}}}}
                \deq \frac{\evt{\;\evt{\te[l\x l^{\prime}]{k\!\left(\te[\mi{M}]{\rv{x}},X\right) K_{Y}^{-1} k\!\left(X,\te[\mi{M^{\prime}}]{\rv{x}}\right) \big\vert \te[\mi{m}]{z},\te[\mi{m^{\prime}}]{z}}}{\mi{M^{\prime}-m^{\prime}}}}{\mi{M-m}}}{\te[l\x l^{\prime}]{F}} \\
                 = \te[l\x\mi{L^{\prime\prime}}\x\mi{N^{\prime\prime}}]{g_{\mi{m}}}^{\dagger}
                \te[\mi{L^{\prime\prime}N^{\prime\prime}}\x\mi{L^{\prime\prime\prime}N^{\prime\prime\prime}}]{K_{Y}^{-1}} 
                \te[l^{\prime}\x\mi{L^{\prime\prime\prime}}\x\mi{N^{\prime\prime\prime}}]{g_{\mi{m^{\prime}}}}^{\dagger} \qquad\qquad\qquad
            \end{multline*}
            and
            \begin{equation*}
                \begin{aligned}
                    \te[l\x l^{\prime}\x\mi{m}\x\mi{m^{\prime\prime}}]{\Upsilon} &\deq \te[\mi{m}\x\mi{M}]{\Theta}
                    \diag[\mi{M}\x\mi{M^{\prime}}]{\diag[l\x l^{\prime}]{\Lambda^{2}+1}\diag[l\x l^{\prime}]{\Lambda^{2}+2}^{-1}} \te[\mi{m^{\prime\prime}}\x\mi{M^{\prime}}]{\Theta}^{\intercal} \\
                    \te[l\x l^{\prime}\x \mi{M^{\prime}}\x\mi{M^{\prime\prime\prime}}]{\Pi}^{-1} &\deq \te[l\x l^{\prime}\x\mi{M^{\prime}\x\mi{m}}]{\Phi}
                    \te[l\x l^{\prime}\x\mi{m}\x\mi{m^{\prime\prime}}]{\Gamma}^{-1} \te[l\x l^{\prime}\x\mi{m^{\prime\prime}}\x\mi{M^{\prime\prime\prime}}]{\Phi} \\
                    &\phantom{\deq}\ + \te[l\x l^{\prime}\x \mi{M^{\prime}}\x\mi{M^{\prime\prime\prime}}]{\Upsilon}^{-1} \\
                    \te[l\x l^{\prime}\x \mi{m^{\prime}}]{Z} &\deq 
                    \te[l\x l^{\prime}\x \mi{m^{\prime}}\x\mi{M}]{\Pi}
                    \te[l\x l^{\prime}\x\mi{M}\x\mi{m^{\prime\prime}}]{\Phi}
                    \te[l\x l^{\prime}\x\mi{m^{\prime\prime}}\x\mi{m}]{\Gamma}^{-1}
                    \te[\mi{m}]{\rv{z}}
                \end{aligned}
            \end{equation*}


\section{Gaussian Process Estimates}\label{sec:GPEst}
    \subsection{Expected Value}\label{sub:GPEst:Expectation}
    Using the shorthand
    \begin{equation*}
        \te[l\x\mi{L^{\prime}}\mi{N^{\prime}}]{KY\!g_{\mi{0}}} \deq \te[\mi{L^{\prime}}\mi{N^{\prime}}]{K_{Y}^{-1} Y^{\dagger}} \circ \te[l\x\mi{L^{\prime}}\x\mi{N^{\prime}}]{g_{\mi{0}}}^{\dagger}
    \end{equation*}
    to write
    \begin{equation*}                
        \evt{\te[l\x l^{\prime}]{\te[\mi{L}]{\mu_{\mi{m}}}^{2}}}{\mi{m}} 
        = \te[l\x\mi{L^{\prime\prime}}\mi{N^{\prime\prime}}]{KY\!g_{\mi{0}}}
        \te[l\x\mi{L^{\prime\prime}}\x\mi{N^{\prime\prime}} \x l^{\prime}\x\mi{L^{\prime\prime\prime}}\x\mi{N^{\prime\prime\prime}}]{H_{\mi{m}}}^{\dagger}
        \te[l^{\prime}\x\mi{L^{\prime\prime\prime}}\mi{N^{\prime\prime\prime}}]{KY\!g_{\mi{0}}}
    \end{equation*}
    results in
    \begin{align*}
        &\te[l\x\mi{L^{\prime\prime}}\x\mi{N^{\prime\prime}} \x l^{\prime}\x\mi{L^{\prime\prime\prime}}\x\mi{N^{\prime\prime\prime}}]{H_{\mi{m}}} \\
        &\deq \ev{\frac{
            \prob{\te[\mi{m}]{\rv{z}}}{\te[\mi{m}\x l\x \mi{L^{\prime\prime}\x N^{\prime\prime}}]{G}}{\te[l\x \mi{L^{\prime\prime}}]{\Gamma}} \otimes
            \prob{\te[\mi{m}]{\rv{z}}}{\te[\mi{m}\x l^{\prime}\x \mi{L^{\prime\prime\prime}\x N^{\prime\prime\prime}}]{G}}{\te[l^{\prime}\x\mi{L^{\prime\prime\prime}}]{\Gamma}}}
        {\prob{\te[\mi{m}]{\rv{z}}}{\te[\mi{m}]{0}}{\diag[\mi{m\x m}]{1}}
        \prob{\te[\mi{m}]{\rv{z}}}{\te[\mi{m}]{0}}{\diag[\mi{m\x m}]{1}}}}{\mi{m}} \\
        &\phantom{:}=
        \te[l\x \mi{L^{\prime\prime}}\x l^{\prime}\x \mi{L^{\prime\prime\prime}}]{\modulus{\Psi}^{-1}} \circ
        \frac{
            \prob{\te[\mi{m}\x l\x\mi{L^{\prime\prime}}\x\mi{N^{\prime\prime}}]{G}}
            {\te[\mi{m}\x l^{\prime}\x\mi{L^{\prime\prime\prime}}\x\mi{N^{\prime\prime\prime}}]{G}}
            {\te[l \x\mi{L^{\prime\prime}}\x l^{\prime}\x\mi{L^{\prime\prime\prime}}]{\Sigma}}
            }
        {\prob{\te[\mi{m}]{0}}
        {\te[\mi{m}\x l\x\mi{L^{\prime\prime}}\x\mi{N^{\prime\prime}}\x l^{\prime}\x\mi{L^{\prime\prime\prime}}\x\mi{N^{\prime\prime\prime}}]{\Sigma G}}{\te[l \x\mi{L^{\prime\prime}}\x l^{\prime}\x\mi{L^{\prime\prime\prime}}]{\Sigma\Psi}}}
    \end{align*}
    where
    \begin{equation*}
        \begin{aligned}
            \te[l\x l^{\prime\prime}\x l^{\prime}\x l^{\prime\prime\prime}\x\mi{m}^{2}]{\Sigma} &\deq 
            \te[l\x l^{\prime\prime}\x\mi{m}^{2}]{\Gamma} + \te[l^{\prime}\x l^{\prime\prime\prime}\x\mi{m}^{2}]{\Gamma} \\
            \te[l\x l^{\prime\prime}\x l^{\prime}\x l^{\prime\prime\prime}\x\mi{m}\x\mi{m^{\prime}}]{\Psi} &\deq 
            \te[l\x l^{\prime\prime}\x l^{\prime}\x l^{\prime\prime\prime}\x\mi{m}\x\mi{m^{\prime}}]{\Sigma} \\
            &\phantom{:}- \te[l\x l^{\prime\prime}\x\mi{m}\x\mi{m^{\prime\prime}}]{\Gamma} \te[l^{\prime}\x l^{\prime\prime\prime}\x\mi{m^{\prime\prime}}\x\mi{m^{\prime}}]{\Gamma} \\
            \te[l\x l^{\prime\prime}\x l^{\prime}\x l^{\prime\prime\prime}]{\modulus{\Psi}^{-1}} &\deq 
            \modulus{\te[l\x l^{\prime\prime}\x l^{\prime}\x l^{\prime\prime\prime}\x\mi{m}^{2}]{\Psi}}^{-1} \\
            \te[\mi{m}\x l\x l^{\prime\prime}\x\mi{N^{\prime\prime}}\x l^{\prime}\x l^{\prime\prime\prime}\x\mi{N^{\prime\prime\prime}}]{\Sigma G} &\deq 
            \te[l^{\prime}\x l^{\prime\prime\prime}\x\mi{m}\x\mi{m^{\prime}}]{\Gamma}
            \te[\mi{m^{\prime}}\x l\x l^{\prime\prime}\x\mi{N^{\prime\prime}}]{G}\\
            &\phantom{\deq}+
            \te[l\x l^{\prime\prime}\x\mi{m}\x\mi{m^{\prime}}]{\Gamma}
            \te[\mi{m^{\prime}}\x l^{\prime}\x l^{\prime\prime\prime}\x\mi{N^{\prime\prime\prime}}]{G}\\
            \te[l\x l^{\prime\prime}\x l^{\prime}\x l^{\prime\prime\prime}\x\mi{m}\x\mi{m^{\prime}}]{\Sigma\Psi} &\deq 
            \te[l\x l^{\prime\prime}\x l^{\prime}\x l^{\prime\prime\prime}\x\mi{m}\x\mi{m^{\prime\prime}}]{\Sigma}
            \te[l\x l^{\prime\prime}\x l^{\prime}\x l^{\prime\prime\prime}\x\mi{m^{\prime\prime}}\x\mi{m^{\prime}}]{\Psi}
        \end{aligned}                    
    \end{equation*}

    \subsection{Variance}\label{sub:GPEst:Variance}
        In calculating
        \begin{equation*}
            \evt{\;\evt{\te[\mi{L}^{2} \x \mi{L^{\prime}}^{2}]{\mu_{\mi{m}} \otimes \mu_{\mi{mm^{\prime}}} \otimes \mu_{\mi{m^{\prime}}}}}{\mi{m^{\prime}}}}{\mi{m}}
        \end{equation*}
        from \cref{eq:GPMom:Second} the terms containing $E$ reduce to squares of $g_{\mi{0}}$ by iterated expectations, and these will obviously cancel across \cref{eq:SPEst:W}. 
        We may therefore assume $E=0$ in \cref{eq:GPMom:Second}. The remaining terms are given by
        \begin{multline*}
            \evt{\;\evt{\te[]{\te[l]{\mu_{\mi{m}}} \otimes \te[l^{\prime\prime}\x l^{\prime}]{\phi_{\mi{mm^{\prime}}}} \otimes \te[l^{\prime\prime\prime}]{\mu_{\mi{m^{\prime}}}}}}{\mi{m^{\prime}}}}{\mi{m}} = \\
            \frac
            {\modulus{\diag[l^{\prime\prime}\x l^{\prime}\x\mi{M}^{2}]{\Lambda^{2}}}^{1/2}(2\pi)^{m/2}}
            {\modulus{\diag[l^{\prime\prime}\x l^{\prime}\x\mi{M}^2]{\Lambda^{2}+2}}^{1/2}}
            \te[\mi{L^{*}N^{*}}]{K_{Y}^{-1} Y^{\dagger}} \otimes
            \te[\mi{L^{*\prime}N^{*\prime}}]{K_{Y}^{-1} Y^{\dagger}} \\
            \left\lbrack\frac{
            \te[l\x\mi{L^{*}}\x\mi{N^{*}}]{g_{\mi{0}}} \otimes
            \te[l^{\prime\prime\prime}\x\mi{L^{*\prime}}\x\mi{N^{*\prime}}]{g_{\mi{0}}}}
            {\prob{\te[\mi{m^{\prime}}]{0}}{\te[\mi{m^{\prime}}\x l^{\prime\prime\prime}\x \mi{L^{*\prime}\x N^{*\prime}}]{G}}{\te[l^{\prime\prime\prime}\x \mi{L^{*\prime}}]{\Phi}}} \circ \right.\\
            \prob{\te[\mi{m}]{0}}{\te[l^{\prime\prime}\x l^{\prime}]{1-\Upsilon}^{1/2} \te[\mi{m}\x l\x \mi{L^{*}\x N^{*}}]{G}} 
            {\diag[]{1} +
            \te[l^{\prime\prime}\x l^{\prime}]{1-\Upsilon}^{1/2} \te[l\x \mi{L^{*}}]{\Phi}\te[l^{\prime\prime}\x l^{\prime}]{1-\Upsilon}^{\intercal/2}} \circ \\
            \left. \prob{\te[\mi{m^{\prime}}\x l^{\prime\prime\prime}\x \mi{L^{*\prime}\x N^{*\prime}}]{G}}
            {\te[]{\Omega} \te[]{C} \te[l\x \mi{L^{*}}]{\Gamma}^{-1} \te[\mi{m}\x l\x \mi{L^{*}\x N^{*}}]{G}}{\te[]{B}+\te[]{\Omega} \te[]{C} \te[]{\Omega}^{\intercal}}
            \phantom{\frac{_{\vert}^{\vert}}{_{\vert}^{\vert}}}\right\rbrack^{\dagger}
        \end{multline*}
        using the Cholesky decomposition
        \begin{equation*}
            \diag[\mi{m}^{2}]{1} - \te[l^{\prime\prime}\x l^{\prime}\x\mi{m}^{2}]{\Upsilon}
            = \te[l^{\prime\prime}\x l^{\prime}]{1-\Upsilon}^{1/2} \te[l^{\prime\prime}\x l^{\prime}]{1-\Upsilon}^{\intercal/2}
        \end{equation*}
        and
        \begin{equation*}
            \begin{aligned}
                \te[\mi{m^{\prime}\x m}]{\Omega} &\deq 
                \te[l^{\prime\prime\prime}\x l^{*\prime}\x\mi{m^{\prime}}\x\mi{m^{\prime\prime\prime}}]{\Phi}
                \te[l^{\prime\prime}\x l^{\prime}\x\mi{m^{\prime\prime\prime}}\x\mi{M}]{\Pi}
                \te[l^{\prime\prime}\x l^{\prime}\x\mi{M}\x\mi{m^{\prime\prime}}]{\Phi}
                \te[l^{\prime\prime}\x l^{\prime}\x\mi{m^{\prime\prime}}\x\mi{m}]{\Gamma}^{-1} \\
                \te[\mi{m^{\prime}}^{2}]{B} &\deq 
                \te[l^{\prime\prime\prime}\x l^{*\prime}\x\mi{m^{\prime}}\x\mi{m^{*\prime}}]{\Gamma}
                \te[l^{\prime\prime\prime}\x l^{*\prime}\x\mi{m^{*\prime}}\x\mi{m^{\prime\prime\prime}}]{\Phi} + \\
                &\phantom{\deq\ }\te[l^{\prime\prime\prime}\x l^{*\prime}\x\mi{m^{\prime}}\x\mi{m^{*\prime\prime\prime}}]{\Phi}
                \te[l^{\prime\prime}\x l^{\prime}\x\mi{m^{*\prime\prime\prime}}\x\mi{m^{*\prime}}]{\Pi}
                \te[l^{\prime\prime\prime}\x l^{*\prime}\x\mi{m^{*\prime}}\x\mi{m^{\prime\prime\prime}}]{\Phi} \\
                \te[\mi{m}^{2}]{C} &\deq 
                \te[l^{\prime\prime}\x l^{\prime}\x\mi{m}\x\mi{m^{*}}]{\Upsilon} \\
                &\phantom{\deq\ }
                \te[\mi{m^{*}}\x\mi{m^{*\prime\prime}}]{\te[]{\te[l\x l^{*}\x\mi{m^{**}}^{2}]{\Gamma}
                + \te[l\x l^{*}\x\mi{m^{**}}\x\mi{m^{**\prime\prime}}]{\Phi}\te[l^{\prime\prime}\x l^{\prime}\x\mi{m^{**\prime\prime}}\x\mi{m^{***}}]{\Upsilon}}^{-1}} \\
                &\phantom{\deq\ }\te[l\x l^{*}\x\mi{m^{*\prime\prime}}\x\mi{m^{\prime\prime}}]{\Gamma}
            \end{aligned}
        \end{equation*}
        and
        \begin{multline*}
            \evt{\;\evt{\te[]{\te[l]{\mu_{\mi{m}}} \otimes \te[l^{\prime\prime}\x l^{\prime}]{\psi_{\mi{mm^{\prime}}}} \otimes \te[l^{\prime\prime\prime}]{\mu_{\mi{m^{\prime}}}}}}{\mi{m^{\prime}}}}{\mi{m}} = \\
            \te[\mi{L^{*}N^{*}}]{K_{Y}^{-1} Y^{\dagger}} \otimes
            \te[\mi{L^{*\prime\prime}N^{*\prime\prime}}\x\mi{L^{*\prime}N^{*\prime}}]{K_{Y}^{-1}} \otimes
            \te[\mi{L^{*\prime\prime\prime}N^{*\prime\prime\prime}}]{K_{Y}^{-1} Y^{\dagger}} \\
            \left\lbrack\frac{
            \te[l\x\mi{L^{*}}\x\mi{N^{*}}]{g_{\mi{0}}} \otimes
            \te[l^{\prime\prime}\x\mi{L^{*\prime\prime}}\x\mi{N^{*\prime\prime}}]{g_{\mi{0}}} \otimes
            \te[l^{\prime}\x\mi{L^{*\prime}}\x\mi{N^{*\prime}}]{g_{\mi{0}}} \otimes
            \te[l^{\prime\prime\prime}\x\mi{L^{*\prime\prime\prime}}\x\mi{N^{*\prime\prime\prime}}]{g_{\mi{0}}}}
            {\prob{\te[\mi{m}]{0}}{\te[\mi{m}\x l\x \mi{L^{*}\x N^{*}}]{G}}{\te[l\x \mi{L^{*}}]{\Phi}} \otimes
            \prob{\te[\mi{m^{\prime}}]{0}}{\te[\mi{m^{\prime}}\x l^{\prime\prime\prime}\x \mi{L^{*\prime\prime\prime}\x N^{*\prime\prime\prime}}]{G}}{\te[l^{\prime\prime\prime}\x \mi{L^{*\prime\prime\prime}}]{\Phi}}} \right.\\
            \circ \prob{\te[\mi{m}\x l\x \mi{L^{*}\x N^{*}}]{G}}
            {\te[\mi{m}\x l^{\prime\prime}\x \mi{L^{*\prime\prime}\x N^{*\prime\prime}}]{G}}{\te[l\x\mi{L^{*}}\x l^{\prime\prime}\x\mi{L^{*\prime\prime}}]{D}} \circ \phantom{\frac{_{\vert}^{\vert}}{}}\\
            \left. \prob{\te[\mi{m^{\prime}}\x l^{\prime\prime\prime}\x \mi{L^{*\prime\prime\prime}\x N^{*\prime\prime\prime}}]{G}}
            {\te[\mi{m^{\prime}}\x l^{\prime}\x \mi{L^{*\prime}\x N^{*\prime}}]{G}}{\te[l^{\prime\prime\prime}\x\mi{L^{*\prime\prime\prime}}\x l^{\prime}\x\mi{L^{*\prime}}]{D}}
            \phantom{\frac{_{\vert}^{\vert}}{_{\vert}^{\vert}}}\right\rbrack^{\dagger}
        \end{multline*}
        where
        \begin{equation*}
            \begin{aligned}
                \te[l^{\prime\prime\prime}\x l^{*\prime\prime\prime}\x l^{\prime}\x l^{*\prime}\x\mi{m^{\prime}}^{2}]{D} &\deq 
                \te[l^{\prime\prime\prime}\x l^{*\prime\prime\prime}\x\mi{m^{\prime}}\x\mi{m^{*\prime}}]{\Gamma}
                \te[l^{\prime\prime\prime}\x l^{*\prime\prime\prime}\x\mi{m^{*\prime}}\x\mi{m^{\prime\prime\prime}}]{\Phi} + \\
                &\phantom{\deq\ }\te[l^{\prime\prime\prime}\x l^{*\prime\prime\prime}\x\mi{m^{\prime}}\x\mi{m^{*\prime\prime\prime}}]{\Phi}
                \te[l^{\prime}\x l^{*\prime}\x\mi{m^{*\prime\prime\prime}}\x\mi{m^{*\prime}}]{\Gamma}
                \te[l^{\prime\prime\prime}\x l^{*\prime\prime\prime}\x\mi{m^{*\prime}}\x\mi{m^{\prime\prime\prime}}]{\Phi} \\
            \end{aligned}
        \end{equation*}
        and similarly for $\te[l\x\mi{L^{*}}\x l^{\prime\prime}\x\mi{L^{*\prime\prime}\x m}^{2}]{D}$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{elsarticle-num} 
\bibliography{master}
\end{document}
\endinput
