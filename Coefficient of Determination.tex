\documentclass[preprint,12pt]{elsarticle}
    \usepackage{algorithm}
    \usepackage{algorithmic}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \usepackage{amssymb}
    \usepackage{amsmath}
    \usepackage[hidelinks]{hyperref}
    \usepackage[capitalize]{cleveref}
    \usepackage{xspace} 
    \usepackage{ifthen} 
    \usepackage{csvsimple}
    \setlength {\marginparwidth }{2cm}
    \usepackage{todonotes}
    \usepackage{float}
    \newcommand*{\M}[1]{\ensuremath{#1}\xspace} 
    \newcommand*{\tr}[1]{\M{#1}}
    \newcommand*{\x}{\times}
    \newcommand*{\mi}[1]{\mathbf{#1}} 
    \newcommand*{\st}[1]{\mathbb{#1}} 
    \newcommand*{\rv}[1]{\mathsf{#1}} 
    \newcommand*{\te}[2][]{\left\lbrack{#2}\right\rbrack_{#1}}
    \newcommand*{\tte}[2][]{\lbrack{#2}\rbrack_{#1}}
    \newcommand*{\tse}[2][]{\mi{\lbrack#2\rbrack}_{#1}}
    \newcommand*{\tme}[3][]{\lbrack{#3}\rbrack_{\tse[#1]{#2}}}
    \newcommand*{\diag}[2][]{\left\langle{#2}\right\rangle_{#1}}
    \newcommand*{\prob}[3]{\M{\mathrm{p}\!\left(\left.{#1}\right\vert{#2,#3}\right)}} 
    \newcommand*{\deq}{\M{\mathrel{\mathop:}=}} 
    \newcommand*{\deqr}{\M{=\mathrel{\mathop:}}} 
    \newcommand{\T}[1]{\text{#1}} 
    \newcommand*{\QT}[2][]{\M{\quad\T{#2}\ifthenelse{\equal{#1}{}}{\quad}{#1}}} 
    \newcommand*{\ev}[3][]{\mathbb{E}_{#3}^{#1}\!\left\lbrack{#2}\right\rbrack}
    \newcommand*{\evt}[3][]{\mathbb{E}_{#3}^{#1}\!#2}
    \newcommand*{\cov}[3][]{\ifthenelse{\equal{#1}{}}{\mathbb{V}_{#3}\!\left\lbrack{#2}\right\rbrack}{\mathbb{V}_{#3}\!\left\lbrack{#2,#1}\right\rbrack}}
    \newcommand*{\covt}[2]{\mathbb{V}_{#2}\!{#1}}
    \newcommand*{\gauss}[2]{\mathsf{N}\!\left({#1,#2}\right)}
    \newcommand*{\uni}[2]{\mathsf{U}\!\left({#1,#2}\right)}
    \newcommand*{\tgauss}[2]{\mathsf{N}({#1,#2})}
    \newcommand*{\gaussd}[2]{\mathsf{N}^{\dagger}\!\left({#1,#2}\right)}
    \newcommand*{\modulus}[1]{\M{\left\lvert{#1}\right\rvert}} 
    \newcommand*{\norm}[1]{\M{\left\lVert{#1}\right\rVert}} 
    \newcommand*{\ceil}[1]{\M{\left\lceil{#1}\right\rceil}} 
    \newcommand*{\set}[1]{\M{\left\lbrace{#1}\right\rbrace}} 
    \newcommand*{\setbuilder}[2]{\M{\left\lbrace{#1}\: \big\vert \:{#2}\right\rbrace}}
    \newcommand*{\uniti}{\lbrack 0,1\rbrack}
    \DeclareMathOperator*{\argmax}{argmax}
    \DeclareMathOperator*{\argmin}{argmin}
    \DeclareMathOperator*{\trace}{tr\!}

\journal{Reliability Engineering and System Safety}

\begin{document}
\begin{frontmatter}

    \title{The Coefficient of Determination of a Reduced Order Model}

    \author{Robert A. Milton}
    \ead{r.a.milton@sheffield.ac.uk}

    \author{Solomon F. Brown}
    \ead{s.f.brown@sheffield.ac.uk}

    \author{Aaron S. Yeardley}
    \ead{asyeardley1@sheffield.ac.uk}

    \address{Department of Chemical and Biological Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom}       

    \begin{abstract}
        %% Text of abstract
    \end{abstract}

    \begin{keyword}
        Global Sensitivity Analysis, Sobol' Index, Surrogate Model, Active Subspace
    \end{keyword}

\end{frontmatter}

\section{Introduction}\label{sec:Intro}


\section{Coefficient of Determination}\label{sec:COD}
    Given a regression model
    \begin{equation*}
        \T{Integrable } y \colon \uniti^{M+1} \mapsto \st{R}^{L}
    \end{equation*}
    begin by taking a uniformly distributed random variable (RV)
    \begin{equation*}
        \rv{u} \sim \uni{\te[\mi{M+1}]{0}}{\te[\mi{M+1}]{1}} \deq \uni{0}{1}^{M+1}
    \end{equation*}
    Exponentiation is categorical -- repeated cartesian $\x$ or tensor $\otimes$ -- unless otherwise specified. Square bracketed quantities are tensors, carrying their dimensions as a von Neumann ordinal subscript, in this case
    \begin{equation*}
        \mi{M+1} \deq (0,\ldots,M) \supseteq \mi{m+1} \deq (0,\ldots,m \leq M)
    \end{equation*}
    with void $\mi{0}$ voiding any tensor it subscripts. Ordinals are concatenated by Cartesian $\times$ and may be subtracted like sets, as in $\mi{M-m} \deq (m,\ldots,M-1)$. 
    Subscripts refer to the tensor prior to any superscript operation, so $\te[\mi{L}]{y(\rv{u})}^{2}$ is an $\mi{L}^{2} \deq \mi{L\x L}$ tensor, for example.
    The preference throughout this work is for uppercase constants and lowercase variables, in case of ordinals the lowercase ranging over the uppercase. We prefer $o$ for an unbounded positive integer, to avoid O.

    Expectations and variances will be subscripted by the dimensions of $\rv{u}$ marginalized (integrated over). Conditioning on the remaining dimensions is left implicit after \cref{def:Theory:y_m}, to lighten notation.
    Now, Construct $M+1$ stochastic processes (SPs)
    \begin{equation}\label{def:Theory:y_m}
        \te[\mi{L}]{\rv{y_m}} \deq \ev{y(\rv{u})}{\mi{M-m}} \deq \ev{y(\rv{u}) \big\vert \te[\mi{m}]{u}}{\mi{M-m}}
    \end{equation}
    ranging from $\tte[\mi{L}]{\rv{y_0}}$ to $\tte[\mi{L}]{\rv{y_M}}$. Every SP quietly depends on the ungovernable noise dimension $\tte[M]{\rv{u}} \perp \tte[\mi{M}]{\rv{u}}$, while ignoring governed input dimensions $\tte[\mi{M-m}]{\rv{u}}$. 
    Sans serif symbols such as $\rv{u,y}$ generally refer to RVs and SPs, italic $u,y$ being reserved for (tensor) functions and variables.
    
    Following Daniell-Kolmogorov \cite{Rogers.Williams2000} pp.124 we may regard an SP as a random function, from which we shall freely extract finite dimensional distributions generated by a design matrix $\tte[\mi{M\x o}]{u}$ of $o \in \st{Z}^{+}$ samples.
    Daniell-Kolmogorov incidentally secures $\rv{u}$. 
    Because $y$ is (Lebesgue) integrable it must be measurable, guaranteeing $\tte[\mi{L}]{\rv{y_0}}$.
    Because all probability measures are finite, integrability of $y$ implies integrability of $y^n$ for all $n \in \st{Z}^{+}$ \cite{Villani1985}. 
    So Fubini's Theorem \cite{Williams1991} pp.77 allows all expectations to be taken in any order. 
    This is sufficient to ensure every object appearing in this work.

    Our aim is to compare predictions from a reduced model $\rv{y_m}$ with those of the full model $\rv{y_M}$. Correlation between these predictions is squared -- using element-wise (Hadamard) multiplication $\circ$ and division -- to form an RV called the coefficient of determination
    \begin{equation}
        \te[\mi{L}^2]{\rv{R_{m}^{2}}} \deq 
        \frac{\cov[\rv{y_M}]{\rv{y_m}}{\mi{M}} \circ \cov[\rv{y_M}]{\rv{y_m}}{\mi{M}}}
        {\cov{\rv{y_m}}{\mi{m}} \circ \cov{\rv{y_M}}{\mi{M}}} =
        \frac{\cov{\rv{y_m}}{\mi{m}}}{\cov{\rv{y_M}}{\mi{M}}} \deqr
        \te[\mi{L}^2]{\rv{S_m}}
    \end{equation}
    The closed Sobol' index is the complement of the commonplace total Sobol' index
    \begin{equation*}
        \te[\mi{L}^2]{\rv{S_m}} \deqr \te[\mi{L}^2]{1} - \te[\mi{L}^2]{\rv{S^{T}_{M-m}}}
    \end{equation*}
    It has mean value over the ungovernable noise dimension of
    \begin{align}\label{def:COD:mean}
        \te[\mi{L}^2]{S_{\mi{m}}} &\deq \evt{\te[]{\rv{S_m}}}{M} = \frac{V_{\mi{m}}}{V_{\mi{M}}} \\            
        \T{where }\te[\mi{L}^2]{V_{\mi{m}}} &\deq \evt{\; \cov{\rv{y_m}}{\mi{m}}}{M} \ \ \forall \mi{m}\subseteq \mi{M}
    \end{align}
    and variance due to ungovernable noise of
    \begin{align}\label{def:COD:variance}
        \te[\mi{L}^4]{T_\mi{m}} &\deq 
        \cov{\rv{S_m}}{M} = \frac{V_{\mi{m}}^{2}}{V_{\mi{M}}^{2}} \circ
        \left(
            \frac{W_{\mi{mm}}}{V_{\mi{m}}^{2}}
            -2\frac{W_{\mi{mM}}}{{V_{\mi{m}}\otimes V_{\mi{M}}}}
            +\frac{W_{\mi{MM}}}{V_{\mi{M}}^{2}}
        \right) \\                
        \T{where }\te[\mi{L}^4]{W_{\mi{mm^{\prime}}}} &\deq \cov[\cov{\rv{y_{m^{\prime}}}}{\mi{m^{\prime}}}]{\cov{\rv{y_{m}}}{\mi{m}}}{M} \ \ \forall \mi{m,m^{\prime}} \subseteq \mi{M}
    \end{align}
    The remainder of this paper is devoted to calculating these two quantities -- the coefficient of determination and its variance over noise (measurement error, squared).


\section{Stochastic Process Estimates}\label{sec:SPEst}
    Define noiseless mean predictors (functions of $\tte[\mi{m}]{\rv{u}}$) and their error SPs as
    \begin{equation}\label{def:SPEst:fmem}
        \begin{aligned}
            \te[\mi{L}]{f_{\mi{m}}} &\deq \evt{\te[\mi{L}]{\rv{y_m}}}{\mi{M+1-m}} \\
            \te[\mi{L}]{\rv{e_m}} &\deq \te[\mi{L}]{\rv{y_m}} - \te[\mi{L}]{f_{\mi{m}}} \\
        \end{aligned}
    \end{equation}
    Adopt as design matrix a triad of inputs, their mutual dependence a function of conditioning, labelled according to how they will be marginalized
    \begin{equation*}
            \te[\mi{M+1\x 3}]{\rv{u}} \deq \left\lbrack \rv{u_0},\rv{u_m},\rv{u_M} \right\rbrack
    \end{equation*}
    The triad elicits the response
    \begin{equation}\label{def:SPEst:e}
        \te[\mi{L\x 3}]{\rv{e}} \deq y(\tte[\mi{M+1\x 3}]{\rv{u}}) - \evt{\;\evt{\;\ev{y(\tte[\mi{M+1\x 3}]{\rv{u}})}{M^{\prime\prime}}}{\mi{M^{\prime}+1^{\prime}-m^{\prime}}}}{\mi{M+1}}
    \end{equation}
    Primes mark independent input axes, otherwise each expectation applies to all three members of $\tte[\mi{M+1}]{\rv{u}}^3$. It is not always obvious whether axes are independent or shared by the triad, but this can be mechanically checked against the measure of integration behind an expectation. Repeated expectations over the same axis are rare here, usually indicating that apparent repetitions must be ``primed''.

    This purpose of the triad is to interrogate its response for moments in respect of shared noise
    \begin{equation}\label{def:SPEst:mu}
        \begin{aligned}
            \te[(\mi{L\x 3})^{n}]{\mu_{n}} &\deq \ev{\tte[\mi{L\x 3}]{\rv{e}}^{n}}{M} \\
            \te[\mi{L\x 3}]{\mu_{1}} &\phantom{:}= \te[\mi{L\x 3}]{0}
        \end{aligned}
    \end{equation}
    for these embody
    \begin{equation*}
        \te[\mi{L}^{n}]{\mu_{\mi{m^{\prime}\ldots m}^{n\prime}}} \deq \te[(\mi{L\x}i)^{n}]{\mu_{n}} = \ev{\tte[\mi{L}]{\rv{e}_{\mi{m}^{\prime}}}\otimes\cdots\otimes\tte[\mi{L}]{\rv{e}_{\mi{m}^{n\prime}}}}{M}
    \end{equation*}
    where $\mi{m}^{i\prime} \in \set{\mi{0},\mi{m},\mi{M}}$. This expression, along with $f_{\mi{m}}$, underpins both quantities we seek. The reduction which follows repeatedly realizes
    \begin{equation}\label{eq:SPEstimates:reduction}
        \te[\mi{L}^{n}]{\mu_{\mi{0\ldots 0}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}} \deq 
        \evt{\te[\mi{L}^{n}]{\mu_{\mi{M\ldots M}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}}}{\mi{M}} = 
        \evt{\te[\mi{L}^{n}]{\mu_{\mi{m\ldots m}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}}}{\mi{m}}
    \end{equation}

    The expected conditional variance in \cref{def:COD:mean} amounts to
    \begin{equation}
        \begin{aligned}
            \te[\mi{L}]{V_{\mi{m}}} 
            &= \evt{\;\ev{\te[\mi{L}]{\rv{e_m} + f_{\mi{m}}}^{2}}{M}}{\mi{m}}
            - \ev{\te[\mi{L}]{\rv{e_0} + f_{\mi{0}}}^{2}}{M} \\
            &= \ev{\te[\mi{L}]{f_{\mi{m}}}^{2}}{\mi{m}} - \te[\mi{L}]{f_{\mi{0}}}^{2} + 
            \evt{\te[\mi{L}^2]{\mu_{\mi{mm}}}}{\mi{m}} - \te[\mi{L}^2]{\mu_{\mi{00}}} \\
            &= \ev{\te[\mi{L}]{f_{\mi{m}}}^{2}}{\mi{m}} - \te[\mi{L}]{f_{\mi{0}}}^{2}
        \end{aligned}
    \end{equation}

    The covariance between conditional variances in \cref{def:COD:variance} is
    \begin{equation*}
        \begin{aligned}
            \te[\mi{L}^4]{W_{\mi{mm^{\prime}}}} &\deq \cov[\cov{\rv{y_{m^{\prime}}}}{\mi{m^{\prime}}}]{\cov{\rv{y_{m}}}{\mi{m}}}{M} \\
            &\phantom{:}=
            \cov[\ev{\te[\mi{L}]{\rv{y_{m^{\prime}}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m^{\prime}}}]{\ev{\te[\mi{L}]{\rv{y_{m}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m}}}{M} \\
            &\phantom{:}=
            \ev{\ev{\te[\mi{L}]{\rv{y_{m}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m}} \otimes\ev{\te[\mi{L}]{\rv{y_{m^{\prime}}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m^{\prime}}}}{M}\\
            &\phantom{\deq}\  - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}} \\       
            &\phantom{:}= \te[\mi{L}^4]{A_{\mi{mm^{\prime}}}-A_{\mi{0m^{\prime}}}-A_{\mi{m0}}+A_{\mi{00}}}
        \end{aligned}
    \end{equation*}
    Here, for any $\mi{m},\mi{m^{\prime}}\subseteq\mi{M}$
    \begin{equation*}
        \begin{aligned}
            \te[\mi{L}^4]{A_{\mi{mm^{\prime}}}}
            &\deq \evt{\;\evt{\;\ev{\te[\mi{L}]{\rv{y_{m}}}^{2} \otimes \te[\mi{L}]{\rv{y_{m^{\prime}}}}^{2}}{M}}{\mi{m^{\prime}}}}{\mi{m}} - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}} \\
            &\phantom{:}= \evt{\;\evt{\;\ev{\te[\mi{L}]{\rv{e_{m}}+f_{\mi{m}}}^{2} \otimes \te[\mi{L}]{\rv{e_{m^{\prime}}}+ f_{\mi{m^{\prime}}}}^{2}}{M}}{\mi{m^{\prime}}}}{\mi{m}}
            - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}}
        \end{aligned}
    \end{equation*}
    exploiting the fact that $V_{\mi{0}} = \te[\mi{L}^2]{0}$. \Cref{eq:SPEstimates:reduction} cancels all terms beginning with $\te[\mi{L}]{\rv{e_{m}}}^{2}$, first across $A_{\mi{m,m^{\prime}}}-A_{\mi{0,m^{\prime}}}$ then across $A_{\mi{m,0}}-A_{\mi{0,0}}$. All remaining terms ending in $\te[\mi{L}]{f_\mi{m^{\prime}}}^{2}$ are eliminated by error centralization $\mu_{1} = 0$ and
    \begin{multline*}
        \evt{\;\ev{\te[\mi{L}]{f_{\mi{m}}}^{2} \otimes \te[\mi{L}]{f_{\mi{m^{\prime}}}}^{2}}{\mi{m^{\prime}}}}{\mi{m}}
        - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}} \\
        = \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}]{f_{\mi{0}}}^{2}
        + \te[\mi{L}]{f_{\mi{0}}}^{2}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}}
        + \te[\mi{L}]{f_{\mi{0}}}^{4}
    \end{multline*}
    cancelling across $A_{\mi{m,m^{\prime}}}-A_{\mi{0,m^{\prime}}} - A_{\mi{m,0}}+A_{\mi{0,0}}$.
    Similar arguments eliminate $\te[\mi{L}]{\rv{e_{m^{\prime}}}}^{2}$ and $\te[\mi{L}]{f_\mi{m}}^{2}$.
    Effectively then
    \begin{equation}
        \te[\mi{L}^4]{A_{\mi{mm^{\prime}}}} = \sum_{\pi(\mi{L}^{2})} \sum_{\pi(\mi{L^{\prime}}^{2})}
        \evt{\;\evt{\te[\mi{L}^{2} \x \mi{L^{\prime}}^{2}]{f_{\mi{m}} \otimes \mu_{\mi{mm^{\prime}}} \otimes f_{\mi{m^{\prime}}}}}{\mi{m^{\prime}}}}{\mi{m}}
    \end{equation}
    where each summation is over permutations of tensor axes
    \begin{equation*}
        \pi(\mi{L}^{2}) \deq \set{(\mi{L}\x\mi{L^{\prime\prime}}), (\mi{L^{\prime\prime}}\x\mi{L})} \QT{;} \pi(\mi{L^{\prime}}^{2}) \deq \set{(\mi{L^{\prime}}\x\mi{L^{\prime\prime\prime}}), (\mi{L^{\prime\prime\prime}}\x\mi{L^{\prime}})}
    \end{equation*}
    Primes on constants are for bookeeping purposes only ($\mi{L}^{i\prime} = \mi{L}$ always), they do not change the value of the constant -- unlike primes on variables ($\mi{m}^{i\prime}$ need not equal $\mi{m}$ in general).
    In order to further elucidate these estimates, we must fill in the details of the underlying stochastic processes, sufficiently identifying the regression $y$ by its first two moments $f, \mu_{2}$.


\section{Interlude: Gaussian Process (GP) Regression} \label{sec:GPR}
    A GP over $x$ is formally defined and specified by
    \begin{equation*}
        \te[\mi{L}]{\rv{y_M}} \big\vert \te[\mi{M}\x\mi{o}]{x} \sim 
        \gaussd{\te[\mi{L}\x\mi{o}]{\bar{y}(x)}}{\te[(\mi{L}\x\mi{o})^{2}]
        {k_{\rv{y}}(x,x)}} \quad \forall o \in \st{Z^{+}}
    \end{equation*}
    where tensor ranks concatenate into a multivariate normal distribution
    \begin{equation*}
        \begin{aligned}
            \te[\mi{L}\x\mi{o}]{} \sim \gaussd{\te[\mi{L}\x\mi{o}]{}}{\te[(\mi{L\x o})^{2}]{}}
            & \Longleftrightarrow
            \te[\mi{L}\x\mi{o}]{}^{\dagger} \sim \gauss{\te[\mi{L}\x\mi{o}]{}^{\dagger}}{\te[(\mi{L\x o})^{2}]{}^{\dagger}} \\
            \te[\mi{lo}-\mi{(l-1)o}]{\te[\mi{L}\x\mi{o}]{}^{\dagger}} 
            &\deq \te[l-1\x\mi{o}]{} \\
            \te[(\mi{lo}-(\mi{l-1})\mi{o}) \x (\mi{l^{\prime}o}-\mi{(l^{\prime}-1)o})]
            {\te[(\mi{L\x o})^{2}]{}^{\dagger}} 
            &\deq \te[l-1\x\mi{o} \x l^{\prime}-1\x\mi{o}]{} \\
        \end{aligned}
    \end{equation*}
    supporting the fundamental definition of the Gaussian process kernel, as a covariance over response space
    \begin{equation*}
        \te[l\x o\x l^{\prime}\x o^{\prime}]{k_{\rv{y}}(x,x)} 
        \deq \cov[{\te[l^{\prime}\x o^{\prime}]{\rv{y_M}\vert x}}]{\te[l\x o]{\rv{y_M}\vert x}}{\mi{Lo}}
    \end{equation*}

    \subsection{Tensor Gaussians} \label{sub:GPR:Tensor}
        A tensor Gaussian like $\prob{\te[\mi{m}\x\mi{o}]{x}}{\te[\mi{m}\x\mi{o^{\prime}}]{x^{\prime}}}
        {\te[\mi{L}^{2}\x\mi{m}^{2}]{\Sigma}}$ is defined element-wise
        \begin{multline} \label{def:Notation:p}
            \te[l\x o \x l^{\prime}\x o^{\prime}]{\prob{\te[\mi{m}\x\mi{o}]{x}}{\te[\mi{m}\x\mi{o^{\prime}}]{x^{\prime}}}
            {\te[\mi{L}^{2}\x\mi{m}^{2}]{\Sigma}}}
            \deq (2 \pi)^{-M/2} \modulus{\te[l\x l^{\prime}]{\Sigma}}^{-1/2} \\
            \exp\left(-\frac{
                \te[\mi{m}\x l\x o\x l^{\prime}\x o^{\prime}]{x-x^{\prime}}^{\intercal} 
            \te[l\x l^{\prime}]{\Sigma}^{-1} 
            \te[\mi{m}\x l\x o\x l^{\prime}\x o^{\prime}]{x-x^{\prime}}}
            {2}\right)
        \end{multline}
        in terms of the matrix
        \begin{equation*}
            \te[l\x l^{\prime}]{\Sigma} \deq
            \te[l\x l^{\prime}\x\mi{m}^{2}]{\Sigma}
        \end{equation*}
        and the transpose $^{\intercal}$ (moving first multi-index to last) of the broadcast difference between two tensors
        \begin{equation*}
            \te[\mi{m}\x l\x o\x l^{\prime}\x o^{\prime}]{x-x^{\prime}} \deq
            \te[\mi{m}\x o]{x}
            - \te[\mi{m}\x o^{\prime}]{x^{\prime}}
        \end{equation*}
        Remarkably, the algebraic development in the remainder of this paper relies almost exclusively on an invaluable product formula reported in \cite{Rasmussen2016}:
        \begin{multline} \label{eq:GPR:product}
            \prob{z}{a}{A} \circ \prob{\Theta^{\intercal}z}{\tr{b}}{\tr{B}}
            = \prob{0}{(b-\Theta^{\intercal}a)}{(B + \Theta^{\intercal}A\Theta)} \\
            \circ \prob{z}
            {(A^{-1}+\Theta B^{-1}\Theta^{\intercal})^{-1}(A^{-1}a+\Theta B^{-1}b)}
            {(A^{-1}+\Theta B^{-1}\Theta^{\intercal})^{-1}}
        \end{multline}
        This formula and the Gaussian tensors behind it will appear in a variety of guises.

    \subsection{Prior GP} \label{sub:GPR:Prior}
        GP regression decomposes output $\te[\mi{L}]{\rv{y_M}}$ into signal GP $\te[\mi{L}]{\rv{f_M}}$, and independent noise GP $\te[\mi{L}]{\rv{e}_M}$ with constant noise covariance $\te[\mi{L}^2]{E}$
        \begin{equation*}
            \begin{aligned}
                \te[\mi{L}]{\rv{y_M}\vert E} 
                &= \te[\mi{L}]{\rv{f_M}} + \te[\mi{L}]{\rv{e}_M\vert E} \\
                \te[\mi{L}]{\rv{e}_M\vert E} \big\vert \te[\mi{M}\x\mi{o}]{x}
                &\sim \gaussd{\te[\mi{L}\x\mi{o}]{0}}{\te[(\mi{L}\x 1)^2]{E} \circ \diag[(1\x\mi{o})^2]{1}}
            \end{aligned}
        \end{equation*}
        The RBF kernel is hyperparametrized by signal covariance $\te[\mi{L}^2]{F}$ and the matrix $\te[\mi{L}\x\mi{M}]{\Lambda}$ of characteristic lengthscales for each output/input combination. 
        Angle brackets denoting a (perhaps broadcast) diagonal tensor, such as the identity matrix $\diag[\mi{m}^2]{1} \deqr \diag[]{\tte[\mi{m}]{1}}$, are used to define
        \begin{equation*}
            \begin{aligned}
                \diag[l\x l^{\prime}\x\mi{M}^{2}]{\Lambda^{2} \pm I} 
                &\deq \diag{\te[l\x\mi{M}]{\Lambda} \circ \te[l^{\prime}\x\mi{M}]{\Lambda} \pm \te[\mi{M}]{I}} 
                \qquad I \in \set{0}\cup\st{Z}^{+} \\
                \diag[l\x l^{\prime}\x\mi{M}^{2}]{\Lambda^{2}} &\deq 
                \diag[l\x l^{\prime}]{\Lambda^{2} \pm 0} \\
                    \te[l\x l^{\prime}]{\pm F} 
                &\deq (2 \pi)^{M/2} \modulus{\diag[l\x l^{\prime}]{\Lambda^{2}}}^{1/2} \te[l\x l^{\prime}]{F}
            \end{aligned}
        \end{equation*}
        and implement the objective RBF prior using \cref{def:Notation:p}
        \begin{equation*}
            \te[\mi{L}]{\rv{f_M} \vert F,\Lambda}
            \big\vert \te[\mi{M}\x\mi{o}]{x} \sim \\
            \gaussd{\te[\mi{L}\x\mi{o}]{0}}{\te[(\mi{L}\x 1)^{2}]{\pm F} \circ 
            \prob{\te[\mi{M}\x\mi{o}]{x}}{\te[\mi{M}\x\mi{o}]{x}}
            {\diag[\mi{L}^{2}\x\mi{M}^{2}]{\Lambda^{2}}}} 
        \end{equation*}
        
    \subsection{Predictive GP} \label{sub:GPR:Predictive}
        Bayesian inference for GP regression further conditions the hyper-parametrized GP $\rv{y} \vert E,F,\Lambda$ on the observed realization of the random variable $\te{\rv{y}\vert X}$
        \begin{equation*}
            \te[\mi{L} \x \mi{N}]{Y}^{\dagger} \deq \te{\te[\mi{L}]{\rv{y_M}\vert E,F,\Lambda} \big\vert \te[\mi{M}\x\mi{N}]{X}}^{\dagger}\!(\omega) \in \st{R}^{LN}
        \end{equation*}
        To this end we define
        \begin{equation} \label{def:GPR:Kk}
            \begin{aligned}
                \te[\mi{Lo}\x\mi{Lo}]{K_{\rv{e}}} &\deq 
                \cov{\te{\te[\mi{L}]{\rv{e}_{M}\vert E} \big\vert \te[\mi{M}\x\mi{o}]{x}}^{\dagger}}{\mi{Lo}} \\
                &\phantom{:}= \te{\te[(\mi{L}\x 1)^2]{E} \circ \diag[(1\x\mi{o})^2]{1}}^{\dagger} \\
                \te[\mi{Lo}\x\mi{L^{\prime}o^{\prime}}]{k(x, x^{\prime})} &\deq
                \cov[{\te{\te[\mi{L^{\prime}}]{\rv{f_M}\vert F,\Lambda} \big\vert \te[\mi{M}\x\mi{o^{\prime}}]{x^{\prime}}}^{\dagger}}]
                {\te{\te[\mi{L}]{\rv{f_M}\vert F,\Lambda} \big\vert \te[\mi{M}\x\mi{o}]{x}}^{\dagger}}{\mi{Lo}} \\
                &\phantom{:}= \te{\te[\mi{L}^{2}]{\pm F} \circ 
                \prob{\te[\mi{M}\x\mi{o}]{x}}{\te[\mi{M}\x\mi{o^{\prime}}]{x^{\prime}}}
                {\diag[\mi{L}^{2}\x\mi{M}^{2}]{\Lambda^{2}}}}^{\dagger} \\
                %
                \te[\mi{LN}\x\mi{LN}]{K_{Y}} &\deq 
                \cov{\te{\te[\mi{L}]{\rv{y}\vert E,F,\Lambda} \big\vert \te[\mi{M}\x\mi{N}]{X}}^{\dagger}}{\mi{Lo}} \\
                &\phantom{:}= k(\te[\mi{M}\x\mi{N}]{X},\te[\mi{M}\x\mi{N}]{X}) + \te[\mi{LN}\x\mi{LN}]{K_{\rv{e}}})
            \end{aligned}
        \end{equation}
        Applying Bayes' rule
        \begin{equation*}
            \begin{aligned}
                \mathsf{p}(\rv{f_M}\vert Y)\mathsf{p}(Y) = \mathsf{p}(Y\vert \rv{f_M})\mathsf{p}(\rv{f_M})
                &= \prob{Y^{\dagger}}{\rv{f_M}^{\dagger}}{K_{\rv{e}_{M}}} \prob{\rv{f_M}^{\dagger}}{\te[\mi{LN}]{0}}{k(X,X)} \\
                &= \prob{\rv{f_M}^{\dagger}}{Y^{\dagger}}{K_{\rv{e}_{M}}} \prob{\rv{f_M}^{\dagger}}{\te[\mi{LN}]{0}}{k(X,X)}
            \end{aligned}
        \end{equation*}
        \cref{eq:GPR:product} immediately reveals the marginal likelihood
        \begin{equation} \label{eq:GPR:marginalLikelihood}
            \mathsf{p}\!\left(\te{Y \vert E,F,\Lambda} \big\vert X\right)
            = \prob{\te[\mi{L\x N}]{Y}^{\dagger}}{\te[\mi{LN}]{0}}{K_Y}
        \end{equation}
        and the posterior distribution
        \begin{multline*}
            \te[\mi{L\x N}]{\te{\rv{f_M} \vert Y \vert E,F,\Lambda} \big\vert X}^{\dagger} \sim \\
            \gauss{k(X,X) K_{Y}^{-1} Y^{\dagger}}{\ k(X,X) - k(X,X) K_{Y}^{-1} k(X,X)}
        \end{multline*}

        The ultimate goal is the posterior predictive GP which extends the posterior distribution to arbitrary -- usually unobserved -- $\te[\mi{M}\x\mi{o}]{x}$. This is traditionally derived from the definition of conditional probability, but this seems unnecessary, for the extension must recover the posterior distribution when $x=X$. There is only one way of selectively replacing $X$ with $x$ in the posterior formula which preserves the coherence of tensor ranks:
        \begin{multline} \label{def:GPR:Predictive}
            \te[\mi{L\x o}]{\te{\rv{f_M} \vert Y \vert E,F,\Lambda} \big\vert x}^{\dagger} \sim \\
            \gauss{k(x,X) K_{Y}^{-1} Y^{\dagger}}{\ k(x,x) - k(x,X) K_{Y}^{-1} k(X,x)}
        \end{multline}

    \subsection{GP Optimization} \label{sub:GPR:Optimization}
        Henceforth we implicitly condition on optimal hyperparameters, which maximise the marginal likelihood \cref{eq:GPR:marginalLikelihood}.
        \begin{equation} \label{eq:GPR:hyperparameters}
            \te[\mi{L}^{2}]{E},\te[\mi{L}^{2}]{F},\te[\mi{L}\x\mi{M}]{\Lambda} \deq \argmax \prob{\te[\mi{L\x N}]{Y}^{\dagger}}{\te[\mi{LN}]{0}}{K_Y}
        \end{equation}
        The lengthscale tensor could feasibly have been of maximal rank $\te[\mi{L}^{2}\x\mi{M}]{\Lambda}$. We have restricted this to $\te[\mi{L}\x\mi{M}]{\Lambda}$, as one set of ARD lengthscales per output is heuristically satisfying and enables effective optimization as follows.
        For each output $l \in \mi{L}$ construct a separate GP to optimize the diagonal hyperparameters
        \begin{equation*}
            \te[l^{2}]{E},\te[l^{2}]{F},\te[l\x\mi{M}]{\Lambda} = \\
            \argmax \prob{\te[l\x \mi{N}]{Y}^{\dagger}}{\te[\mi{N}]{0}}{\te[\mi{(l+1)N-N}\x\mi{(l+1)N-N}]{K_Y}}
        \end{equation*}
        From this starting point, $E,F$ may be optimized (off-diagonal elements in particular) in the full multi-output GP \cref{eq:GPR:hyperparameters}. One may then attempt to re-optimize lengthscales according to \cref{eq:GPR:hyperparameters}, and iterate, although this may be gilding the lily.


    \section{Gaussian Process Estimates}\label{sec:GPEst}
        This Section fills out the stochastic process estimates for the coefficient of determination in the case of Gaussian Process Regression. 
        Let $c\colon \st{R} \to [0,1]$ be the (bijective) CDF of the standard, univariate normal distribution, and define the triads
        \begin{equation*}
            \begin{aligned}
                \te[\mi{M\x 3}]{\rv{z}} &\deq c^{-1}\!\left(\te[\mi{M\x 3}]
                {\rv{u}}\right) \sim \gauss{\te[\mi{M+1\x 3}]{0}}{\diag[(\mi{M+1})^{2}]{1}} \\
                \te[\mi{M^{\prime}\x 3}]{\rv{x}} &\deq \te[\mi{M\x M^{\prime}}]{\Theta}^{\intercal} \te[\mi{M\x 3}]{\rv{z}}
            \end{aligned}
        \end{equation*}
        Here, the rotation matrix $\te[\mi{M\x M^{\prime}}]{\Theta}^{\intercal} = \te[\mi{M\x M^{\prime}}]{\Theta}^{-1}$ is broadcast to multiply the triad $\tte[\mi{M\x 3}]{\rv{z}}$. Throughout the remainder of this paper, primed ordinal subscripts are used to specify einstein summed (contracted) products of tensors.
        The purpose of ths arbitrary rotation is to allow GPs whose input basis $\rv{x}$ is not aligned with the fundamental basis $\rv{u}$ of model reduction and coefficient of determination. The latter is aligned with $\rv{z}$ which is the input we must condition.

        Adding shared gaussian noise $\te[\mi{L}]{\rv{e}_M\vert E}$ to \cref{def:GPR:Predictive} yields
        \begin{multline}\label{eq:GPEst:yDist}
            \te[\mi{L\x 3}]{y(\te[\mi{M+1\x 3}]{\rv{u}}) \big\vert \te[\mi{M\x 3}]{u}}^{\dagger} 
            = \te[\mi{L\x 3}]{\te{\rv{y_M} \vert Y \vert E,F,\Lambda} \big\vert \tte[\mi{M\x 3}]{z}}^{\dagger} \sim \\
            \gauss{k(x,X) K_{Y}^{-1} Y^{\dagger}}{\ k(x,x) - k(x,X) K_{Y}^{-1} k(X,x) + E^{\dagger}}
        \end{multline}
        using broadcast $\tte[\mi{L3\x L3}]{E^{\dagger}} \deq \tte[(\mi{L\x 3})^{2}]{\tte[(\mi{L}\x 1)^{2}]{E} \circ \tte[(1\x\mi{3})^{2}]{1}}^{\dagger}$. 
        From the development in \cref{sec:SPEst}, especially \crefrange{def:SPEst:fmem}{def:SPEst:mu}, the triad response has mean $f$ and variance $\mu{2}$ and this is all the information we need to compute the coefficient of determination and its variance. 
        Obviously these can be read straight off \cref{eq:GPEst:yDist}.

        For the sake of completeness, and elucidation we bring the GP estimate fully under the umbrella of the SP estimate by identifying ungovernable noise, ultimately ascribable to $\tte[M]{\rv{u}}$.
        let $d_{\mi{L}}\colon (0,1) \to (0,1)^{L}$ concatenate every $L^{\mathrm{th}}$ decimal place starting at $l$, for each dimension $l\leq L$ of $(0,1)^{L}$, then \cref{eq:GPEst:yDist} can be rewritten
        \begin{equation}\label{eq:GPEst:yReveal}
            y(\te[\mi{M+1\x 3}]{\rv{u}}) \big\vert \te[\mi{M\x 3}]{u} = \te[\mi{L\x 3}]{f} + \te[(\mi{L\x 3})^{2}]{\mu_{2}}^{/\intercal} \te[\mi{L}\x 3]{c^{-1}\!\left(d_{\mi{L}}\left(\te[M]{\rv{u}}\right)\right)}
        \end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussian Process Estimates}\label{sec:GPEstimates}
    This Section fills out the stochastic process estimates for the coefficient of determination in the case of Gaussian Process Regression. 
    Let $c\colon \st{R}^{Mo} \to [0,1]^{Mo}$ be the (bijective) CDF of the standard, univariate normal distribution, and let $d_{\mi{L}}\colon (0,1) \to (0,1)^{L}$ concatenate every $L^{\mathrm{th}}$ decimal place starting at $l$, for each dimension $l\leq L$ of $(0,1)^{L}$. Then
    \begin{equation*}
        \begin{aligned}
            \te[\mi{M}]{\rv{z}} \deq c^{-1}\!\left(\te[\mi{M}]{\rv{u}}\right) &\sim \gauss{\te[\mi{M}]{0}}{\diag[\mi{M}^2]{1}} \\
            \te[L]{e_{M}} \deq c^{-1}\!\left(d_{\mi{L}}\left(\te[M]{\rv{u}}\right)\right) &\sim \gauss{\te[\mi{L}]{0}}{\diag[\mi{L}^2]{1}}
        \end{aligned}
    \end{equation*}

    Algebraic developments rely exclusively on trivial normal marginalization and scaling
    \begin{equation} \label{eq:Notation:marginalization}
        \begin{aligned}
            \te[\mi{M}]{\rv{z}} \sim \gauss{\te[\mi{M}]{Z}}{\te[\mi{M}\x\mi{M}]{\Sigma}} &\Rightarrow
            \te[\mi{m}]{\rv{z}} \sim \gauss{\te[\mi{m}]{Z}}{\te[\mi{m}\x\mi{m}]{\Sigma}} \\
            \te[\mi{M}]{\rv{z}} \sim \gauss{\te[\mi{M}]{Z}}{\te[\mi{M}\x\mi{M}]{\Sigma}} &\Rightarrow
            \te[\mi{M}\x\mi{M}]{\Theta}^{\intercal}\te[\mi{M}]{\rv{z}} \sim \gauss{\Theta^{\intercal}Z}{\Theta^{\intercal}\Sigma\Theta}                        
        \end{aligned}
    \end{equation}
    \begin{equation} \label{eq:Notation:scaling}
    \end{equation}
        \begin{equation}
            \te[\mi{L}]{f_{\mi{m}}} = \ev{k(\rv{x},X) K_{Y}^{-1} Y^{\dagger} \big\vert \tte[\mi{m}]{\rv{z}}}{\mi{M+1-m}}
        \end{equation}
        and
        \begin{multline}
            \te[(\mi{L\x 3})^{2}]{\mu_{2}} = \\
            \evt{\;\evt{\;\ev{k(\rv{x},\rv{x}) - k(\rv{x},X) K_{Y}^{-1} k(X,\rv{x}) + E^{\dagger} \big\vert \tte[\mi{M\x 3}]{\rv{z}}}{M^{\prime\prime}}}{\mi{M+1-m}}}{\mi{M+1}}
        \end{multline}
    


\bibliographystyle{elsarticle-num} 
\bibliography{master}
\end{document}
\endinput
