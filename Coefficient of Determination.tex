\documentclass[preprint,12pt]{elsarticle}
    \usepackage{algorithm}
    \usepackage{algorithmic}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \usepackage{amssymb}
    \usepackage{amsmath}
    \usepackage[hidelinks]{hyperref}
    \usepackage[capitalize]{cleveref}
    \usepackage{xspace} 
    \usepackage{ifthen} 
    \usepackage{csvsimple}
    \setlength {\marginparwidth }{2cm}
    \usepackage{todonotes}
    \usepackage{float}
    \newcommand*{\M}[1]{\ensuremath{#1}\xspace} 
    \newcommand*{\tr}[1]{\M{#1}}
    \newcommand*{\x}{\times}
    \newcommand*{\mi}[1]{\mathbf{#1}} 
    \newcommand*{\st}[1]{\mathbb{#1}} 
    \newcommand*{\rv}[1]{\mathsf{#1}} 
    \newcommand*{\te}[2][]{\left\lbrack{#2}\right\rbrack_{#1}}
    \newcommand*{\tte}[2][]{\lbrack{#2}\rbrack_{#1}}
    \newcommand*{\tse}[2][]{\mi{\lbrack#2\rbrack}_{#1}}
    \newcommand*{\tme}[3][]{\lbrack{#3}\rbrack_{\tse[#1]{#2}}}
    \newcommand*{\diag}[2][]{\left\langle{#2}\right\rangle_{#1}}
    \newcommand*{\prob}[3]{\M{\mathsf{p}\!\left(\left.{#1}\right\vert{#2,#3}\right)}} 
    \newcommand*{\deq}{\M{\mathrel{\mathop:}=}} 
    \newcommand*{\deqr}{\M{=\mathrel{\mathop:}}} 
    \newcommand{\T}[1]{\text{#1}} 
    \newcommand*{\QT}[2][]{\M{\quad\T{#2}\ifthenelse{\equal{#1}{}}{\quad}{#1}}} 
    \newcommand*{\ev}[3][]{\mathbb{E}_{#3}^{#1}\!\left\lbrack{#2}\right\rbrack}
    \newcommand*{\evt}[3][]{\mathbb{E}_{#3}^{#1}\!#2}
    \newcommand*{\cov}[3][]{\ifthenelse{\equal{#1}{}}{\mathbb{V}_{#3}\!\left\lbrack{#2}\right\rbrack}{\mathbb{V}_{#3}\!\left\lbrack{#2,#1}\right\rbrack}}
    \newcommand*{\covt}[2]{\mathbb{V}_{#2}\!{#1}}
    \newcommand*{\gauss}[2]{\mathsf{N}\!\left({#1,#2}\right)}
    \newcommand*{\uni}[2]{\mathsf{U}\!\left({#1,#2}\right)}
    \newcommand*{\tgauss}[2]{\mathsf{N}({#1,#2})}
    \newcommand*{\gaussd}[2]{\mathsf{N}^{\dagger}\!\left({#1,#2}\right)}
    \newcommand*{\modulus}[1]{\M{\left\lvert{#1}\right\rvert}} 
    \newcommand*{\norm}[1]{\M{\left\lVert{#1}\right\rVert}} 
    \newcommand*{\ceil}[1]{\M{\left\lceil{#1}\right\rceil}} 
    \newcommand*{\set}[1]{\M{\left\lbrace{#1}\right\rbrace}} 
    \newcommand*{\setbuilder}[2]{\M{\left\lbrace{#1}\: \big\vert \:{#2}\right\rbrace}}
    \newcommand*{\uniti}{\lbrack 0,1\rbrack}
    \DeclareMathOperator*{\argmax}{argmax}
    \DeclareMathOperator*{\argmin}{argmin}
    \DeclareMathOperator*{\trace}{tr\!}

\journal{Reliability Engineering and System Safety}

\begin{document}
\begin{frontmatter}

    \title{The Coefficient of Determination of a Reduced Order Model}

    \author{Robert A. Milton}
    \ead{r.a.milton@sheffield.ac.uk}

    \author{Solomon F. Brown}
    \ead{s.f.brown@sheffield.ac.uk}

    \author{Aaron S. Yeardley}
    \ead{asyeardley1@sheffield.ac.uk}

    \address{Department of Chemical and Biological Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom}       

    \begin{abstract}
        %% Text of abstract
    \end{abstract}

    \begin{keyword}
        Global Sensitivity Analysis, Sobol' Index, Surrogate Model, Active Subspace
    \end{keyword}

\end{frontmatter}


\section{Introduction}\label{sec:Intro}


\section{Coefficient of Determination}\label{sec:COD}
    Given
    \begin{equation*}
        \T{Integrable } y \colon \uniti^{M+1} \mapsto \st{R}^{L}
    \end{equation*}
    take a random variable (RV)
    \begin{equation*}
        \rv{u} \sim \uni{\te[\mi{M+1}]{0}}{\te[\mi{M+1}]{1}} \deq \uni{0}{1}^{M+1}
    \end{equation*}
    Exponentiation is categorical -- repeated outer cartesian $\x$ or tensor $\otimes$ -- unless otherwise specified. Tensors are square bracketed quantities, their dimensions a von Neumann ordinal subscript, in this case
    \begin{equation*}
        \mi{M+1} \deq (0,\ldots,M) \supseteq \mi{m+1} \deq (0,\ldots,m \leq M)
    \end{equation*}
    with void $\mi{0}$ voiding any tensor it subscripts. Ordinals are concatenated by Cartesian $\times$ and may be subtracted like sets, as in $\mi{M-m} \deq (m,\ldots,M-1)$.
    Expectations and variances will be subscripted by the dimensions of $\rv{u}$ marginalized (integrated over). Conditioning on the remaining dimensions is left implicit after \cref{def:Theory:y_m}, to lighten notation.

    Construct $M+1$ stochastic processes (SPs)
    \begin{equation}\label{def:Theory:y_m}
        \te[\mi{L}]{\rv{y_m}} \deq \ev{y(\rv{u})}{\mi{M-m}} \deq \ev{y(\rv{u}) \big\vert \te[\mi{m}]{\rv{u}}}{\mi{M-m}}
    \end{equation}
    ranging from $\tte[\mi{L}]{\rv{y_0}}$ to $\tte[\mi{L}]{\rv{y_M}}$. Every one of these quietly depends on the ungovernable noise dimension $\tte[M]{\rv{u}} \perp \tte[\mi{M}]{\rv{u}}$, while ignoring governed input dimensions $\tte[\mi{M-m}]{\rv{u}}$. Following Daniell-Kolmogorov \cite{Rogers.Williams2000} pp.124 we may regard an SP as a random function, from which we shall freely extract finite dimensional distributions generated by a design matrix $\tte[\mi{M\x o}]{\rv{u}}$ of $o \in \st{Z}^{+}$ samples.
    Daniell-Kolmogorov incidentally secures $\rv{u}$. 
    Because $y$ is (Lebesgue) integrable it must be measurable, guaranteeing $\tte[\mi{L}]{\rv{y_0}}$.
    Because all probability measures are finite, integrability of $y$ implies integrability of $y^n$ for all $n\geq 1$ \cite{Villani1985}. 
    So Fubini's Theorem \cite{Williams1991} pp.77 allows all expectations to be taken in any order. 
    This is sufficient to ensure every object appearing in this work.

    Our aim is to compare predictions from a reduced model $\rv{y_m}$ with those of the full model $\rv{y_M}$. Correlation between these predictions is squared -- using element-wise (Hadamard) multiplication $\circ$ and division -- to form an RV called the coefficient of determination
    \begin{equation}
        \te[\mi{L}^2]{\rv{R_{m}^{2}}} \deq 
        \frac{\cov[\rv{y_M}]{\rv{y_m}}{\mi{M}} \circ \cov[\rv{y_M}]{\rv{y_m}}{\mi{M}}}
        {\cov{\rv{y_m}}{\mi{m}} \circ \cov{\rv{y_M}}{\mi{M}}} =
        \frac{\cov{\rv{y_m}}{\mi{m}}}{\cov{\rv{y_M}}{\mi{M}}} \deqr
        \te[\mi{L}^2]{\rv{S_m}}
    \end{equation}
    The closed Sobol' index is the complement of the commonplace total Sobol' index
    \begin{equation*}
        \te[\mi{L}^2]{\rv{S_m}} \deqr \te[\mi{L}^2]{1} - \te[\mi{L}^2]{\rv{S^{T}_{M-m}}}
    \end{equation*}
    It has mean value over the ungovernable noise axis of
    \begin{align}\label{def:COD:mean}
        \te[\mi{L}^2]{S_{\mi{m}}} &\deq \evt{\te[]{\rv{S_m}}}{M} = \frac{V_{\mi{m}}}{V_{\mi{M}}} \\            
        \te[\mi{L}^2]{V_{\mi{m}}} &\deq \evt{\; \cov{\rv{y_m}}{\mi{m}}}{M}
    \end{align}
    and variance over the noise axis
    \begin{align}\label{def:COD:variance}
        \te[\mi{L}^4]{T_\mi{m}} &\deq 
        \cov{\rv{S_m}}{M} = \frac{V_{\mi{m}}^{2}}{V_{\mi{M}}^{2}} \circ
        \left(
            \frac{W_{\mi{m},\mi{m}}}{V_{\mi{m}}^{2}}
            -2\frac{W_{\mi{m},\mi{M}}}{{V_{\mi{m}}\otimes V_{\mi{M}}}}
            +\frac{W_{\mi{M},\mi{M}}}{V_{\mi{M}}^{2}}
        \right) \\                
        \te[\mi{L}^4]{W_{\mi{m},\mi{m^{\prime}}}} &\deq \cov[\cov{\rv{y_{m^{\prime}}}}{\mi{m^{\prime}}}]{\cov{\rv{y_{m}}}{\mi{m}}}{M}
    \end{align}
    The remainder of this paper is devoted to calculating these two quantities -- the coefficient of determination and its variance over noise (measurement error, squared).


\section{Stochastic Process Estimates}\label{sec:SPestimates}
    Define noiseless mean predictors (functions of $\tte[\mi{m}]{\rv{u}}$) and their error SPs as
    \begin{equation}
        \begin{aligned}
            \te[\mi{L}]{f_{\mi{m}}} &\deq \evt{\te[\mi{L}]{\rv{y_m}}}{\mi{M+1-m}} \\
            \te[\mi{L}]{\rv{e_m}} &\deq \te[\mi{L}]{\rv{y_m}} - \te[\mi{L}]{f_{\mi{m}}} \\
        \end{aligned}
    \end{equation}
    Adopt as design matrix a triad of inputs, their mutual dependence a function of conditioning, labelled according to how they will be marginalized
    \begin{equation*}
            \te[\mi{M+1}]{\rv{u}}^3 \deq \left\lbrack \rv{u_0},\rv{u_m},\rv{u_M} \right\rbrack
    \end{equation*}
    The triad elicits the response
    \begin{equation*}
        \te[\mi{L}]{\rv{e}}^3 \deq y(\tte[\mi{M+1}]{\rv{u}}^3) - \evt{\;\evt{\;\ev{y(\tte[\mi{M+1}]{\rv{u}}^3)}{M^{\prime\prime}}}{\mi{M^{\prime}+1^{\prime}-m^{\prime}}}}{\mi{M+1}}
    \end{equation*}
    Primes mark independent input axes, otherwise each expectation applies to all three members of $\tte[\mi{M+1}]{\rv{u}}^3$. It is alway obvious whether axes are independent or shared by the trio, but this can be mechanically checked against the measure of integration behind an expectation. Repeated expectations over the same axis are rare here, usually indicating that apparent repetitions must be ``primed''.

    This purpose of the triad is to interrogate its response for moments in respect of shared noise
    \begin{equation*}
        \begin{aligned}
            \te[(\mi{L\x 3})^{n}]{\mu_{n}} &\deq \ev{\te[]{\tte[\mi{L}]{\rv{e}}^{3}}^{n}}{M} \\
            \te[\mi{L\x 3}]{\mu_{1}} &\phantom{:}= \te[\mi{L\x 3}]{0}
        \end{aligned}
    \end{equation*}
    for these embody
    \begin{equation*}
        \te[\mi{L}^{n}]{\mu_{\mi{m^{\prime}\ldots m}^{n\prime}}} \deq \te[(\mi{L\x}i)^{n}]{\mu_{n}} = \ev{\tte[\mi{L}]{\rv{e}_{\mi{m}^{\prime}}}\x\cdots\x\tte[\mi{L}]{\rv{e}_{\mi{m}^{n\prime}}}}{M}
    \end{equation*}
    where $\mi{m}^{i\prime} \in \set{\mi{0},\mi{m},\mi{M}}$. This expression, along with $f_{\mi{m}}$, underpins both quantities we seek. The reduction which follows repeatedly realizes
    \begin{equation}\label{eq:SPestimates:reduction}
        \te[\mi{L}^{n}]{\mu_{\mi{0\ldots 0}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}} \deq 
        \evt{\te[\mi{L}^{n}]{\mu_{\mi{M\ldots M}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}}}{\mi{M}} = 
        \evt{\te[\mi{L}^{n}]{\mu_{\mi{m\ldots m}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}}}{\mi{m}}
    \end{equation}

    The expected variance in \cref{def:COD:mean} amounts to
    \begin{equation}
        \begin{aligned}
            \te[\mi{L}]{V_{\mi{m}}} 
            &= \evt{\;\ev{\te[\mi{L}]{\rv{e_m} + f_{\mi{m}}}^{2}}{M}}{\mi{m}}
            - \ev{\te[\mi{L}]{\rv{e_0} + f_{\mi{0}}}^{2}}{M} \\
            &= \ev{\te[\mi{L}]{f_{\mi{m}}}^{2}}{\mi{m}} - \te[\mi{L}]{f_{\mi{0}}}^{2} + 
            \evt{\te[\mi{L}^2]{\mu_{\mi{mm}}}}{\mi{m}} - \te[\mi{L}^2]{\mu_{\mi{00}}} \\
            &= \ev{\te[\mi{L}]{f_{\mi{m}}}^{2}}{\mi{m}} - \te[\mi{L}]{f_{\mi{0}}}^{2}
        \end{aligned}
    \end{equation}

    The covariance between variances in \cref{def:COD:variance} is
    \begin{equation*}
        \begin{aligned}
            \te[\mi{L}^4]{W_{\mi{m},\mi{m^{\prime}}}} &\deq \cov[\cov{\rv{y_{m^{\prime}}}}{\mi{m^{\prime}}}]{\cov{\rv{y_{m}}}{\mi{m}}}{M} \\
            &\phantom{:}=
            \cov[\ev{\te[\mi{L}]{\rv{y_{m^{\prime}}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m^{\prime}}}]{\ev{\te[\mi{L}]{\rv{y_{m}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m}}}{M} \\
            &\phantom{:}=
            \ev{\ev{\te[\mi{L}]{\rv{y_{m}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m}} \otimes\ev{\te[\mi{L}]{\rv{y_{m^{\prime}}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m^{\prime}}}}{M}\\
            &\phantom{\deq}\  - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}} \\       
            &\phantom{:}= \te[\mi{L}^4]{A_{\mi{m,m^{\prime}}}-A_{\mi{0,m^{\prime}}}-A_{\mi{m,0}}+A_{\mi{0,0}}}
        \end{aligned}
    \end{equation*}
    Here, for any $\mi{m},\mi{m^{\prime}}\subseteq\mi{M}$
    \begin{equation*}
        \begin{aligned}
            \te[\mi{L}^4]{A_{\mi{m},\mi{m^{\prime}}}}
            &\deq \evt{\;\evt{\;\ev{\te[\mi{L}]{\rv{y_{m}}}^{2} \otimes \te[\mi{L}]{\rv{y_{m^{\prime}}}}^{2}}{M}}{\mi{m^{\prime}}}}{\mi{m}} - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}} \\
            &\phantom{:}= \evt{\;\evt{\;\ev{\te[\mi{L}]{\rv{e_{m}}+f_{\mi{m}}}^{2} \otimes \te[\mi{L}]{\rv{e_{m^{\prime}}}+ f_{\mi{m^{\prime}}}}^{2}}{M}}{\mi{m^{\prime}}}}{\mi{m}}
            - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}}
        \end{aligned}
    \end{equation*}
    exploiting the fact that $V_{\mi{0}} = \te[\mi{L}^2]{0}$. \Cref{eq:SPestimates:reduction} cancels all terms beginning with $\te[\mi{L}]{\rv{e_{m}}}^{2}$, first across $A_{\mi{m,m^{\prime}}}-A_{\mi{0,m^{\prime}}}$ then across $A_{\mi{m,0}}-A_{\mi{0,0}}$. All remaining terms ending in $\te[\mi{L}]{f_\mi{m^{\prime}}}^{2}$ are eliminated by $\mu_{1} = 0$ and
    \begin{equation*}
        \evt{\;\ev{\te[\mi{L}]{f_{\mi{m}}}^{2} \otimes \te[\mi{L}]{f_{\mi{m^{\prime}}}}^{2}}{\mi{m^{\prime}}}}{\mi{m}}
        = \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}}
    \end{equation*}
    Clearly, the same arguments eliminate $\te[\mi{L}]{\rv{e_{m^{\prime}}}}^{2}$ and $\te[\mi{L}]{f_\mi{m}}^{2}$.
    Finally then
    \begin{equation}
        \te[\mi{L}^4]{W_{\mi{m},\mi{m^{\prime}}}} = \sum_{\pi(\mi{L}^{2})} \sum_{\pi(\mi{L^{\prime}}^{2})}
        \te[\mi{L}^{2} \x \mi{L^{\prime}}^{2}]{f_{\mi{m}} \otimes \mu_{\mi{mm^{\prime}}} \otimes f_{\mi{m^{\prime}}}}
    \end{equation}
    where each summation is over permutations of tensor axes
    \begin{equation*}
        \pi(\mi{L}^{2}) = \set{(\mi{L}\x\mi{L^{\prime\prime}}), (\mi{L^{\prime\prime}}\x\mi{L})} \QT{;} \pi(\mi{L^{\prime}}^{2}) = \set{(\mi{L^{\prime}}\x\mi{L^{\prime\prime\prime}}), (\mi{L^{\prime\prime\prime}}\x\mi{L^{\prime}})}
    \end{equation*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{elsarticle-num} 
\bibliography{master}
\end{document}
\endinput
