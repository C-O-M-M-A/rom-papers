\documentclass[preprint,12pt]{elsarticle}
    \usepackage{algorithm}
    \usepackage{algorithmic}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \usepackage{amssymb}
    \usepackage{amsmath}
    \usepackage[hidelinks]{hyperref}
    \usepackage[capitalize]{cleveref}
    \usepackage{xspace} 
    \usepackage{ifthen} 
    \usepackage{csvsimple}
    \setlength {\marginparwidth }{2cm}
    \usepackage{todonotes}
    \usepackage{float}
    \newcommand*{\M}[1]{\ensuremath{#1}\xspace} 
    \newcommand*{\tr}[1]{\M{#1}}
    \newcommand*{\x}{\times}
    \newcommand*{\mi}[1]{\mathbf{#1}} 
    \newcommand*{\st}[1]{\mathbb{#1}} 
    \newcommand*{\rv}[1]{\mathsf{#1}} 
    \newcommand*{\te}[2][]{\left\lbrack{#2}\right\rbrack_{#1}}
    \newcommand*{\tte}[2][]{\lbrack{#2}\rbrack_{#1}}
    \newcommand*{\tse}[2][]{\mi{\lbrack#2\rbrack}_{#1}}
    \newcommand*{\tme}[3][]{\lbrack{#3}\rbrack_{\tse[#1]{#2}}}
    \newcommand*{\diag}[2][]{\left\langle{#2}\right\rangle_{#1}}
    \newcommand*{\prob}[3]{\M{\mathrm{p}\!\left(\left.{#1}\right\vert{#2,#3}\right)}} 
    \newcommand*{\deq}{\M{\mathrel{\mathop:}=}} 
    \newcommand*{\deqr}{\M{=\mathrel{\mathop:}}} 
    \newcommand{\T}[1]{\text{#1}} 
    \newcommand*{\QT}[2][]{\M{\quad\T{#2}\ifthenelse{\equal{#1}{}}{\quad}{#1}}} 
    \newcommand*{\ev}[3][]{\mathbb{E}_{#3}^{#1}\!\left\lbrack{#2}\right\rbrack}
    \newcommand*{\evt}[3][]{\mathbb{E}_{#3}^{#1}\!#2}
    \newcommand*{\cov}[3][]{\ifthenelse{\equal{#1}{}}{\mathbb{V}_{#3}\!\left\lbrack{#2}\right\rbrack}{\mathbb{V}_{#3}\!\left\lbrack{#2,#1}\right\rbrack}}
    \newcommand*{\covt}[2]{\mathbb{V}_{#2}\!{#1}}
    \newcommand*{\gauss}[2]{\mathsf{N}\!\left({#1,#2}\right)}
    \newcommand*{\uni}[2]{\mathsf{U}\!\left({#1,#2}\right)}
    \newcommand*{\tgauss}[2]{\mathsf{N}({#1,#2})}
    \newcommand*{\gaussd}[2]{\mathsf{N}^{\dagger}\!\left({#1,#2}\right)}
    \newcommand*{\modulus}[1]{\M{\left\lvert{#1}\right\rvert}} 
    \newcommand*{\norm}[1]{\M{\left\lVert{#1}\right\rVert}} 
    \newcommand*{\ceil}[1]{\M{\left\lceil{#1}\right\rceil}} 
    \newcommand*{\set}[1]{\M{\left\lbrace{#1}\right\rbrace}} 
    \newcommand*{\setbuilder}[2]{\M{\left\lbrace{#1}\: \big\vert \:{#2}\right\rbrace}}
    \newcommand*{\uniti}{\lbrack 0,1\rbrack}
    \DeclareMathOperator*{\argmax}{argmax}
    \DeclareMathOperator*{\argmin}{argmin}
    \DeclareMathOperator*{\trace}{tr\!}

\journal{Reliability Engineering and System Safety}

\begin{document}
\begin{frontmatter}

    \title{Generalized Sobol' indices for multi-output regression models}

    \author{Robert A. Milton}
    \ead{r.a.milton@sheffield.ac.uk}

    \author{Solomon F. Brown}
    \ead{s.f.brown@sheffield.ac.uk}

    \author{Aaron S. Yeardley}
    \ead{asyeardley1@sheffield.ac.uk}

    \address{Department of Chemical and Biological Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom}       

    \begin{abstract}
        Variance based global sensitivity usually measures the relevance of inputs to a single output using Sobol' indices. This paper extends the definition in a natural way to multiple outputs, directly measuring the relevance of inputs to the linkages between outputs in a correlation-like matrix of indices. The usual Sobol' indices constitute the diagonal of this matrix. Existence, uniqueness and uncertainty quantification are established by developing the indices from a putative regression model. Analytic expressions for generalized Sobol' indices and their standard errors are computed for Gaussian Process regression with an anisotropic radial basis function kernel. The formulae allow for rotation of the inputs to facilitate locating an active subspace.
    \end{abstract}

    \begin{keyword}
        Global Sensitivity Analysis, Sobol' Index, Surrogate Model, Multi-Output, Gaussian Process
    \end{keyword}

\end{frontmatter}

\section{Introduction}\label{sec:Intro}
    This paper is concerned with analysing the results of experiments or computer simulations in a design matrix of $M\geq 1$ input plus $L\geq 1$ output columns, over $N$ rows (datapoints). Global Sensitivity Analysis (GSA) \cite{Razavi2021} examines the relevance of the various inputs to the various outputs. When pursued via ANOVA decomposition, this leads naturally to the well known Sobol' indices, which have by now been applied across most fields of science and engineering \cite{Saltelli2019,Ghanem2017}. 

    The Sobol' decomposition apportions the variance of an output to sets of one or more inputs \cite{Sobol2001}. We shall use ordinals of inputs $\mi{m}\deq(0,\ldots ,m-1) \subseteq \mi{M}$, tuples which are conveniently also naive sets. The maximal ordinal $\mi{M}$ of all $M$ inputs explains everything explicable, so its Sobol' index is 1 by definition. The void ordinal $\mi{0}$ explains nothing, so its Sobol' index is 0 by definition. 
    The influence of an isolated ordinal of inputs $\mi{m}$ is measured by its closed Sobol' index $S_{\mi{m}} \in \lbrack 0,1\rbrack$. A first-order Sobol' index $S_{m^{\prime}}$ is simply the closed Sobol' index of a single input $m^{\prime}$.
    Because inputs in an isolated ordinal may act in concert with each other, the influence of the ordinal often exceeds the sum of first-order contributions from its members, always obeying 
    \begin{equation*}
        S_{\mi{m}} \geq \sum_{m^{\prime} \in \mi{m}} S_{m^{\prime}}
    \end{equation*}

    The total Sobol index $S^{T}_{\mi{M-m}} \geq 0$ of the set theoretic complement $\mi{M-m}$ is $1-S_{\mi{m}}$, which expresses the influence of non-isolated inputs $\mi{M-m}$ allowed to act in concert with each other \emph{and} isolated inputs $\mi{m}$. When speaking of irrelevant inputs $\mi{M-m}$, we mean that $S^{T}_{\mi{M-m}} \approx 0$. This is synonymous with the isolated ordinal of inputs $\mi{m}$ explaining everything explicable $S_{\mi{m}}\approx 1$.
    It is apparent that we can readily obtain any Sobol' index of interest by ordering input dimensions appropriately and calculating the closed index $S_{\mi{m}}$ of some ordinal set $\mi{m}$.

    Perhaps the most significant use of closed Sobol' indices is to determine a reduced model with $m\leq M$ inputs within the full model $\mi{M}$.
    Apportioning variance is mathematically equivalent to squaring a correlation coefficient to produce a coefficient of determination $R^{2}$ \cite{Chicco2021}. A closed Sobol' index is thus a coefficient of determination between the predictions from the reduced model $\mi{m}$ and predictions from the full model $\mi{M}$. A closed Sobol' index close to 1 confirms that the two models make nearly identical predictions. Simplicity and economy (not least of calculation) motivate the adoption of a reduced model, a closed Sobol' index close to 1 permits it.

    The discussion thus far, and almost all prior GSA, has dealt with a single (i.e scalar) output. With multiple (i.e vector) outputs, the Sobol' decomposition apportions the covariance matrix of outputs rather than the variance of a single output. With $L$ outputs, the closed Sobol' index $S_{\mi{m}}$ is generally a symmetric $\mi{L}\x \mi{L}$ matrix. The diagonal elements express the relevance of inputs to the output variables themselves. The off-diagonal elements express relevance to the linkages between outputs. This may be of considerable interest when outputs are, for example, yield and purity of a product, or perhaps a single output measured at various times. The Sobol indices reveal (amongst other things) which inputs it is worthwhile varying in an effort to alter the linkages between outputs. Prior work on Sobol' indices with multiple outputs \cite{Gamboa.etal2013,Xiao2017,GarciaCabrejo2014} has settled ultimately on just the diagonal elements of the covariance matrix, so this linkage remains unexamined. Although output covariance has been incoroporated indirectly in prior studies by performing principal component analysis (PCA) on ouputs prior to GSA on the (diagonal) variances of the resulting output basis \cite{Campbell2006}. This has been used in particular to study synthetic ``multi-outputs'' which are actually the dynamic response of a single output over time \cite{Lamboni2011, Zhang2020}.

    Accurate calculation of Sobol' indices even for a single output is computationally expensive and requires 10,000+ datapoints \cite{Lamoureux.etal2014}. A (sometimes) more efficient approach is calculation via a surrogate model, such as Polynomial Chaos Expansion (PCE) \cite{Ghanem.Spanos1997,Xiu.Karniadakis2002,Xiu2010}, low-rank tensor approximation \cite{Chevreuil.etal2015,Konakli.Sudret2016}, and support vector regression \cite{Cortes.Vapnik1995}. As well as being efficient, surrogate models also smooth out noise in the output, which is often highly desirable in practice. This paper employs one of the most popular surrogates, the Gaussian Processes (GP) \cite{Sacks.etal1989, Rasmussen.Williams2005} as it is highly tractable. We shall follow the multi-output form (MOGP) described in \cite{Alvarez.etal2011}, in order to examine the linkages between outputs.
    This paper deals exclusively with the anisotropic Radial Basis Function kernel, known as RBF/ARD, which is widely accepted as the kernel of choice for smooth outputs \cite{Rasmussen2016}. This uses the classic Gaussian bell curve to express the proximity of two input points, described in detail in \cref{sub:GPR:Tensor,sub:GPR:Prior}.

    Semi-analytic expressions for Sobol' indices are available for scalar PCEs \cite{Sudret2008}, and the diagonal elements of multi-output PCEs \cite{GarciaCabrejo2014}.
    Semi-analytic expressions for Sobol' indices of GPs have been provided in integral form by \cite{Oakley.OHagan2004} and alternatively by \cite{Chen.etal2005}. These approaches are implemented, examined and compared in \cite{Marrel.etal2009,Srivastava.etal2017}. Both \cite{Oakley.OHagan2004,Marrel.etal2009} estimate the errors on Sobol' indices in semi-analytic, integral form. Fully analytic, closed form expressions have been derived without error estimates for uniformly distributed inputs \cite{Wu.etal2016a} with an RBF kernel. There are currently no closed form expressions for MOGPs, or the errors on Sobol' indices, or any GPs for which inputs are not uniformly distributed. 

    In this paper we provide explicit, closed-form analytic formulae for the $\mi{L}\x \mi{L}$ matrices of closed Sobol' indices and their errors, for a class of MOGP with an RBF/ARD kernel applicable to smoothly varying outputs. 
    We transform uniformly distributed inputs $u$ to normally distributed inputs $z$ prior to fitting a GP and performing analytic calculation of closed Sobol' indices. This leads to relatively concise expressions in terms of exponentials, and enables ready calculation of the errors (variances) of these expressions. It also allows for an arbitrary rotation $\Theta$ of inputs, as normal variables are additive, whereas summing uniform inputs does not produce uniform inputs. If the goal is reducing inputs, rotating their basis first boosts the possibilities immensely \cite{Constantine2015}. It presents the possibilty of choosing $\Theta$ to maximise the closed Sobol' index of the first few inputs.

    The quantities to be calculated and their formal context are introduced in \cref{sec:GSI}, assuming only that the output is an integrable function of the input. Our approach effectively regards a regression model which quantifies uncertainty with each prediction as just another name for a stochastic process. A great deal of progress is made in \cref{sec:SPEst} using general stochastic (not necessarily Gaussian) processes. This approach is analytically cleaner, as it is not obfuscated by the GP details. Furthermore, it turns out that the desirable properties of the Gaussian (lack of skew, simple kurtosis) are not actually helpful, as these terms cancel of their own accord. This development leaves just two terms to be calculated, which require the stochastic process to be specified. MOGPs with an RBF/ARD kernel are tersely developed and described in \cref{sec:GPR}, then used to calculate the two unknown terms in \cref{sec:GPMom,sec:GPEst}. Methods to reduce computational complexity are discussed in \cref{sec:Complexity}.Conclusions are drawn in \cref{sec:Conc}.


\section{Generalized Sobol' indices}\label{sec:GSI}
    Apply a constant offset to a Lebesgue integrable model
    \begin{equation} \label{def:GSI:y}
        y \colon \uniti^{M+1} \rightarrow \st{R}^{L} \QT{to obey} \int y(u) \; \mathrm{d}u = \te[\mi{L}]{0}
    \end{equation}
    taking as input a uniformly distributed random variable (RV)
    \begin{equation} \label{def:GSI:u}
        \rv{u} \sim \uni{\te[\mi{M+1}]{0}}{\te[\mi{M+1}]{1}} \deq \uni{0}{1}^{M+1}
    \end{equation}
    Throughout this paper exponentiation is categorical -- repeated cartesian $\x$ or tensor $\otimes$ -- unless otherwise specified. Square bracketed quantities are tensors, carrying their axes as a subscript tuple. In this case the subscript tuple is the (von Neumann) ordinal
    \begin{equation*}
        \mi{M+1} \deq (0,\ldots,M) \supset \mi{m} \deq (0,\ldots,m-1 \leq M-1)
    \end{equation*}
    with void $\mi{0}\deq ()$ voiding any tensor it subscripts. Ordinals are concatenated into tuples by Cartesian $\times$ and will be subtracted like sets, as in $\mi{M-m} \deq (m,\ldots,M-1)$. 
    Subscripts refer to the tensor prior to any superscript operation, so $\te[\mi{L}]{y(\rv{u})}^{2}$ is an $\mi{L}^{2} \deq \mi{L\x L}$ tensor, for example.
    The preference throughout this work is for uppercase constants and lowercase variables, in case of ordinals the lowercase ranging over the uppercase. We prefer $o$ for an unbounded positive integer, avoiding O.

    Expectations and variances will be subscripted by the dimensions of $\rv{u}$ marginalized. Conditioning on the remaining inputs is left implicit after \cref{def:Theory:y_m}, to lighten notation.
    Now, construct $M+1$ stochastic processes (SPs)
    \begin{equation}\label{def:Theory:y_m}
        \te[\mi{L}]{\rv{y_m}} \deq \ev{y(\rv{u})}{\mi{M-m}} \deq \ev{y(\rv{u}) \big\vert \te[\mi{m}]{u}}{\mi{M-m}}
    \end{equation}
    ranging from $\tte[\mi{L}]{\rv{y_0}}$ to $\tte[\mi{L}]{\rv{y_M}}$. Every SP depends stochastically on the ungoverned noise dimension $\tte[M]{\rv{u}} \perp \tte[\mi{M}]{\rv{u}}$ and deterministically on the first $m$ governed inputs $\te[\mi{m}]{u}$, marginalizing the remaining inputs $\tte[\mi{M-m}]{\rv{u}}$. 
    Sans serif symbols such as $\rv{u,y}$ generally refer to RVs and SPs, italic $u,y$ being reserved for (tensor) functions and variables. Each SP is simply a regression model for $y$ on the first $m$ dimensions of $u$.
    
    Following the Kolmogorov extension theorem \cite[pp.124]{Rogers.Williams2000} we may regard an SP as a random function, from which we shall freely extract finite dimensional distributions generated by a design matrix $\tte[\mi{M\x o}]{u}$ of $o \in \st{Z}^{+}$ input samples.
    The Kolmogorov extension theorem incidentally secures $\rv{u}$. 
    Because $y$ is (Lebesgue) integrable it must be measurable, guaranteeing $\tte[\mi{L}]{\rv{y_0}}$.
    Because all probability measures are finite, integrability of $y$ implies integrability of $y^n$ for all $n \in \st{Z}^{+}$ \cite{Villani1985}. 
    So Fubini's Theorem \cite[pp.77]{Williams1991} allows all expectations to be taken in any order. These observations suffice to secure every object appearing in this work. Changing the order of expectations, as permitted by Fubini's Theorem, is the vital tool in the construction of this work. 

    Our aim is to compare predictions from a reduced regression model $\rv{y_m}$ with those from the full regression model $\rv{y_M}$. Correlation between these predictions is squared -- using element-wise (Hadamard) multiplication $\circ$ and division $/$ -- to form an RV called the coefficient of determination
    \begin{equation}
        \te[\mi{L}^2]{\rv{R_{mM}^{2}}} \deq 
        \frac{\cov[\rv{y_M}]{\rv{y_m}}{\mi{M}} \circ \cov[\rv{y_M}]{\rv{y_m}}{\mi{M}}}
        {\cov{\rv{y_m}}{\mi{m}} \circ \cov{\rv{y_M}}{\mi{M}}} =
        \frac{\cov{\rv{y_m}}{\mi{m}}}{\cov{\rv{y_M}}{\mi{M}}}
    \end{equation}
    However, this is undefined whenever $\covt{\;\tte[l\x l^{\prime}]{\rv{y_M}}}{\mi{M}} = 0$, obscuring potentially useful information about $\covt{\;\tte[l\x l^{\prime}]{\rv{y_m}}}{\mi{m}}$. Introducing 1-tensors representing the square root of the diagonal of a covariance matrix
    \begin{equation}
        \te[l]{\sqrt{\cov[\cdot]{\cdot}{}_{\mi{L}^{2}}}} \deq \cov[\cdot]{\cdot}{}_{l^2}^{1/2}
    \end{equation}
    the correlation coefficient between output dimensions is
    \begin{equation}
        \te[\mi{L\x L^{\prime}}]{\rv{R_{m}}} \deq \frac{\cov{\rv{y_m}}{\mi{m}}_{\mi{L\x L^{\prime}}}} {\sqrt{\covt{\,\tte[\mi{L}^2]{\rv{y_m}}}{\mi{m}}}\otimes\sqrt{\covt{\,\tte[\mi{L^{\prime}}^2]{\rv{y_m}}}{\mi{m}}}}
        \ \ \forall \mi{m}\subseteq \mi{M}
    \end{equation}
    Let us define the multi-output closed Sobol' index as the product of the full correlation between output dimensions and the coefficient of determination
    \begin{equation}
            \te[\mi{L\x L^{\prime}}]{\rv{S_m}} \deq \te[\mi{L\x L^{\prime}}]{\rv{R_{M}}} \circ \te[\mi{L\x L^{\prime}}]{\rv{R_{mM}^{2}}} 
    \end{equation}
    and the multi output total Sobol' index as its complement
    \begin{equation}
        \te[\mi{L\x L^{\prime}}]{\rv{S^{T}_{M-m}}} \deq \te[\mi{L\x L^{\prime}}]{\rv{R_M}} - \te[\mi{L\x L^{\prime}}]{\rv{S_m}}
    \end{equation}
    These definitions coincide precisely with the traditional Sobol' index along the diagonal $\sqrt{\tte[\mi{L}^{2}]{\rv{S_m}}} \circ \sqrt{\tte[\mi{L}^{2}]{\rv{S_m}}}$, which has been very much the focus of prior literature \cite{Gamboa.etal2013,Xiao2017,GarciaCabrejo2014}. The off-diagonal elements are bound by the diagonal as
    \begin{equation}
        -\te[l^2]{\rv{S_m}}^{1/2}\te[l^{\prime 2}]{\rv{S_m}}^{1/2} \leq
            \te[l\x l^{\prime}]{\rv{S_m}} = 
            \te[l\x l^{\prime}]{\rv{R_{m}}}\te[l^2]{\rv{S_m}}^{1/2}\te[l^{\prime 2}]{\rv{S_m}}^{1/2} \leq \te[l^2]{\rv{S_m}}^{1/2}\te[l^{\prime 2}]{\rv{S_m}}^{1/2}
    \end{equation}
    To calculate moments over ungoverned noise we use the Taylor series method \cite[pp.353]{Kendall1994}, which is valid provided $\covt{\,\te[l^{2}]{\rv{y_M}}}{\mi{M}}$ is well approximated by its mean
    \begin{equation}
        \te[l^{2}]{V_{\mi{M}}} \deq \evt{\,\covt{\,\te[l]{\rv{y_M}}}{\mi{M}}}{M} \gg \big\vert\covt{\,\te[l^{2}]{\rv{y_M}}}{\mi{M}} - \te[l^{2}]{V_{\mi{M}}}\big\vert
    \end{equation}
    This provides the mean Sobol' index
    \begin{align}\label{def:GSI:mean}
        \te[\mi{L\x L^{\prime}}]{S_{\mi{m}}} &\deq \evt{\te[\mi{L\x L^{\prime}}]{\rv{S_m}}}{M}
        = \frac{\tte[\mi{L\x L^{\prime}}]{V_{\mi{m}}}}{\sqrt{\tte[\mi{L}^{2}]{V_{\mi{M}}}} \otimes \sqrt{\tte[\mi{L^{\prime}}^2]{V_{\mi{M}}}}} \\            
        \T{where }\te[\mi{L}^2]{V_{\mi{m}}} &\deq \evt{\; \cov{\rv{y_m}}{\mi{m}}}{M} \quad \forall \mi{m}\subseteq \mi{M}
    \end{align}
    with uncertainty due to ungoverned noise of
    \begin{align}\label{def:GSI:variance}
        \te[(\mi{L\x L^{\prime}})^2]{T_\mi{m}} &\deq 
        \covt{\,\te[\mi{L\x L^{\prime}}]{\rv{S_m}}}{M} = \frac{\te[(\mi{L\x L^{\prime}})^{2}]{W_{\mi{mm}}}}{\te[\mi{L}^{2}]{V_{\mi{M}}}^{2/2} \otimes \te[\mi{L^{\prime}}^{2}]{V_{\mi{M}}}^{2/2}} \\
        \T{where }\te[\mi{L}^4]{W_{\mi{mm^{\prime}}}} &\deq \cov[\cov{\rv{y_{m^{\prime}}}}{\mi{m^{\prime}}}]{\cov{\rv{y_{m}}}{\mi{m}}}{M} \ \ \forall \mi{m,m^{\prime}} \subseteq \mi{M}
    \end{align}
    where improper fractions exponentiate a square root diagonal of $V_{\mi{M}}$.
    In practice it is best to retain only the term in $W_{\mi{mm}}$, ignoring the uncertainty in $V_{\mi{M}}$ conveyed by $W_{\mi{mM}},W_{\mi{MM}}$. This is primarily because one is normally interested in adequate reduced models, for which $V_{\mi{m}} \approx V_{\mi{M}}$ implies a drastically vanishing uncertainty when $\tte[(l\x l^{\prime})^{2}]{T_\mi{m}}$ includes the correction term
    \begin{multline}\label{eq:GSI:correction}
        \left(
            \frac{\te[l^{4}]{W_\mi{MM}}}{4\te[l^{2}]{V_\mi{M}}^{4/2}} + 
            \frac{\te[l^{2}\x l^{\prime 2}]{W_\mi{MM}}}{2\te[l^{2}]{V_\mi{M}}^{2/2} \otimes \te[l^{\prime 2}]{V_\mi{M}}^{2/2}} + 
            \frac{\te[l^{\prime 4}]{W_\mi{MM}}}{4\te[l^{\prime}]{V_\mi{M}}^{4/2}} \right. \\
        \left. 
            -\frac{\te[l\x l^{\prime}\x l^{2}]{W_\mi{mM}}}{\te[l\x l^{\prime}]{V_\mi{m}} \otimes \te[l^{2}]{V_\mi{M}}^{2/2}}
            -\frac{\te[l\x l^{\prime}\x l^{\prime 2}]{W_\mi{mM}}}{\te[l\x l^{\prime}]{V_\mi{m}} \otimes \te[l^{\prime 2}]{V_\mi{M}}^{2/2}} \right) \circ \te[l\x l^{\prime}]{S_{\mi{m}}}^{2}
    \end{multline}
    The remainder of this paper is devoted to calculating these two quantities -- the coefficient of determination and $S_{\mi{m}}$ its variance over ungoverned noise (i.e. measurement uncertainty, squared) $T_{\mi{m}}$.


\section{Stochastic Process estimates}\label{sec:SPEst}
    The central problem in calculating errors on Sobol' indices is that they involve ineluctable covariances between differently marginalized SPs, via their moments over ungoverned noise. But marginalization and moment determination are both a matter of taking expectations. So the ineluctable can be avoided by reversing the order of expectations -- taking moments over ungoverned noise, then marginalizing.
    To this end, adopt as design matrix a triad of inputs to condition $\te[\mi{M+1\x 3}]{\rv{u}}$, eliciting the response
    \begin{equation}\label{def:SPEst:y}
        \te[\mi{L\x 3}]{\rv{y}} \deq 
        \evt{\;\evt{\;\ev{y(\tte[\mi{(M+1)\x 3}]{\rv{u}}) 
            \big\vert \te[]{\te[\mi{0}]{u}, \te[\mi{m^{\prime}}]{u}, \te[\mi{M^{\prime\prime}}]{u}}}{\mi{0^{\prime\prime}}}}
        {\mi{M^{\prime}-m^{\prime}}}}{\mi{M}}
    \end{equation}
    Primes mark independent inputs, otherwise expectations are shared by all three members of the triad. It is not always obvious whether inputs are independent or shared by the triad, but this can be mechanically checked against the measure of integration behind an expectation. Repeated expectations over the same axis are rare here, usually indicating that apparent repetitions must be ``primed''. The purpose of the triad is to interrogate its response for moments in respect of ungoverned noise (which is shared by the triad members)
    \begin{equation}\label{def:SPEst:mu}
            \te[(\mi{L\x 3})^{n}]{\mu_{n}} \deq \ev{\tte[\mi{L\x 3}]{\rv{y}}^{n}}{M} \ \ \forall n \in \st{Z}^{+}
    \end{equation}
    for these embody
    \begin{equation*}
        \te[\mi{L}^{n}]{\mu_{\mi{m^{\prime}\ldots m}^{n\prime}}} \deq \te[\prod_{j=1}^{n}(\mi{L\x}i_{j})]{\mu_{n}} = \ev{\tte[\mi{L}]{\rv{y}_{\mi{m}^{\prime}}}\otimes\cdots\otimes\tte[\mi{L}]{\rv{y}_{\mi{m}^{n\prime}}}}{M}
    \end{equation*}
    where $i_{j}\in \mi{3}$ corresponds to $\mi{m}^{j\prime} \in \set{\mi{0},\mi{m},\mi{M}}$. This expression underpins the quantities we seek. The reduction which follows repeatedly realizes
    \begin{equation}\label{eq:SPEst:reduction}
        \te[\mi{L}^{n}]{\mu_{\mi{0\ldots 0}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}} \deq 
        \evt{\te[\mi{L}^{n}]{\mu_{\mi{M\ldots M}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}}}{\mi{M}} = 
        \evt{\te[\mi{L}^{n}]{\mu_{\mi{m\ldots m}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}}}{\mi{m}}
    \end{equation}
    and that $y$ was offset in \cref{def:GSI:y} to obey
    \begin{equation} \label{eq:SPEst:mu0}
        \te[\mi{L}]{\mu_{\mi{0}}} = \te[\mi{L}]{0}
    \end{equation}
    Defining
    \begin{equation}\label{def:SPEst:e}
        \te[\mi{L\x 3}]{\rv{e}} \deq \rv{y} - \mu_{1}
    \end{equation}
    the expected conditional variance in \cref{def:GSI:mean} amounts to
    \begin{equation}\label{eq:SPEst:V}
        \begin{aligned}
            \te[\mi{L}^{2}]{V_{\mi{m}}} 
            &= \evt{\;\ev{\te[\mi{L}]{\rv{e_m} + \mu_{\mi{m}}}^{2}}{M}}{\mi{m}}
            - \ev{\te[\mi{L}]{\rv{e_0} + \mu_{\mi{0}}}^{2}}{M} \\
            &= \ev{\te[\mi{L}]{\mu_{\mi{m}}}^{2}}{\mi{m}} - \te[\mi{L}]{\mu_{\mi{0}}}^{2} + 
            \evt{\te[\mi{L}^2]{\mu_{\mi{mm}}}}{\mi{m}} - \te[\mi{L}^2]{\mu_{\mi{00}}} \\
            &= \ev{\te[\mi{L}]{\mu_{\mi{m}}}^{2}}{\mi{m}}
        \end{aligned}
    \end{equation}
    and the covariance between conditional variances in \cref{def:GSI:variance} is
    \begin{equation*}
        \begin{aligned}
            \te[\mi{L}^4]{W_{\mi{mm^{\prime}}}} &\deq \cov[\cov{\rv{y_{m^{\prime}}}}{\mi{m^{\prime}}}]{\cov{\rv{y_{m}}}{\mi{m}}}{M} \\
            &\phantom{:}=
            \cov[\ev{\te[\mi{L}]{\rv{y_{m^{\prime}}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m^{\prime}}}]{\ev{\te[\mi{L}]{\rv{y_{m}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m}}}{M} \\
            &\phantom{:}=
            \ev{\ev{\te[\mi{L}]{\rv{y_{m}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m}} \otimes\ev{\te[\mi{L}]{\rv{y_{m^{\prime}}}}^{2} - \te[\mi{L}]{\rv{y_{0}}}^{2}}{\mi{m^{\prime}}}}{M}\\
            &\phantom{\deq}\  - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}} \\       
            &\phantom{:}= \te[\mi{L}^4]{A_{\mi{mm^{\prime}}}-A_{\mi{0m^{\prime}}}-A_{\mi{m0}}+A_{\mi{00}}}
        \end{aligned}
    \end{equation*}
    Here, the inputs within any $\mi{m},\mi{m^{\prime}}\subseteq\mi{M}$ clearly vary independently, and
    \begin{equation*}
        \begin{aligned}
            \te[\mi{L}^4]{A_{\mi{mm^{\prime}}}}
            &\deq \evt{\;\evt{\;\ev{\te[\mi{L}]{\rv{y_{m}}}^{2} \otimes \te[\mi{L}]{\rv{y_{m^{\prime}}}}^{2}}{\mi{m}}}{\mi{m^{\prime}}}}{M} - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}} \\
            &\phantom{:}= \evt{\;\evt{\;\ev{
                \te[\mi{L}]{\rv{e_{m}}+\mu_{\mi{m}}}^{2} \otimes \te[\mi{L}]{\rv{e_{m^{\prime}}}+ \mu_{\mi{m^{\prime}}}}^{2} - 
                \te[\mi{L}]{\mu_{\mi{m}}}^{2} \otimes \te[\mi{L}]{\mu_{\mi{m^{\prime}}}}^{2}
            }{M}}{\mi{m^{\prime}}}}{\mi{m}}
        \end{aligned}
    \end{equation*}
    exploiting the fact that $V_{\mi{0}} = \te[\mi{L}^2]{0}$. \Cref{eq:SPEst:reduction} cancels all terms beginning with $\te[\mi{L}]{\rv{e_{m}}}^{2}$, first across $A_{\mi{mm^{\prime}}}-A_{\mi{0m^{\prime}}}$ then across $A_{\mi{m0}}-A_{\mi{00}}$. All remaining terms ending in $\te[\mi{L}]{\mu_{\mi{m^{\prime}}}}^{2}$ are eliminated by centralization $\evt{\,\tte[]{\rv{e_{m}}}}{M} = 0$.
    Similar arguments eliminate $\te[\mi{L}]{\rv{e_{m^{\prime}}}}^{2}$ and $\te[\mi{L}]{\mu_{\mi{m}}}^{2}$.
    Effectively then
    \begin{equation*}
        \te[\mi{L}^4]{A_{\mi{mm^{\prime}}}} = \sum_{\pi(\mi{L}^{2})} \sum_{\pi(\mi{L^{\prime}}^{2})}
        \evt{\;\evt{\te[\mi{L}^{2} \x \mi{L^{\prime}}^{2}]{\mu_{\mi{m}} \otimes \mu_{\mi{mm^{\prime}}} \otimes \mu_{\mi{m^{\prime}}}}}{\mi{m^{\prime}}}}{\mi{m}}
    \end{equation*}
    so \cref{eq:SPEst:mu0} entails
    \begin{equation}\label{eq:SPEst:W}
        \te[\mi{L}^4]{W_{\mi{mm^{\prime}}}} = \sum_{\pi(\mi{L}^{2})} \sum_{\pi(\mi{L^{\prime}}^{2})}
        \evt{\;\evt{\te[\mi{L}^{2} \x \mi{L^{\prime}}^{2}]{\mu_{\mi{m}} \otimes \mu_{\mi{mm^{\prime}}} \otimes \mu_{\mi{m^{\prime}}}}}{\mi{m^{\prime}}}}{\mi{m}}
    \end{equation}
    where each summation is over permutations of tensor axes
    \begin{equation*}
        \pi(\mi{L}^{2}) \deq \set{(\mi{L}\x\mi{L^{\prime\prime}}), (\mi{L^{\prime\prime}}\x\mi{L})} \QT{;} \pi(\mi{L^{\prime}}^{2}) \deq \set{(\mi{L^{\prime}}\x\mi{L^{\prime\prime\prime}}), (\mi{L^{\prime\prime\prime}}\x\mi{L^{\prime}})}
    \end{equation*}
    Primes on constants are for bookeeping purposes only ($\mi{L}^{j\prime} = \mi{L}$ always), they do not change the value of the constant -- unlike primes on variables ($\mi{m}^{j\prime}$ need not equal $\mi{m}$ in general). One is normally only interested in variances (errors), constituted by the diagonal $\mi{L^{\prime}}^{2}=\mi{L}^{2}$, for which the summation in \cref{eq:SPEst:W} is over a pair of transposed pairs.

    In order to further elucidate these estimates, we must fill in the details of the underlying SPs, sufficiently identifying the regression $\rv{y}$ by its first two moments $\mu_{1}, \mu_{2}$. Then the Sobol' indices are given by \cref{def:GSI:mean,eq:SPEst:V}, and their uncertainties by \cref{def:GSI:variance,eq:SPEst:W}.


\section{Interlude: Gaussian Process regression} \label{sec:GPR}
    The development in this Section is based on \cite{Alvarez.etal2011}, with slightly different notation. A GP over $x$ is formally defined and specified by
    \begin{equation*}
        \te[\mi{L}]{\rv{y_M}} \big\vert \te[\mi{M}\x\mi{o}]{x} \sim 
        \gaussd{\te[\mi{L}\x\mi{o}]{\bar{y}(x)}}{\te[(\mi{L}\x\mi{o})^{2}]
        {k_{\rv{y}}(x,x)}} \quad \forall o \in \st{Z^{+}}
    \end{equation*}
    where tensor ranks concatenate into a multivariate normal distribution
    \begin{equation*}
        \begin{aligned}
            \te[\mi{L}\x\mi{o}]{} \sim \gaussd{\te[\mi{L}\x\mi{o}]{}}{\te[(\mi{L\x o})^{2}]{}}
            & \Longleftrightarrow
            \te[\mi{L}\x\mi{o}]{}^{\dagger} \sim \gauss{\te[\mi{L}\x\mi{o}]{}^{\dagger}}{\te[(\mi{L\x o})^{2}]{}^{\dagger}} \\
            \te[\mi{lo}-\mi{(l-1)o}]{\te[\mi{L}\x\mi{o}]{}^{\dagger}} 
            &\deq \te[(l-1)\x\mi{o}]{} \\
            \te[(\mi{lo}-(\mi{l-1})\mi{o}) \x (\mi{l^{\prime}o}-\mi{(l^{\prime}-1)o})]
            {\te[(\mi{L\x o})^{2}]{}^{\dagger}} 
            &\deq \te[(l-1)\x\mi{o} \x (l^{\prime}-1)\x\mi{o}]{} \\
        \end{aligned}
    \end{equation*}
    supporting the fundamental definition of the GP kernel, as a covariance (over ungoverned noise) between responses
    \begin{equation*}
        \te[l\x o\x l^{\prime}\x o^{\prime}]{k_{\rv{y}}(x,x)} 
        \deq \cov[{\te[l^{\prime}\x o^{\prime}]{\rv{y_M}\vert x}}]{\te[l\x o]{\rv{y_M}\vert x}}{M}
    \end{equation*}

    \subsection{Tensor Gaussians} \label{sub:GPR:Tensor}
        Henceforth, tensors will be broadcast when necessary, as described in \cite{Numpy2022,Harris2020}. This means that ranks and dimensions are implicitly expanded as necessary to perform an algebraic operation between tensors of differing signature. A tensor Gaussian like $\prob{\te[\mi{m}\x\mi{o}]{x}}{\te[\mi{m}\x\mi{o^{\prime}}]{x^{\prime}}}
        {\te[\mi{L}^{2}\x\mi{m}^{2}]{\Sigma}}$ is defined element-wise, using broadcasting
        \begin{multline} \label{def:Notation:p}
            \te[l\x o \x l^{\prime}\x o^{\prime}]{\prob{\te[\mi{m}\x\mi{o}]{x}}{\te[\mi{m}\x\mi{o^{\prime}}]{x^{\prime}}}
            {\te[\mi{L}^{2}\x\mi{m}^{2}]{\Sigma}}}
            \deq (2 \pi)^{-M/2} \modulus{\te[l\x l^{\prime}]{\Sigma}}^{-1/2} \\
            \exp\left(-\frac{
                \te[\mi{m}\x l\x o\x l^{\prime}\x o^{\prime}]{x-x^{\prime}}^{\intercal} 
            \te[l\x l^{\prime}\x\mi{m\x m^{\prime}}]{\Sigma}^{-1} 
            \te[\mi{m^{\prime}}\x l\x o\x l^{\prime}\x o^{\prime}]{x-x^{\prime}}}
            {2}\right)
        \end{multline}
        for $\mi{m^{\prime}} = \mi{m}$ and transposition $^{\intercal}$ moving first rank to last.

        Remarkably, the algebraic development in the remainder of this paper relies almost exclusively on an invaluable product formula reported in \cite{Rasmussen2016}:
        \begin{multline} \label{eq:GPR:product}
            \prob{z}{a}{A} \circ \prob{\Theta^{\intercal}z}{\tr{b}}{\tr{B}}
            = \prob{0}{(b-\Theta^{\intercal}a)}{(B + \Theta^{\intercal}A\Theta)} \\
            \circ \prob{z}
            {(A^{-1}+\Theta B^{-1}\Theta^{\intercal})^{-1}(A^{-1}a+\Theta B^{-1}b)}
            {(A^{-1}+\Theta B^{-1}\Theta^{\intercal})^{-1}}
        \end{multline}
        This formula and the tensor Gaussians behind it will appear in a variety of guises.

    \subsection{Prior GP} \label{sub:GPR:Prior}
        GP regression decomposes output $\te[\mi{L}]{\rv{y_M}}$ into signal GP $\te[\mi{L}]{\rv{f_M}}$, and independent noise GP $\te[\mi{L}]{\rv{e}_M}$ with homoskedastic noise (also known as likelihood) covariance $\te[\mi{L}^2]{E}$
        \begin{equation*}
            \begin{aligned}
                \te[\mi{L}]{\rv{y_M}\vert E} 
                &= \te[\mi{L}]{\rv{f_M}} + \te[\mi{L}]{\rv{e}_M\vert E} \\
                \te[\mi{L}]{\rv{e}_M\vert E} \big\vert \te[\mi{M}\x\mi{o}]{x}
                &\sim \gaussd{\te[\mi{L}\x\mi{o}]{0}}{\te[(\mi{L}\x 1)^2]{E} \circ \diag[(1\x\mi{o})^2]{1}}
            \end{aligned}
        \end{equation*}
        Angle brackets denote a (perhaps broadcast) diagonal tensor, such as the identity matrix $\diag[(1\x\mi{o})^2]{1} \deqr \diag[]{\tte[(1\x\mi{o})^2]{1}}$.

        The RBF kernel is hyperparametrized by signal covariance $\te[\mi{L}^2]{F}$ and the tensor $\te[\mi{L}^{2}\x\mi{M}]{\Lambda}$ of characteristic lengthscales, which must be symmetric $\te[l\x l^{\prime}\x\mi{M}]{\Lambda}=\te[l^{\prime}\x l\x\mi{M}]{\Lambda}$. Now use
        \begin{equation*}
            \begin{aligned}
                \diag[l\x l^{\prime}\x\mi{M}^{2}]{\Lambda^{2} \pm I} 
                &\deq \diag{\te[l\x\mi{M}]{\Lambda} \circ \te[l^{\prime}\x\mi{M}]{\Lambda} \pm \te[\mi{M}]{I}} 
                \qquad I \in \set{0}\cup\st{Z}^{+} \\
                \diag[l\x l^{\prime}\x\mi{M}^{2}]{\Lambda^{2}} &\deq 
                \diag[l\x l^{\prime}]{\Lambda^{2} \pm 0} \\
                    \te[l\x l^{\prime}]{\pm F} 
                &\deq (2 \pi)^{M/2} \modulus{\diag[l\x l^{\prime}]{\Lambda^{2}}}^{1/2} \te[l\x l^{\prime}]{F}
            \end{aligned}
        \end{equation*}
        to implement the non-informative RBF prior according to \cref{def:Notation:p}
        \begin{equation*}
            \te[\mi{L}]{\rv{f_M} \vert F,\Lambda}
            \big\vert \te[\mi{M}\x\mi{o}]{x} \sim \\
            \gaussd{\te[\mi{L}\x\mi{o}]{0}}{\te[(\mi{L}\x 1)^{2}]{\pm F} \circ 
            \prob{\te[\mi{M}\x\mi{o}]{x}}{\te[\mi{M}\x\mi{o}]{x}}
            {\diag[\mi{L}^{2}\x\mi{M}^{2}]{\Lambda^{2}}}} 
        \end{equation*}
        
    \subsection{Predictive GP} \label{sub:GPR:Predictive}
        Bayesian inference for GP regression further conditions the hyper-parametrized GP $\rv{y} \vert E,F,\Lambda$ on the observed realization (over ungoverned noise) of the random variable $\te{\rv{y}\vert X}$
        \begin{equation*}
            \te[\mi{L} \x \mi{N}]{Y}^{\dagger} \deq \te{\te[\mi{L}]{\rv{y_M}\vert E,F,\Lambda} \big\vert \te[\mi{M}\x\mi{N}]{X}}^{\dagger}\!(\omega) \in \st{R}^{LN}
        \end{equation*}
        To this end we define
        \begin{equation} \label{def:GPR:Kk}
            \begin{aligned}
                \te[\mi{Lo}\x\mi{Lo}]{K_{\rv{e}}} &\deq 
                \cov{\te{\te[\mi{L}]{\rv{e}_{M}\vert E} \big\vert \te[\mi{M}\x\mi{o}]{x}}^{\dagger}}{M} \\
                &\phantom{:}= \te{\te[(\mi{L}\x 1)^2]{E} \circ \diag[(1\x\mi{o})^2]{1}}^{\dagger} \\
                \te[\mi{Lo}\x\mi{L o^{\prime}}]{k(x, x^{\prime})} &\deq
                \cov[{\te{\te[\mi{L}]{\rv{f_M}\vert F,\Lambda} \big\vert \te[\mi{M}\x\mi{o^{\prime}}]{x^{\prime}}}^{\dagger}}]
                {\te{\te[\mi{L}]{\rv{f_M}\vert F,\Lambda} \big\vert \te[\mi{M}\x\mi{o}]{x}}^{\dagger}}{M} \\
                &\phantom{:}= \te{\te[\mi{L}^{2}]{\pm F} \circ 
                \prob{\te[\mi{M}\x\mi{o}]{x}}{\te[\mi{M}\x\mi{o^{\prime}}]{x^{\prime}}}
                {\diag[\mi{L}^{2}\x\mi{M}^{2}]{\Lambda^{2}}}}^{\dagger} \\
                %
                \te[\mi{LN}\x\mi{LN}]{K_{Y}} &\deq 
                \cov{\te{\te[\mi{L}]{\rv{y}\vert E,F,\Lambda} \big\vert \te[\mi{M}\x\mi{N}]{X}}^{\dagger}}{M} \\
                &\phantom{:}= k(\te[\mi{M}\x\mi{N}]{X},\te[\mi{M}\x\mi{N}]{X}) + \te[\mi{LN}\x\mi{LN}]{K_{\rv{e}}}
            \end{aligned}
        \end{equation}
        Applying Bayes' rule
        \begin{equation*}
            \begin{aligned}
                p(\rv{f_M}\vert Y)p(Y) = p(Y\vert \rv{f_M})p(\rv{f_M})
                &= \prob{Y^{\dagger}}{\rv{f_M}^{\dagger}}{K_{\rv{e}}} \prob{\rv{f_M}^{\dagger}}{\te[\mi{LN}]{0}}{k(X,X)} \\
                &= \prob{\rv{f_M}^{\dagger}}{Y^{\dagger}}{K_{\rv{e}}} \prob{\rv{f_M}^{\dagger}}{\te[\mi{LN}]{0}}{k(X,X)}
            \end{aligned}
        \end{equation*}
        Product formula \cref{eq:GPR:product} immediately reveals the marginal likelihood
        \begin{equation} \label{eq:GPR:marginalLikelihood}
            p\!\left(\te{Y \vert E,F,\Lambda} \big\vert X\right)
            = \prob{\te[\mi{L\x N}]{Y}^{\dagger}}{\te[\mi{LN}]{0}}{K_Y}
        \end{equation}
        and the posterior distribution
        \begin{multline*}
            \te[\mi{L\x N}]{\te{\rv{f_M} \vert Y \vert E,F,\Lambda} \big\vert X}^{\dagger} \sim \\
            \gauss{k(X,X) K_{Y}^{-1} Y^{\dagger}}{\ k(X,X) - k(X,X) K_{Y}^{-1} k(X,X)}
        \end{multline*}

        The ultimate goal is the posterior predictive GP which extends the posterior distribution to arbitrary -- usually unobserved -- $\te[\mi{M}\x\mi{o}]{x}$. This is formally derived from the definition of conditional probability, but this seems unnecessary, for the extension must recover the posterior distribution when $x=X$. Without unfeasible distortions, there is only one way of selectively replacing $X$ with $x$ in the posterior formula which preserves the coherence of tensor ranks:
        \begin{multline} \label{def:GPR:Predictive}
            \te[\mi{L\x o}]{\te{\rv{f_M} \vert Y \vert E,F,\Lambda} \big\vert x}^{\dagger} \sim \\
            \gauss{k(x,X) K_{Y}^{-1} Y^{\dagger}}{\ k(x,x) - k(x,X) K_{Y}^{-1} k(X,x)}
        \end{multline}
        In order to calculate the last term, the Cholesky decomposition $K_{Y}^{1/2}$ is used to write
        \begin{equation*}
            \tte[\mi{Lo}^{2}]{k(x,X) K_{Y}^{-1} k(X,x)} = \tte[\mi{Lo}]{K_{Y}^{-1/2} k(X,x)}^{2}
        \end{equation*}

    \subsection{GP Optimization} \label{sub:GPR:Optimization}
        Henceforth we implicitly condition on optimal hyperparameters, which maximise the marginal likelihood \cref{eq:GPR:marginalLikelihood}.
        \begin{equation} \label{eq:GPR:hyperparameters}
            \te[\mi{L}^{2}]{E},\te[\mi{L}^{2}]{F},\te[\mi{L}^{2}\x\mi{M}]{\Lambda} \deq \argmax \prob{\te[\mi{L\x N}]{Y}^{\dagger}}{\te[\mi{LN}]{0}}{K_Y}
        \end{equation}


\section{Gaussian Process moments}\label{sec:GPMom}
    This Section calculates the SP moments of GP Regression, absorbing \cref{sec:GPR} into the perspective of \cref{sec:SPEst}.
    Let $c\colon \st{R} \to [0,1]$ be the (bijective) CDF of the standard, univariate normal distribution, and define the triads
    \begin{equation*}
        \begin{aligned}
            \te[\mi{M\x 3}]{\rv{z}} &\deq c^{-1}\!\left(\te[\mi{M\x 3}]
            {\rv{u}}\right) \sim \gauss{\te[\mi{M\x 3}]{0}}{\diag[\mi{M}^{2}]{1}} \\
            \te[\mi{M^{\prime}\x 3}]{\rv{x}} &\deq \te[\mi{M\x M^{\prime}}]{\Theta}^{\intercal} \te[\mi{M\x 3}]{\rv{z}}
        \end{aligned}
    \end{equation*}
    Here, the rotation matrix $\te[\mi{M\x M^{\prime}}]{\Theta}^{\intercal} = \te[\mi{M\x M^{\prime}}]{\Theta}^{-1}$ is broadcast to multiply the triad $\tte[\mi{M\x 3}]{\rv{z}}$. 
    The purpose of ths arbitrary rotation is to allow GPs whose input basis $\rv{x}$ is not aligned with the fundamental basis $\rv{u}$ of the coefficient of determination. The latter is aligned with $\rv{z}$ which is the input we must condition. This generalization is cheap, given product formula \cref{eq:GPR:product}, and of great potential benefit. One could, for example, imagine optimizing $\Theta$ to maximize $S_{\mi{m}}$.
    
    Throughout the remainder of this paper, primed ordinal subscripts are used to specify Einstein summation contraction of tensors, the multiplication and summation of elements over a matching index which underpins matrix multiplication. In this work, whenever a subscript primed in a specific fashion appears in adjacent tensors (those not separated by algebraic operations $+,-,\circ,\,\otimes$) and does not subscript the result, it is contracted over, according to the Einstein convention. Implementation examples of the convention are given under {\tt einsum} in \cite{Numpy2022}.
    
    Adding shared Gaussian noise $\te[\mi{L}]{\rv{e}_M\vert E}$ to \cref{def:GPR:Predictive} yields
    \begin{multline}\label{eq:GPMom:yDist}
        \te[\mi{L\x 3}]{y(\te[\mi{M+1\x 3}]{\rv{u}}) \big\vert \te[\mi{M\x 3}]{u}}^{\dagger} 
        = \te[\mi{L\x 3}]{\te{\rv{y_M} \vert Y \vert E,F,\Lambda} \big\vert \tte[\mi{M\x 3}]{z}}^{\dagger} \sim \\
        \gauss{k(x,X) K_{Y}^{-1} Y^{\dagger}}{\ k(x,x) - \tte[\mi{Lo}]{K_{Y}^{-1/2} k(X,x)}^{2} + E^{\dagger}}
    \end{multline}
    using broadcast $\tte[\mi{L3\x L3}]{E^{\dagger}} \deq \tte[(\mi{L\x 3})^{2}]{\tte[(\mi{L}\x 1)^{2}]{E} \circ \tte[(1\x\mi{3})^{2}]{1}}^{\dagger}$. 
    To bring the GP estimate fully under the umbrella of the SP estimate we should identify its ungoverned noise, and ascribe it to $\tte[M]{\rv{u}}$ of the SP.
    Let $d\colon (0,1) \to (0,1)^{L}$ concatenate every $L^{\mathrm{th}}$ decimal place starting at $l$, for each output dimension $l\leq L$ of $(0,1)^{L}$, then \cref{eq:GPMom:yDist} can be written as
    \begin{multline}\label{eq:GPMom:yReveal}
        \te[\mi{L\x 3}]{y(\te[\mi{M+1\x 3}]{\rv{u}}) \big\vert \te[\mi{M\x 3}]{u}}^{\dagger} \\
        = \te[\mi{L\x 3}]{\mu_{1}}^{\dagger}
        + \te[\mi{L\x 3\x L^{\prime}\x 3^{\prime}}]{\mu_{2}}^{\dagger/2} \te[\mi{L^{\prime}\x 3^{\prime}}]{\te[\mi{L}\x 1]{c^{-1}\!\left(d\left(\te[M]{\rv{u}}\right)\right)} \circ \te[1\x\mi{3}]{1}}^{\dagger}
    \end{multline}
    where $\tte[(\mi{L\x 3})^{2}]{\mu_{2}}^{\dagger/2}$ denotes the lower triangular Cholesky decomposition of the matrix $\tte[(\mi{L\x 3})^{2}]{\mu_{2}}^{\dagger}$.
    From the development in \cref{sec:SPEst}, the first two moments $\mu_{1},\mu_{2}$ are sufficient to compute the coefficient of determination and its variance. 
    
    The crucial moments $\mu_{1},\mu_{2}$ are simply read from \cref{eq:GPMom:yDist,eq:GPMom:yReveal}, but still need conditioning. This is entirely a matter of repeatedly applying product formula \cref{eq:GPR:product}, together with the familiar Gaussian identities
    \begin{equation*}
        \begin{aligned}
            \te[\mi{M}]{\rv{z}} \sim \gauss{\te[\mi{M}]{Z}}{\te[\mi{M}\x\mi{M}]{\Sigma}} &\Rightarrow
            \te[\mi{m}]{\rv{z}} \sim \gauss{\te[\mi{m}]{Z}}{\te[\mi{m}\x\mi{m}]{\Sigma}} \\
            \te[\mi{m}]{\rv{z}} \sim \gauss{\te[\mi{m}]{Z}}{\te[\mi{m}\x\mi{m}]{\Sigma}} &\Rightarrow
            \te[\mi{m}\x\mi{m}]{\Theta}^{\intercal}\te[\mi{m}]{\rv{z}} \sim 
            \modulus{\Theta}^{-1}
            \gauss{\Theta^{\intercal}Z}{\Theta^{\intercal}\Sigma\Theta}                        
        \end{aligned}
    \end{equation*}
    Henceforth the ordinal set $\mi{m^{\prime\prime}}$, whether or not decorated with a further \emph{even} number of primes, should be taken as equal to $\mi{m}$. 
    Likewise the ordinal set $\mi{m^{\prime\prime\prime}}$, whether or not decorated with a further \emph{even} number of primes, should be taken as equal to $\mi{m^{\prime}}$. Superscript $^{*}$ will stand for four consecutive primes $^{\prime\prime\prime\prime}$. So $\mi{m}, \mi{m^{\prime}}$ are identified by the parity (even or odd) of the primes adorning $\mi{m}$. Such explicit notation is required to maintain the integrity of einstein summation. This only applies to ordinal sets, not singleton values, so the many different prime decorations of $l$ \emph{always} indicate potentially different values.

    \subsection{First Moments} \label{sub:GPMom:First}
        The first moment of the GP for any $\mi{m}\subseteq\mi{M}$ is given by
        \begin{equation*}
            \te[\mi{L}]{\mu_{\mi{m}}}
            = \ev{k\!\left(\te[\mi{M}]{\rv{x}},X\right) K_{Y}^{-1} Y^{\dagger} \big\vert \te[\mi{m}]{z}}{\mi{M-m}}
            = \te[\mi{L}\x\mi{L^{\prime\prime}}\x\mi{N^{\prime\prime}}]{g_{\mi{m}}}^{\dagger}
            \te[\mi{L^{\prime\prime}N^{\prime\prime}}]{K_{Y}^{-1} Y^{\dagger}}
        \end{equation*}
        where
        \begin{multline*}
            \frac{\te[l\x l^{\prime\prime}\x\mi{N^{\prime\prime}}]{g_{\mi{m}}}}{\te[l\x l^{\prime\prime}\x\mi{N^{\prime\prime}}]{g_{\mi{0}}}} \deq \frac
            {\prob{\te[\mi{m}]{z}}{\te[\mi{m}\x l\x l^{\prime\prime}\x\mi{N^{\prime\prime}}]{G}}{\te[l\x l^{\prime\prime}]{\Gamma}}}
            {\prob{\te[\mi{m}]{z}}{\te[\mi{m}]{0}}{\diag[\mi{m}^{2}]{1}}} \\
            = \frac{{\prob{\te[l\x l^{\prime\prime}\x\mi{m^{\prime\prime}\x m}]{\Phi}\te[\mi{m}]{z}}{\te[\mi{m}\x l\x l^{\prime\prime}\x\mi{N^{\prime\prime}}]{G}}{\te[l\x l^{\prime\prime}\x\mi{m^{*}\x m^{\prime\prime}}]{\Gamma}\te[l\x l^{\prime\prime}\x \mi{m^{\prime\prime}\x m}]{\Phi}}}}
            {\prob{\te[\mi{m}]{0}}{\te[\mi{m}\x l\x l^{\prime\prime}\x\mi{N^{\prime\prime}}]{G}}
            {\te[l\x l^{\prime\prime}]{\Phi}}}
        \end{multline*}
        and
        \begin{equation*}
            \begin{aligned}
                \te[l\x l^{\prime\prime}\x\mi{N^{\prime\prime}}]{g_{\mi{0}}} 
                &\deq \te[l\x l^{\prime\prime}]{\pm F}
                \prob{\te[\mi{M}]{0}}{\te[\mi{M}\x\mi{N^{\prime\prime}}]{X}}
                {\diag[l\x l^{\prime\prime}]{\Lambda^{2}+1}} \\
                \te[\mi{m}\x l\x l^{\prime\prime}\x\mi{N^{\prime\prime}}]{G} &\deq 
                \te[\mi{m}\x\mi{M}]{\Theta} \diag[l\x l^{\prime\prime}\x\mi{M}\x\mi{M^{\prime\prime}}]{\Lambda^{2}+1}^{-1} \te[\mi{M^{\prime\prime}}\x\mi{N^{\prime\prime}}]{X} \\
                \te[l\x l^{\prime\prime}\x\mi{m^{\prime\prime}}\x\mi{m}]{\Phi} &\deq 
                \te[\mi{m^{\prime\prime}}\x\mi{M}]{\Theta}
                \diag[l\x l^{\prime\prime}\x\mi{M}\x\mi{M^{\prime\prime}}]{\Lambda^{2}+1}^{-1} \te[\mi{m}\x\mi{M^{\prime\prime}}]{\Theta}^{\intercal} \\
                \te[l\x l^{\prime\prime}\x\mi{m}^{2}]{\Gamma} &\deq 
                \diag[\mi{m}^{2}]{1} -
                \te[l\x l^{\prime\prime}\x\mi{m}^{2}]{\Phi}
            \end{aligned}
        \end{equation*}
        Note that when $\mi{m} = \mi{M}$, $\Theta$ factors out entirely.
        The unconditional expectation $\mu_{\mi{0}} \approx \te[\mi{L}]{\bar{Y}}$, but this is usually inexact.
        \subsection{Second Moments} \label{sub:GPMom:Second}
        The second moment of the GP for any $\mi{m,m^{\prime}}\subseteq\mi{M}$ is given by
            \begin{equation} \label{eq:GPMom:Second}
                \te[\mi{L}^2]{\mu_{\mi{mm^{\prime}}}} = 
                \te[\mi{L}^2]{F} \circ \te[\mi{L}^2]{\phi_{\mi{mm^{\prime}}}} - \te[\mi{L}^2]{\psi_{\mi{mm^{\prime}}}} + \te[\mi{L}^2]{E}                        
            \end{equation}
            where
            \begin{multline*}
                \te[l\x l^{\prime}]{\phi_{\mi{mm^{\prime}}}}
                \deq \frac{\evt{\;\evt{\te[l\x l^{\prime}]{k\!\left(\te[\mi{M}]{\rv{x}},\te[\mi{M^{\prime}}]{\rv{x}}\right) \big\vert \te[\mi{m}]{z},\te[\mi{m^{\prime}}]{z}}}{\mi{M^{\prime}-m^{\prime}}}}{\mi{M-m}}}{\te[l\x l^{\prime}]{F}} \\
                = \frac
                {\modulus{\diag[l\x l^{\prime}\x\mi{M}^{2}]{\Lambda^{2}}}^{1/2} \prob{\te[\mi{m}]{\rv{z}}}{\te[\mi{m}]{0}}{\te[l\x l^{\prime}\x\mi{m}^2]{1-\Upsilon}}\prob{\te[\mi{m^{\prime}}]{\rv{z}}}{\te[l\x l^{\prime}\x \mi{m^{\prime}}]{Z}}{\te[l\x l^{\prime}\x\mi{m^{\prime}}^{2}]{\Pi}}}
                {\modulus{\diag[l\x l^{\prime}\x\mi{M}^2]{\Lambda^{2}+2}}^{1/2}
                \prob{\te[\mi{m}]{\rv{z}}}{\te[\mi{m}]{0}}{\diag[\mi{m}^{2}]{1}}\prob{\te[\mi{m^{\prime}}]{\rv{z}}}{\te[\mi{m^{\prime}}]{0}}{\diag[\mi{m^{\prime}}^{2}]{1}}}
            \end{multline*}
            \begin{multline*}
                \te[\mi{L\x L^{\prime}}]{\psi_{\mi{mm^{\prime}}}}
                \deq \evt{\;\evt{\te[\mi{L\x L^{\prime}}]{k\!\left(\te[\mi{M}]{\rv{x}},X\right) K_{Y}^{-1} k\!\left(X,\te[\mi{M^{\prime}}]{\rv{x}}\right) \big\vert \te[\mi{m}]{z},\te[\mi{m^{\prime}}]{z}}}{\mi{M^{\prime}-m^{\prime}}}}{\mi{M-m}} \\
                 = \left(\te[\mi{L\x L^{\prime\prime}\x N^{\prime\prime}}]{g_{\mi{m}}}^{\dagger}
                    \te[\mi{L^{\prime\prime\prime}N^{\prime\prime\prime}}\x\mi{L^{\prime\prime}N^{\prime\prime}}]{K_{Y}}^{-1/2}\right)
                \left(\te[\mi{L^{\prime}\x L^{\prime\prime}\x N^{\prime\prime}}]{g_{\mi{m^{\prime}}}}^{\dagger}
                    \te[\mi{L^{\prime\prime\prime}N^{\prime\prime\prime}}\x\mi{L^{\prime\prime}N^{\prime\prime}}]{K_{Y}}^{-1/2}\right)
            \end{multline*}
            using the lower triangular Cholesky decomposition $\tte[\mi{LN}\x\mi{LN}]{K_{Y}}^{1/2}$ and
            \begin{equation*}
                \begin{aligned}
                    \te[l\x l^{\prime}\x\mi{m}\x\mi{m^{\prime\prime}}]{\Upsilon} &\deq 
                    \te[\mi{m}\x\mi{M}]{\Theta}
                    \diag[l\x l^{\prime}\x\mi{M}\x\mi{M^{\prime}}]{\Lambda^{2}+2}^{-1} \te[\mi{m^{\prime\prime}}\x\mi{M^{\prime}}]{\Theta}^{\intercal} \\
                    \te[l\x l^{\prime}\x \mi{M^{\prime}}\x\mi{M^{\prime\prime\prime}}]{\Pi}^{-1} &\deq 
                    \diag[\mi{M^{\prime}}\x\mi{M^{\prime\prime\prime}}]{1} + \te[l\x l^{\prime}\x \mi{M^{\prime}}\x\mi{M^{\prime\prime\prime}}]{\Phi} + \\
                    &\phantom{\deq}\ \te[l\x l^{\prime}\x\mi{M^{\prime}\x\mi{m}}]{\Phi}
                    \te[l\x l^{\prime}\x\mi{m}\x\mi{m^{\prime\prime}}]{\Gamma}^{-1} \te[l\x l^{\prime}\x\mi{m^{\prime\prime}}\x\mi{M^{\prime\prime\prime}}]{\Phi} \\
                    \te[l\x l^{\prime}\x \mi{m^{\prime}}]{Z} &\deq 
                    \te[l\x l^{\prime}\x \mi{m^{\prime}}\x\mi{M}]{\Pi}
                    \te[l\x l^{\prime}\x\mi{M}\x\mi{m^{\prime\prime}}]{\Phi}
                    \te[l\x l^{\prime}\x\mi{m^{\prime\prime}}\x\mi{m}]{\Gamma}^{-1}
                    \te[\mi{m}]{\rv{z}}
                \end{aligned}
            \end{equation*}


\section{Gaussian Process estimates}\label{sec:GPEst}
    Using the work of the last two Sections, we are finally in a position to calculate Gaussian process estimates for the Sobol' indices \cref{def:GSI:mean} and their uncertainties \cref{def:GSI:variance} via the two unknown quantities in \cref{eq:SPEst:V} and \cref{eq:SPEst:W}, as described in \cref{sec:SPEst}.
    \subsection{Expected Value}\label{sub:GPEst:Expectation}
    Using the shorthand
    \begin{equation*}
        \te[l\x\mi{L^{\prime\prime}}\mi{N^{\prime\prime}}]{g_{\mi{0}}KY}^{\dagger} \deq 
        \te[l\x\mi{L^{\prime\prime}}\x\mi{N^{\prime\prime}}]{g_{\mi{0}}}^{\dagger}
        \circ \te[\mi{L^{\prime\prime}}\mi{N^{\prime\prime}}]{K_{Y}^{-1} Y^{\dagger}}
    \end{equation*}
    to write
    \begin{equation*}                
        \evt{\te[l\x l^{\prime}]{\te[\mi{L}]{\mu_{\mi{m}}}^{2}}}{\mi{m}} 
        \deqr \te[l\x\mi{L^{\prime\prime}}\mi{N^{\prime\prime}}]{g_{\mi{0}}KY}^{\dagger}
        \te[l\x\mi{L^{\prime\prime}}\x\mi{N^{\prime\prime}} \x l^{\prime}\x\mi{L^{\prime\prime\prime}}\x\mi{N^{\prime\prime\prime}}]{H_{\mi{m}}}^{\dagger}
        \te[l^{\prime}\x\mi{L^{\prime\prime\prime}}\mi{N^{\prime\prime\prime}}]{g_{\mi{0}}KY}^{\dagger}
    \end{equation*}
    results in
    \begin{align*}
        &\te[l\x\mi{L^{\prime\prime}}\x\mi{N^{\prime\prime}} \x l^{\prime}\x\mi{L^{\prime\prime\prime}}\x\mi{N^{\prime\prime\prime}}]{H_{\mi{m}}} \\
        &\deq \ev{\frac{
            \prob{\te[\mi{m}]{\rv{z}}}{\te[\mi{m}\x l\x \mi{L^{\prime\prime}\x N^{\prime\prime}}]{G}}{\te[l\x \mi{L^{\prime\prime}}]{\Gamma}} \otimes
            \prob{\te[\mi{m}]{\rv{z}}}{\te[\mi{m}\x l^{\prime}\x \mi{L^{\prime\prime\prime}\x N^{\prime\prime\prime}}]{G}}{\te[l^{\prime}\x\mi{L^{\prime\prime\prime}}]{\Gamma}}}
        {\prob{\te[\mi{m}]{\rv{z}}}{\te[\mi{m}]{0}}{\diag[\mi{m\x m}]{1}}
        \prob{\te[\mi{m}]{\rv{z}}}{\te[\mi{m}]{0}}{\diag[\mi{m\x m}]{1}}}}{\mi{m}} \\
        &\phantom{:}=
        \frac{{\prob{\te[l\x l^{\prime\prime}]{\Phi}\te[\mi{m}\x l^{\prime}\x l^{\prime\prime\prime}\x\mi{N^{\prime\prime\prime}}]{G}}{\te[\mi{m}\x l\x l^{\prime\prime}\x\mi{N^{\prime\prime}}]{G}}{\te[l\x l^{\prime\prime}\x l^{\prime}\x l^{\prime\prime\prime}]{\Psi}\te[l\x l^{\prime\prime}\x \mi{m^{\prime\prime}\x m}]{\Phi}}}}
        {\prob{\te[\mi{m}]{0}}{\te[\mi{m}\x l\x l^{\prime\prime}\x\mi{N^{\prime\prime}}]{G}}
        {\te[l\x l^{\prime\prime}]{\Phi}}}
\end{align*}
    where
    \begin{equation*}
        \begin{aligned}
            \te[l\x l^{\prime\prime}\x l^{\prime}\x l^{\prime\prime\prime}\x\mi{m^{*}}\x\mi{m^{\prime\prime}}]{\Psi} &\deq 
            \te[l\x l^{\prime\prime}\x\mi{m^{*}\x m^{\prime\prime}}]{\Gamma} + \te[l^{\prime}\x l^{\prime\prime\prime}\x\mi{m^{*}\x m^{\prime\prime}}]{\Gamma} \\
            &\phantom{:}- \te[l\x l^{\prime\prime}\x\mi{m^{*}}\x\mi{m}]{\Gamma} \te[l^{\prime}\x l^{\prime\prime\prime}\x\mi{m\x m^{\prime\prime}}]{\Gamma} \\
        \end{aligned}                    
    \end{equation*}

    \subsection{Variance}\label{sub:GPEst:Variance}
        Recall from \cref{eq:SPEst:W} that the inputs comprising $\mi{m},\mi{m^{\prime}}$ vary independently when calculating a covariance $W_{\mi{m m^{\prime}}}$ via $A_{\mi{m m^{\prime}}}$. In calculating
        \begin{equation*}
            \evt{\;\evt{\te[\mi{L}^{2} \x \mi{L^{\prime}}^{2}]{\mu_{\mi{m}} \otimes \mu_{\mi{mm^{\prime}}} \otimes \mu_{\mi{m^{\prime}}}}}{\mi{m^{\prime}}}}{\mi{m}}
        \end{equation*}
        in \cref{eq:GPMom:Second} the terms containing the ungoverned noise variance $\te[\mi{L}^2]{E}$ reduce to the same function of $g_{\mi{0}}$ by reduction formula \cref{eq:SPEst:reduction}, so these will obviously cancel across the four $A_{\mi{m m^{\prime}}}$ terms in \cref{eq:SPEst:W}. 
        We may therefore assume $E=0$ in \cref{eq:GPMom:Second}. Firstly
        \begin{multline*}
            \evt{\;\evt{\te[]{\te[l]{\mu_{\mi{m}}} \otimes \te[l^{\prime\prime}\x l^{\prime\prime\prime}]{\phi_{\mi{mm^{\prime}}}} \otimes \te[l^{\prime}]{\mu_{\mi{m^{\prime}}}}}}{\mi{m^{\prime}}}}{\mi{m}} = \\
            \frac
            {\modulus{\diag[l^{\prime\prime}\x l^{\prime\prime\prime}\x\mi{M}^{2}]{\Lambda^{2}}}^{1/2}(2\pi)^{m/2}}
            {\modulus{\diag[l^{\prime\prime}\x l^{\prime\prime\prime}\x\mi{M}^2]{\Lambda^{2}+2}}^{1/2}}
            \te[l\x\mi{L^{*}N^{*}}]{g_{\mi{0}}KY}^{\dagger} \otimes
            \te[l^{\prime}\x\mi{L^{*\prime}N^{*\prime}}]{g_{\mi{0}}KY}^{\dagger} \\
            \left\lbrack
            \prob{\te[\mi{m}]{0}}{\te[l^{\prime\prime}\x l^{\prime\prime\prime}]{\Upsilon}^{1/2} \te[\mi{m}\x l\x \mi{L^{*}\x N^{*}}]{G}} 
            {\diag[]{1} -
            \te[l^{\prime\prime}\x l^{\prime\prime\prime}]{\Upsilon}^{1/2} \te[l\x \mi{L^{*}}]{\Phi}\te[l^{\prime\prime}\x l^{\prime\prime\prime}]{\Upsilon}^{\intercal/2}} \phantom{\frac{_{\vert}^{\vert}}{_{\vert}^{\vert}}} \right.\\
            \left. \circ 
                \frac{
                    \prob{\te[\mi{m^{\prime}}\x l^{\prime}\x \mi{L^{*\prime}\x N^{*\prime}}]{G}}
                    {\te[]{\Omega} \te[]{C} \te[l\x \mi{L^{*}}]{\Gamma}^{-1} \te[\mi{m}\x l\x \mi{L^{*}\x N^{*}}]{G}}{\te[]{B}+\te[]{\Omega} \te[]{C} \te[]{\Omega}^{\intercal}}}
                    {\prob{\te[\mi{m^{\prime}}]{0}}{\te[\mi{m^{\prime}}\x l^{\prime}\x \mi{L^{*\prime}\x N^{*\prime}}]{G}}{\te[l^{\prime}\x \mi{L^{*\prime}}]{\Phi}}}
            \right\rbrack^{\dagger}
        \end{multline*}
        using the lower triangular Cholesky decomposition
        \begin{equation*}
            \te[l^{\prime\prime}\x l^{\prime\prime\prime}\x\mi{m}^{2}]{\Upsilon}
            = \te[l^{\prime\prime}\x l^{\prime\prime\prime}]{\Upsilon}^{1/2} \te[l^{\prime\prime}\x l^{\prime\prime\prime}]{\Upsilon}^{\intercal/2}
        \end{equation*}
        and
        \begin{equation*}
            \begin{aligned}
                \te[\mi{m^{\prime}\x m}]{\Omega} &\deq 
                \te[l^{\prime}\x l^{*\prime}\x\mi{m^{\prime}}\x\mi{m^{\prime\prime\prime}}]{\Phi}
                \te[l^{\prime\prime}\x l^{\prime\prime\prime}\x\mi{m^{\prime\prime\prime}}\x\mi{M}]{\Pi}
                \te[l^{\prime\prime}\x l^{\prime\prime\prime}\x\mi{M}\x\mi{m^{\prime\prime}}]{\Phi}
                \te[l^{\prime\prime}\x l^{\prime\prime\prime}\x\mi{m^{\prime\prime}}\x\mi{m}]{\Gamma}^{-1} \\
                \te[\mi{m^{\prime}\x m^{\prime\prime\prime}}]{B} &\deq 
                \te[l^{\prime}\x l^{*\prime}\x\mi{m^{\prime}}\x\mi{m^{*\prime}}]{\Gamma}
                \te[l^{\prime}\x l^{*\prime}\x\mi{m^{*\prime}}\x\mi{m^{\prime\prime\prime}}]{\Phi} + \\
                &\phantom{\deq\ }\te[l^{\prime}\x l^{*\prime}\x\mi{m^{\prime}}\x\mi{m^{*\prime\prime\prime}}]{\Phi}
                \te[l^{\prime\prime}\x l^{\prime\prime\prime}\x\mi{m^{*\prime\prime\prime}}\x\mi{m^{*\prime}}]{\Pi}
                \te[l^{\prime}\x l^{*\prime}\x\mi{m^{*\prime}}\x\mi{m^{\prime\prime\prime}}]{\Phi} \\
                \te[\mi{m\x m^{\prime\prime}}]{C} &\deq 
                \te[l^{\prime\prime}\x l^{\prime\prime\prime}\x\mi{m}\x\mi{m^{*}}]{1-\Upsilon} \\
                &\phantom{\deq\ }
                \te[\mi{m^{*}}\x\mi{m^{*\prime\prime}}]{\diag[]{1}
                - \te[l\x l^{*}\x\mi{m^{**}}\x\mi{m^{**\prime\prime}}]{\Phi}\te[l^{\prime\prime}\x l^{\prime\prime\prime}\x\mi{m^{**\prime\prime}}\x\mi{m^{***}}]{\Upsilon}}^{-1} \\
                &\phantom{\deq\ }\te[l\x l^{*}\x\mi{m^{*\prime\prime}}\x\mi{m^{\prime\prime}}]{\Gamma}
            \end{aligned}
        \end{equation*}
        Secondly
        \begin{equation*}
            \evt{\;\evt{\te[]{\te[l]{\mu_{\mi{m}}} \otimes \te[l^{\prime\prime}\x l^{\prime\prime\prime}]{\psi_{\mi{mm^{\prime}}}} \otimes \te[l^{\prime}]{\mu_{\mi{m^{\prime}}}}}}{\mi{m^{\prime}}}}{\mi{m}} = 
            \te[l\x l^{\prime\prime}\x \mi{L^{**}N^{**}}]{E_{\mi{m}}}
            \te[l^{\prime}\x l^{\prime\prime\prime}\x \mi{L^{**}N^{**}}]{E_{\mi{m^{\prime}}}}
        \end{equation*}
        where
        \begin{multline*}
            \te[l\x l^{\prime\prime}\x \mi{L^{**}N^{**}}]{E_{\mi{m}}} \deq 
            \left(
                \te[\mi{L^{**}N^{**}}\x\mi{L^{*\prime\prime}N^{*\prime\prime}}]{K_{Y}}^{-1/2} \otimes \te[l\x\mi{L^{*}N^{*}}]{g_{\mi{0}}KY}^{\dagger} \right) \\
            \left\lbrack\frac{
            \te[l^{\prime\prime}\x\mi{L^{*\prime\prime}}\x\mi{N^{*\prime\prime}}]{g_{\mi{0}}} \circ
            \prob
            {\te[l\x \mi{L^{*}}]{\Phi} 
            \te[\mi{m}\x l^{\prime\prime}\x \mi{L^{*\prime\prime}\x N^{*\prime\prime}}]{G}}
            {\te[\mi{m}\x l\x \mi{L^{*}\x N^{*}}]{G}}
            {\te[]{D}}}
            {\prob{\te[\mi{m}]{0}}{\te[\mi{m}\x l\x \mi{L^{*}\x N^{*}}]{G}}{\te[l\x \mi{L^{*}}]{\Phi}}} \right\rbrack^{\dagger}
        \end{multline*}
        \begin{equation*}
            \begin{aligned}
                \te[l\x l^{\prime\prime} \x l^{*}\x l^{*\prime\prime}\x\mi{m}^{2}]{D} &\deq \te[l\x l^{*}\x\mi{m}\x\mi{m^{\prime\prime}}]{\Phi} \\ 
                &\phantom{:} - \te[l\x l^{*}\x\mi{m}\x\mi{m^{*\prime\prime}}]{\Phi}
                \te[l^{\prime\prime}\x l^{*\prime\prime}\x\mi{m^{*\prime\prime}}\x\mi{m^{*}}]{\Phi}
                \te[l\x l^{*}\x\mi{m^{*}}\x\mi{m^{\prime\prime}}]{\Phi}
            \end{aligned}
        \end{equation*}
        and $\te[l^{\prime}\x l^{\prime\prime\prime}\x \mi{L^{**}N^{**}}]{E_{\mi{m^{\prime}}}}$ substitutes $\mi{m}\mapsto\mi{m^{\prime}},\ l\mapsto l^{\prime},\ l^{\prime\prime}\mapsto l^{\prime\prime\prime}$ in these definitions. In other words, add a prime superscript to every symbol which is not superscripted $^{**}$.

        This completes the calculation of all quantities of interest.

\section{Complexity and simplifications}\label{sec:Complexity}
    In this Section we highlight the computational cost of these calculations, assuming GP regression has already performed the Cholesky decomposition $\tte[\mi{(LN)}^{2}]{K_{Y}}^{1/2}$. GP regression typically requires this to be computed several times in optimizing the GP hyperparameters $E, F, \Lambda$. This optimization will always dominate compute time, in repeatedly calculating $\exp(\ldots)$ of $O(L^{2}N^{2}M)$ and the Cholesky decomposition of $O(L^{3}N^{3})$ \cite{Dai2022}. 
    
    We shall therefore concentrate on the memory demands of calculating $\tte[\mi{L^{2}}]{S_{\mi{m}}},\tte[\mi{L^{4}}]{T_{\mi{m}}}$, which are substantial. In all cases, we consider the largest two tensors occurring in an Einstein summation to be the effective memory requirement. All this assumes that the number of inputs is moderate $M \ll N$.

    From \cref{sub:GPEst:Expectation} the memory required to compute a closed index $S_{\mi{m}}$ is
    \begin{equation*}
        \T{cost of }\evt{\te[]{\mu_{\mi{m}}}^{2}}{\mi{m}} = O(L^{6}N^{3})
    \end{equation*}
    From \cref{sub:GPEst:Variance} the memory required to compute an uncertainty $T_{\mi{m}}$ is
    \begin{equation*}
        \T{cost of }\evt{\;\evt{\te[]{\mu_{\mi{m}} \otimes \phi_{\mi{mm^{\prime}}} \otimes \mu_{\mi{m^{\prime}}}}}}{\mi{m^{\prime}}}{\mi{m}} = O(L^{8}N^{3})
    \end{equation*}
    For a medium-sized problem of 10 outputs and 1000 datapoints, this is $L^{6}N^{3}=10^{15}$, for larger problems this could swell to $10^{30}$. This will challenge the memory limitations of a CPU or GPU. However, there are two simplifications which substantially ease this burden.

    \subsection{Independent Kernels}\label{sub:Complexity:Indep}
    The first simplification is to restrict the GP to independent kernels, by constraining the signal covariance to be diagonal 
    \begin{equation*}
        \te[l\x l^{\prime\prime}]{F} = 0 \T{ unless } l^{\prime\prime} = l
    \end{equation*}
    In which case the off-diagonal elements $\tte[l\x l^{\prime\prime}\x\mi{M}]{\Lambda}$ of the lengthscales tensor are completely irrelevant and need not be specified. Note, however, that the off-diagonal elements of ungovened noise (likelihood) covariance $\te[l\x l^{\prime\prime}]{E}$ are unconstrained, provided $\tte[\mi{L}^{2}]{E}$ is a covariance, and therefore symmetric positive definite.

    Restricting $\tte[\mi{L}^{2}]{F}$ to be diagonal implies that
    \begin{gather*}
        l^{\prime\prime} = l \T{ ; } l^{\prime\prime\prime} = l^{\prime} \QT{throughout \cref{sub:GPEst:Expectation}.}  \\ 
        l^{\prime\prime\prime} = l^{\prime\prime} \T{ and } l^{*\prime\prime} = l^{\prime\prime} \T{ ; } l^{*} = l \T{ ; } l^{*\prime\prime\prime} = l^{\prime\prime\prime} \T{ ; } l^{*\prime} = l^{\prime} \QT{throughout \cref{sub:GPEst:Variance}.}
    \end{gather*}
    This reduces the memory required to compute a closed index $S_{\mi{m}}$ by $O(L^3)$ to
    \begin{equation*}
        \T{cost of }\evt{\te[]{\mu_{\mi{m}}}^{2}}{\mi{m}} = O(L^{3}N^{3})
    \end{equation*}
    and the memory required to compute an uncertainty $T_{\mi{m}}$ by $O(L^4)$ to
    \begin{equation*}
        \T{cost of }\evt{\;\evt{\te[]{\mu_{\mi{m}} \otimes \phi_{\mi{mm^{\prime}}} \otimes \mu_{\mi{m^{\prime}}}}}}{\mi{m^{\prime}}}{\mi{m}} = O(L^{4}N^{3})
    \end{equation*}

    \subsection{Diagonal Uncertainty}\label{sub:Complexity:Diag}
    The second simplification observes that assessing uncertainty concerns the variances of Sobol' indices, not the cross covariances between them. 
    As alluded to following \cref{eq:SPEst:W}, this means
    \begin{equation*}
        l^{\prime} = l \T{ ; } l^{\prime\prime\prime} = l^{\prime\prime} \QT{or} 
        l^{\prime} = l^{\prime\prime} \T{ ; } l^{\prime\prime\prime} = l \QT{throughout \cref{sub:GPEst:Variance}.}   
    \end{equation*}
    The memory required to compute an uncertainty $T_{\mi{m}}$ is reduced by $O(L^2)$ to
    \begin{equation*}
        \T{cost of }\evt{\;\evt{\te[]{\mu_{\mi{m}} \otimes \phi_{\mi{mm^{\prime}}} \otimes \mu_{\mi{m^{\prime}}}}}}{\mi{m^{\prime}}}{\mi{m}} = O(L^{6}N^{3})
    \end{equation*}
    unless the independent kernels constraint of \cref{sub:Complexity:Indep} already applies, whence the reduction is by $O(L)$ to
    \begin{equation*}
        \T{cost of }\evt{\;\evt{\te[]{\mu_{\mi{m}} \otimes \phi_{\mi{mm^{\prime}}} \otimes \mu_{\mi{m^{\prime}}}}}}{\mi{m^{\prime}}}{\mi{m}} = O(L^{3}N^{3})
    \end{equation*}
    This simplification is only possible if we are ignoring higher order correction terms \cref{eq:GSI:correction} in the uncertainty, which is usually prudent.
    Obviously it does not affect the memory required to compute a closed index $S_{\mi{m}}$.

\section{Conclusion}\label{sec:Conc}
    In this paper, we transformed uniformly distributed inputs $\rv{u}$ to normally distributed inputs $\rv{z}$, enabling an arbitrary rotation by $\Theta$ to inputs $\rv{x}$ which are still normal. We then preformed Multi-Output Gaussian Process (MOGP) regression with an anisotropic radial basis function (RBF/ARD) kernel on $\rv{x}$, broadly applicable to smoothly varying outputs. Using this surrogate, analytic expressions for closed Sobol' indices $S_{\mi{m}}$ are given by \cref{def:GSI:mean,eq:SPEst:V} and \cref{sub:GPEst:Expectation}. Analytic expressions for the variance or uncertainty of these estimates over ungoverned noise is given by \cref{def:GSI:variance,eq:SPEst:W} and \cref{sub:GPEst:Variance}. Reasonably cheap simplifications of the results are described in \cref{sec:Complexity}.
    In conclusion, we shall assess the utility of these results, pointing to further research directions.

    The value of these novel formulae is somewhat limited by their high computational expense. Although calculations can be performed in seconds, their memory demands are huge. \Cref{sec:Complexity} provides ways to ameliorate this. In addition, the MOGP regression required as precursor is far slower than the Sobol' index calculation, and may run into time constraints. Overall, the computational cost of a direct (presumably Monte Carlo) numerical evaluation of Sobol' indices is far lower.

    The technique we have developed here will be preferable, even indispensable, when training data is scarce or expensive. The use of a surrogate greatly eases data requirements, preliminary tests indicating that $N=100$ datapoints is more than sufficient for Sobol' indices within $10\%$ accuracy, as opposed to $N\geq 10,000$ datapoints for direct calculation. Furthermore, future research could implement our Sobol' index calculations using sparse GPs \cite{Snelson.Ghahramani2006,titsias2009,Hensman2013}, wherein $N$ training data are replicated by $N^{*} \ll N$ inducing points.

    Perhaps related to the remarkably low data requirements for accuracy, it is highly significant that the regression noise $E$ cancels from the calculation of Sobol' indices, and their variances. This means that the accuracy of Sobol' estimates is largely unaffected by noise in the training data. Put another way, the Sobol' comparison between predictions relying on $M$ inputs and predictions only using $m \leq M$ inputs is indifferent to the absolute quality of those predictions. Two poor predictors are compared just as accurately as two good ones. Obviously this is extremely attractive whenever the output is inherently noisy, incorporating unavoidable random error. The Sobol' calculation is remarkably immune to this error. It should be noted that higher order correction terms \cref{eq:GSI:correction} should also remain immaterial thanks to this nise cancelling property.

    A significant limitation is bound to be the number of inputs $M$. All GPs are extremely susceptible to every aspect of the curse of dimensionality \cite{Bellman1966,Binois2021}. Every datapoint tends to the same Euclidean distance from every other, and inevitably lies adjacent to the bounding hypersurface as the dimensionality of the input hyperspace increases. We would caution against allowing more than $M=15$ inputs in any case.

    This is where arbitrary rotation $\Theta$ of inputs comes into its own. If the goal is reducing inputs, rotating their basis first boosts the possibilities immensely \cite{Constantine2015}. This presents the possibilty of choosing $\Theta$ to maximise the closed Sobol' index of the first few inputs, called the active subspace.
    Exciting future work could thereby build the input space from the bottom up, as follows. From a large number of inputs, take 10 likely suspects, fit a GP and rotate to an active subspace of 5 inputs. Then fit a new GP to the active subspace plus 5 inputs previously ignored, and rotate to a new active subspace, again optimizing $\Theta$ to maximize the first 5 Sobol' indices. In this manner a high fidelity surrogate may be achieved without ever confronting the curse of dimensionality. Furthermore, the Sobol' calculations at every step will be accurate and robust, because the inputs ignored will manifest as noise in the output, to which our technique is immune. We can even allow this noise to be correlated between outputs -- as it no doubt will be, considering its source -- because $\tte[\mi{L}^{2}]{E}$ can be non-diagonal in our calculations, even when independent kernels (diagonal $\tte[\mi{L}^{2}]{F}$) are employed. In fact, this should only affect the fidelity of GP regression, as the Sobol' calculation is immune to $E$.

    Finally, we should emphasise that that the fidelity and robustness of everything we have suggested is easily monitored because we provide uncertainty quantification in the form of variances of the Sobol' indices over ungoverned noise. Because of our MOGP approach, all our results and suggestions apply not only to the outputs themselves, but equally to the linkages (covariance) between them.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{elsarticle-num} 
\bibliography{master}
\end{document}
\endinput
