% SIAM Article Template
\documentclass[review,onefignum,onetabnum]{siamonline220329}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.


% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={An Example Article},
  pdfauthor={D. Doe, P. T. Frank, and J. E. Smith}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.


\usepackage{amsfonts} 
\usepackage{xspace} 
\usepackage{ifthen} 
\usepackage{csvsimple}
\usepackage{todonotes}
\usepackage{float}
\newcommand*{\M}[1]{\ensuremath{#1}\xspace} 
\newcommand*{\tr}[1]{\M{#1}}
\newcommand*{\x}{\times}
\newcommand*{\mi}[1]{\mathbf{#1}} 
\newcommand*{\st}[1]{\mathbb{#1}} 
\newcommand*{\rv}[1]{\mathsf{#1}} 
\newcommand*{\te}[2][]{\left\lbrack{#2}\right\rbrack_{#1}}
\newcommand*{\tte}[2][]{\lbrack{#2}\rbrack_{#1}}
\newcommand*{\tse}[2][]{\mi{\lbrack#2\rbrack}_{#1}}
\newcommand*{\tme}[3][]{\lbrack{#3}\rbrack_{\tse[#1]{#2}}}
\newcommand*{\diag}[2][]{\left\langle{#2}\right\rangle_{#1}}
\newcommand*{\prob}[2]{\M{\mathbb{P}\!\left\lbrack\left.{#1}\;\right\vert\;{#2}\right\rbrack}} 
\newcommand*{\deq}{\M{\mathrel{\mathop:}=}} 
\newcommand*{\deqr}{\M{=\mathrel{\mathop:}}} 
\newcommand{\T}[1]{\text{#1}} 
\newcommand*{\QT}[2][]{\M{\quad\T{#2}\ifthenelse{\equal{#1}{}}{\quad}{#1}}} 
\newcommand*{\ev}[3][]{\mathbb{E}_{#3}^{#1}\!\left\lbrack{#2}\right\rbrack}
\newcommand*{\evt}[3][]{\mathbb{E}_{#3}^{#1}#2}
\newcommand*{\cov}[3][]{\ifthenelse{\equal{#1}{}}{\mathbb{V}_{#3}\!\left\lbrack{#2}\right\rbrack}{\mathbb{V}_{#3}\!\left\lbrack{#2,#1}\right\rbrack}}
\newcommand*{\dev}[3][]{\ifthenelse{\equal{#1}{}}{\mathbb{D}_{#3}\!\left\lbrack{#2}\right\rbrack}{\mathbb{D}_{#3}\!\left\lbrack{#2,#1}\right\rbrack}}
\newcommand*{\covt}[2]{\mathbb{V}_{#2}{#1}}
\newcommand*{\devt}[2]{\mathbb{D}_{#2}{#1}}
\newcommand*{\gauss}[2]{\mathcal{N}\!\left({#1,#2}\right)}
\newcommand*{\uni}[2]{\mathrm{U}\!\left({#1,#2}\right)}
\newcommand*{\tgauss}[2]{\mathrm{N}({#1,#2})}
\newcommand*{\gaussd}[2]{\mathrm{N}^{\dagger}\!\left({#1,#2}\right)}
\newcommand*{\modulus}[1]{\M{\left\lvert{#1}\right\rvert}} 
\newcommand*{\norm}[1]{\M{\left\lVert{#1}\right\rVert}} 
\newcommand*{\ceil}[1]{\M{\left\lceil{#1}\right\rceil}} 
\newcommand*{\set}[1]{\M{\left\lbrace{#1}\right\rbrace}} 
\newcommand*{\setbuilder}[2]{\M{\left\lbrace{#1}\: \big\vert \:{#2}\right\rbrace}}
\newcommand*{\uniti}{\lbrack 0,1\rbrack}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\trace}{tr\!}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

% Sets running headers as well as PDF title and authors
\headers{Sobol' Matrices For Multi-Output Models With Quantified Uncertainty}{R. A. Milton, S. F. Brown, and A. S. Yeardley}

% Title. If the supplement option is on, then "Supplementary Material"
% is automatically inserted before the title.
\title{Sobol' Matrices For Multi-Output Models With Quantified Uncertainty\thanks{Submitted to the editors DATE.
\funding{This work was funded by the Fog Research Institute under contract no.~FRI-454.}}}

% Authors: full names plus addresses.
\author{Robert A. Milton\thanks{Department of Chemical and Biological Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom
  (\email{r.a.milton@sheffield.ac.uk}, \url{https://www.browngroupsheffield.com/}).}
\and Solomon F. Brown\thanks{Department of Chemical and Biological Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom
(\email{s.f.brown@sheffield.ac.uk}, \url{https://www.browngroupsheffield.com/}).}
\and Aaron S. Yeardley\thanks{Department of Chemical and Biological Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom
(\email{asyeardley1@sheffield.ac.uk}, \url{https://www.browngroupsheffield.com/}).}}

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
  Variance based global sensitivity usually measures the relevance of inputs to a single output using Sobol' indices. This paper extends the definition in a natural way to multiple outputs, directly measuring the relevance of inputs to the linkages between outputs in a correlation-like matrix of indices. The usual Sobol' indices constitute the diagonal of this matrix. Existence, uniqueness and uncertainty quantification are established by developing the indices from a putative regression model. Analytic expressions for generalized Sobol' indices and their standard deviations are given, and tested computationally against test functions whose ANOVA can be performed.
\end{abstract}

% REQUIRED
\begin{keywords}
  Global Sensitivity Analysis, Sobol' Index, Surrogate Model, Multi-Output, Gaussian Process, Uncertainty Quantification
\end{keywords}

% REQUIRED
\begin{MSCcodes}
  60G07,60G15,62J10, 62H99
\end{MSCcodes}

\section{Introduction}\label{sec:Intro}
    This paper is concerned with analysing the results of experiments or computer simulations in a design matrix of $M\geq 1$ input and $L\geq 1$ output columns, over $N$ rows (datapoints). Global Sensitivity Analysis (GSA) \cite{Razavi2021} examines the relevance of the various inputs to the various outputs. When pursued via ANOVA decomposition of a single output $L=1$, this leads naturally to the well known Sobol' indices, which have by now been applied across most fields of science and engineering \cite{Saltelli2019,Ghanem2017}. This paper extends the definition in a natural way to multiple outputs $L\geq 1$. Incidentally, the preference throughout this work is for uppercase constants and lowercase variables.

    The Sobol' decomposition apportions the variance of a single output to sets of one or more inputs \cite{Sobol2001}. We shall use ordinals of inputs, tuples which are conveniently also naive sets. 
    \begin{equation} \label{def:intro:m}
        \mi{m}\deq (0,\ldots ,m-1) \subseteq (0,\ldots ,M-1) \deqr \mi{M}    
    \end{equation}
    Obviously this restricts the subsets of inputs being studied, but this is a loss of convenience not generality, as any desired subset may be studied by ordering the inputs appropriately in advance.
    There is no generally accepted extension of Sobol' indices to multiple outputs $L>1$. 
    The discussion thus far, and almost all prior GSA, has dealt with a single (i.e scalar) output. With multiple (i.e vector) outputs, the Sobol' decomposition apportions the covariance matrix of outputs rather than the variance of a single output. With $L$ outputs, the closed Sobol' index $S_{\mi{m}}$ is generally a symmetric $\mi{L}\x \mi{L}$ matrix. The diagonal elements express the relevance of inputs to the output variables themselves. The off-diagonal elements express relevance to the linkages between outputs. This may be of considerable interest when outputs are, for example, yield and purity of a product, or perhaps a single output measured at various times. The Sobol indices reveal (amongst other things) which inputs it is worthwhile varying in an effort to alter the linkages between outputs. Prior work on Sobol' indices with multiple outputs \cite{Gamboa.etal2013,Xiao2017,GarciaCabrejo2014} has settled ultimately on just the diagonal elements of the covariance matrix, so this linkage remains unexamined. Although output covariance has been incoroporated indirectly in prior studies by performing principal component analysis (PCA) on ouputs prior to GSA on the (diagonal) variances of the resulting output basis \cite{Campbell2006}. This has been used in particular to study synthetic ``multi-outputs'' which are actually the dynamic response of a single output over time \cite{Lamboni2011, Zhang2020}.

    Accurate calculation of Sobol' indices even for a single output is computationally expensive and requires 10,000+ datapoints \cite{Lamoureux.etal2014}. A (sometimes) more efficient approach is calculation via a surrogate model, such as Polynomial Chaos Expansion (PCE) \cite{Ghanem.Spanos1997,Xiu.Karniadakis2002,Xiu2010}, low-rank tensor approximation \cite{Chevreuil.etal2015,Konakli.Sudret2016}, and support vector regression \cite{Cortes.Vapnik1995}. As well as being efficient, surrogate models also smooth out noise in the output, which is often highly desirable in practice. This paper employs one of the most popular surrogates, the Gaussian Processes (GP) \cite{Sacks.etal1989, Rasmussen.Williams2005} as it is highly tractable. We shall follow the multi-output form (MOGP) described in \cite{Alvarez.etal2011}, in order to examine the linkages between outputs.
    This paper deals exclusively with the anisotropic Radial Basis Function kernel, known as RBF/ARD, which is widely accepted as the kernel of choice for smooth outputs \cite{Rasmussen2016}. This uses the classic Gaussian bell curve to express the proximity of two input points, described in detail in \cref{sub:GPR:Tensor,sub:GPR:Prior}.

    Semi-analytic expressions for Sobol' indices are available for scalar PCEs \cite{Sudret2008}, and the diagonal elements of multi-output PCEs \cite{GarciaCabrejo2014}.
    Semi-analytic expressions for Sobol' indices of GPs have been provided in integral form by \cite{Oakley.OHagan2004} and alternatively by \cite{Chen.etal2005}. These approaches are implemented, examined and compared in \cite{Marrel.etal2009,Srivastava.etal2017}. Both \cite{Oakley.OHagan2004,Marrel.etal2009} estimate the errors on Sobol' indices in semi-analytic, integral form. Fully analytic, closed form expressions have been derived without error estimates for uniformly distributed inputs \cite{Wu.etal2016a} with an RBF kernel. There are currently no closed form expressions for MOGPs, or the errors on Sobol' indices, or any GPs for which inputs are not uniformly distributed. 

    In this paper we provide explicit, closed-form analytic formulae for the $\mi{L}\x \mi{L}$ matrices of closed Sobol' indices and their errors, for a class of MOGP with an RBF/ARD kernel applicable to smoothly varying outputs. 
    We transform uniformly distributed inputs $\rv{u}$ to normally distributed inputs $\rv{z}$ prior to fitting a GP and performing analytic calculation of closed Sobol' indices. This leads to relatively concise expressions in terms of exponentials, and enables ready calculation of the errors (variances) of these expressions. It also allows for an arbitrary rotation $\Theta$ of inputs, as normal variables are additive, whereas summing uniform inputs does not produce uniform inputs. If the goal is reducing inputs, rotating their basis first boosts the possibilities immensely \cite{Constantine2015}. It presents the possibilty of choosing $\Theta$ to maximise the closed Sobol' index of the first few inputs.

    The quantities to be calculated and their formal context are introduced in \cref{sec:GSI}, assuming only that the output is an integrable function of the input. Our approach effectively regards a regression model which quantifies uncertainty with each prediction as just another name for a stochastic process. A great deal of progress is made in \cref{sec:CK} using general stochastic (not necessarily Gaussian) processes. This approach is analytically cleaner, as it is not obfuscated by the GP details. Furthermore, it turns out that the desirable properties of the Gaussian (lack of skew, simple kurtosis) are not actually helpful, as these terms cancel of their own accord. This development leaves just two terms to be calculated, which require the stochastic process to be specified. MOGPs with an RBF/ARD kernel are tersely developed and described in \cref{sec:GPR}, then used to calculate the two unknown terms in \cref{sec:GPMom,sec:GPEst}. Methods to reduce computational complexity are discussed in \cref{sec:Complexity}.Conclusions are drawn in \cref{sec:Conc}.

    The void ordinal $\mi{0}\deq ()$ voids any tensor it subscripts.
\pagebreak
\section{Multi-output models with quantified uncertainty}\label{sec:MQU}
    The purpose of this Section is to set the scene for this study, serve as a glossary of notation, and prove the formal foundation of the construction to follow. Our notation is not entirely standard, but is preferred for lightness and fluency once grasped. Regarding notation and other topics, the intention is to facilitate computation by combining efficient, tensorized operations such as {\tt einsum} \cite{Numpy2023,PyTorch2023,Tensorflow2023} into numerically stable calculations. As such, the devices employed herein will be familiar to practitioners of machine learning.

    Throughout this paper, square bracketed quantities such as $\tte[\mi{M\x o}]{u}$ are tensors subscripted by (the cartesian product of) their ranks, for bookkeeping. Ranks are expressed as ordinals of axes, tuples which are conveniently also naive sets and will be subtracted as such
    \begin{equation*} \label{def:MQU:m}
        \begin{aligned}
            \mi{0} \deq () \subseteq{} \mi{m}&\deq (0,\ldots ,m-1) \subseteq \mi{M} \deq (0,\ldots ,M-1) \\   
            \mi{M-m} &\deq \setbuilder{m^{\prime} \in \mi{M}}{m^{\prime} \notin \mi{m}} = (m, \ldots, M-1)
        \end{aligned}
    \end{equation*}
    GSA decomposes input space by sets of axes, and this notation facilitates that. It does assume that input axes are expediently ordered already, which might impair convenience but not generality. Conventional indexing is permitted under this scheme, by allowing singleton axes to be written in italic without parentheses.

    From a Bayesian perspective a determined tensor is a function of determined input(s), a random variable (RV) a function of undetermined input, and a stochastic process (SP) a function of both determined and undetermined inputs. In any case, all sources of (sans-serif) uncertainty may be gathered in a single input $\tte[M]{\rv{u}}$, final to the definition of input space:
    \begin{equation} \label{def:MQU:input}
        \begin{aligned}
            \mi{M} &\deq (0,\ldots,M-1) \QT{determined inputs on the unit interval} &\te[\mi{M}]{u}\in \uniti^{M} \\
            M &\deqr \mi{M+1-M} \quad\QT{an undetermined input on the unit interval} &\te[M]{\rv{u}} \sim \uni{0}{1}
        \end{aligned}   
    \end{equation}
    Exponentiation is always categorical -- repeated cartesian $\x$ or tensor $\otimes$ -- unless stated otherwise.
    It is crucial to this work that all $\mi{M+1}$ inputs vary completely independently of each other -- they are in no way codependent or correlated.
    Uncertainty is distributed uniformly $\tte[M]{\rv{u}} \sim \uni{0}{1}$ to exploit the ``universality of the uniform'' \cite[pp.224]{Blitzstein2019}, better known as the inverse probability integral transform \cite{Angus1994}. This explicitly provides a bijective mapping to any RV $\tte[M]{\rv{\tilde{u}}}$ by inverting its cumulative distribution function (CDF) $\tilde{\mathrm{F}} \colon \tte[M]{\rv{\tilde{u}}} \mapsto \tte[M]{\rv{u}}$ in the generalized sense
    \begin{equation*}
        \tilde{\mathrm{F}}^{-1}(\tte[M]{u}) \deq \inf \setbuilder{\tte[M]{\tilde{u}}}{\tilde{\mathrm{F}}(\tte[M]{\tilde{u}}) \geq \tte[M]{u}}        
    \end{equation*}
    This mapping $\tilde{\mathrm{F}} \colon \tte[M]{\rv{u}} \mapsto \tte[M]{\rv{\tilde{u}}}$ is the quantile function of $\tte[M]{\rv{\tilde{u}}}$ on $\uniti$, and embedding it in an output model $y$ can express any continuous RV at a given determined input $\tte[\mi{M}]{u}$. In other words, the output model could be any continuous SP.
    The amenity of definition \cref{def:MQU:input} is that each input $\te[m]{\rv{u}}$ is automatically a probability measure, for any $m\in\mi{M}$.

    Throughout this work, expectations are subscripted by the inputs marginalized
    \begin{equation*}
        \ev{\bullet }{\#} \deq \int_{\te[\#]{0}}^{\te[\#]{1}} \te{\bullet} \, \mathrm{d}\te[\#]{u} \qquad \forall \# \subseteq \mi{M+1} \T{ or } \# \in \mi{M+1}
    \end{equation*}
    Covariances invoke the tensor product (summing the ranks of the arguments), and carry the subscript of their underlying expectation
    \begin{equation*}
        \cov[\te{*}]{\te{\bullet}}{\#} \deq \ev{\te{\bullet}\otimes\te{*}}{\#} - \ev{\bullet}{\#} \otimes \ev{*}{\#} \qquad \forall \# \subseteq \mi{M+1} \T{ or } \# \in \mi{M+1}
    \end{equation*}
    The covariance of anything with itself is expressed with a single argument $\cov{\bullet}{\mi{\#}} \deq \cov[\bullet]{\bullet}{\mi{\#}}$, as is customary.

    A multi-output model with quantified uncertainty (MQU) is defined as any Lebesgue integrable function of input space \cref{def:MQU:input}
    \begin{equation} \label{def:MQU:y}
        \begin{gathered}
            y \colon \uniti^{M+1} \rightarrow \st{R}^{L} \QT{such that} \\ \covt{\evt{\te[l]{y(\te[\mi{M+1}]{\rv{u}})}}{M}}{\mi{M}} > \evt{\evt{\te[l]{y(\te[\mi{M+1}]{\rv{u}})}}{M}}{\mi{M}} = 0 \quad \forall l\in\mi{L}            
        \end{gathered}
    \end{equation}
    To avoid division by zero in GSA, every output component $l\in\mi{L}$ must depend on at least one determined input: constant functions and pure noise are not allowed.
    Without loss of generality we have also offset the model to have zero mean $\tte[\mi{L}]{0}$. This is algebraically unnecessary, but it can be crucial to numerical stability of computation.
    % We ascertain no advantage, however, in enforcing the customary and cogent principle of unbiased output uncertainty (noise)
    % \begin{equation*}
    %     \ev{y}{M} = \left(y \big\vert \te[M]{\rv{u}}=1/2\right)
    % \end{equation*}

    Our notation will reflect machine learning practice \cite{Numpy2023,PyTorch2023,Tensorflow2023} which facilitates parallel computation by tensorizing function application over batch dimensions such as $\mi{o}$ according to
    \begin{equation*}
        \te[\mi{L}\x \dot{o}]{y(\te[\mi{M+1\x o}]{\rv{u}})} \deq y(\te[\mi{M+1}\x \dot{o}]{\rv{u}}) \qquad \forall \dot{o} \in \mi{o}
    \end{equation*}

    To elucidate the meaning and purpose of an MQU, and set the stage for GSA, consider a design matrix of $\tte[\mi{M\x o}]{u}$ inputs alongside $\tte[\mi{L\x o}]{Y}$ outputs:
    \begin{equation*}
        \begin{array}{lllll}
            \te[0\x 0]{u} & \cdots & \te[M-1\x 0]{u} \\
            \phantom{t}\vdots & \ddots & \phantom{t}\vdots \\
            \te[0\x o-1]{u} & \cdots & \te[M-1\x o-1]{u} \\
        \end{array}
        \begin{array}{lllll}
            \te[0\x 0]{Y} & \cdots & \te[L-1\x 0]{Y} \\
            \phantom{t}\vdots & \ddots & \phantom{t}\vdots \\
            \te[0\x o-1]{Y} & \cdots & \te[L-1\x o-1]{Y} \\
        \end{array}
    \end{equation*}
    The sole function of an MQU is actually to provide a single number for any such design matrix of $o\in\st{Z}^{+}$ samples (rows): namely, the probability that the output samples result from the input samples (to within some error $\sigma>0$)
    \begin{equation*}
        \prob{\left(y(\te[\mi{M+1\x o}]{\rv{u}})-\te[\mi{L\x o}]{Y}\right)^{2} < \sigma^{2}}{\te[\mi{M\x o}]{\rv{u}}=\te[\mi{M\x o}]{u}}
    \end{equation*}
    This is what it means to generate $L$ quantifiably uncertain outputs from $M$ determined inputs.
    Following the Kolmogorov extension theorem \cite[pp.124]{Rogers.Williams2000}, the meaning of an MQU is nothing other than an SP. This includes fully determined MQUs -- $y(\tte[\mi{M+1}]{\rv{u}})$ which do not depend on the final, undetermined input $\tte[M]{\rv{u}}$ -- as SPs with zero variance. Kolmogorov's extension theorem formally identifies an SP -- a collection $y(\tte[\mi{M+1}]{\rv{u}}) = \setbuilder{y(\tte[\mi{M+1}]{\rv{u}})}{\tte[\mi{M}]{\rv{u}}=\tte[\mi{M}]{u}}$ of RVs indexed by determined inputs $\tte[\mi{M}]{u}$ -- with all its finite dimensional distributions $\setbuilder{y(\tte[\mi{M+1\x o}]{\rv{u}})}{\tte[\mi{M\x o}]{\rv{u}}=\tte[\mi{M\x o}]{u}}$. The latter viewpoint is the formal version of the random field \cite{Khoshnevisan2002} or random function interpretation of SPs \cite[pp.42]{Skorokhod2005} frequently alluded to in machine learning \cite{Rasmussen.Williams2005}.
    It can be a helpful perspective on the objects appearing in this work.

    In conception MQUs include any explicit or black-box function (simulation), surrogate (emulator, response suface, meta-model) or regression which supplies a probability distribution (even a zero variance one) on its outputs (predictions). In practice, such an MQU is usually inferred from a design of experiments (DOE) which samples the determined inputs $\tte[\mi{M}]{u}$ from $\uni{0}{1}^{M}$. This is not restrictive, a preferred set of determined inputs $\tte[\mi{M}]{x}$ may have any sampling distribution whatsoever, so long as it is continuous and its $M$ dimensions mutually independent. Simply take the CDF of $\tte[\mi{M}]{\rv{x}}$ to obtain $\tte[\mi{M}]{\rv{u}}$ before beginning, and apply the quantile function of $\tte[\mi{M}]{\rv{x}}$ (described above and in \cite{Angus1994}) after finishing. In general and in summary, simply treat any $\tilde{y}(\tte[\mi{M+1}]{\rv{x}})$ as the pullback of an MQU by the CDF of $\tte[\mi{M+1}]{\rv{x}}$.

    To close this Section we shall formally secure the construction which follows.
    An MQU $y$ is Lebesgue integrable by definition \cref{def:MQU:y}, so it must be measurable. All measures throughout this work are finite (probability measures, in fact), so integrability of $y$ implies integrability of $y^n$ for all $n \in \st{Z}^{+}$ \cite{Villani1985}.
    Therefore, Fubini's theorem \cite[pp.77]{Williams1991} allows all expectations to be taken in any order (which will be crucial later). We can thus safely construct any SP which is polynomial in $y$ and its marginals; and freely extract finite dimensional distributions from it by the Kolmogorov extension theorem \cite[pp.124]{Rogers.Williams2000}. This guarantees existence and uniqueness of every device in this paper. 

    
    \section{GSA Sobol' indices}\label{sec:GSAI}
    This Section recapitulates the definition of classical Sobol' indices in the context of an MQU, to prepare the way for easy generalization to Sobol' matrices in the next Section. Sobol' indices are traditionally constructed \cite{Sobol2001} from a High Dimensional Model Representation (HDMR or Hoeffding-Sobol' decomposition \cite{Chastaing2011}). This is enlightening, but not strictly necessary. Instead, we shall swiftly construct the indices from an MQU. Later, this will allow us to derive standard errors on the Sobol' indices.

    In essence, GSA is performed by marginalizing (obscuring) determined input axes selectively. 
    A reduced model (mQU) is an SP defined as
    \begin{equation}\label{def:GSAI:y_m}
    \te[\mi{L}]{\rv{y}_{\mi{m}}} \deq \ev{y(\te[\mi{M+1}]{\rv{u}})}{\mi{M-m}}_{\mi{L}} \qquad \forall \mi{m} \subseteq \mi{M}
    \end{equation}
    When $\mi{M-m}=\mi{0}$, nothing is marginalized and $\rv{y}_{\mi{M}}$ is called the full model (MQU).
    The undetermined input $\tte[M]{\rv{u}}$ is never marginalized, and each mQU depends on $\mi{m}$ determined inputs $\tte[\mi{m}]{u}$ alongside $\tte[M]{\rv{u}}$. Any distributional assumption about inputs simply pulls back (factors through) expectations here with no impact whatsoever: $\uni{0}{1}$ is no more than a proxy for the CDF of any continuous distribution.

    An mQU $\te[\mi{L}]{\rv{y}_{\mi{m}}}$ is not automatically converted to a new MQU over $\te[\mi{m+1}]{\rv{u}}$ by re-indexing $\te[M]{\rv{u}}$ to $\te[m]{\rv{u}}$. This would not be the same as simply hiding columns $\mi{M-m}$ in the design matrix of the previous Section. In general the new MQU would not adequately reflect the orginal full model, because any output variation due to marginalized inputs $\te[\mi{M-m}]{u}$ has simply been lost: it cannot be re-allocated to the new undetermined input $\te[m]{\rv{u}}$. This gets to the heart of GSA, assessing the degree to which a reduced model mQU mimics its full model. Which is a matter of assessing how much output variation has been lost through marginalization.
    
    The marginal variance of an mQU is a tensor-valued RV (function of $\te[M]{\rv{u}}$) defined as
    \begin{equation}\label{def:GSAI:V_m}
        \cov{\rv{y}_{\mi{m}}}{\mi{m}}_{\mi{L\x L}} \deq \cov{\te[\mi{L}]{\rv{y}_{\mi{m}}}}{\mi{m}} 
    \end{equation}
    It is -- for all realisations $\te[M]{\rv{u}}$=$\te[M]{u}$ -- a symmetric tensor by definition and positive semi-definite by Jensen's inequality. We may therefore define a vector RV which is the square root of its diagonal, namely the standard deviation $\dev{\rv{y}_{\mi{m}}}{\mi{m}}_{\mi{L}}$
    \begin{equation}\label{def:GSAI:D_m}
        \dev{\rv{y}_{\mi{m}}}{\mi{m}}_{l} \deq \sqrt{\cov{\rv{y}_{\mi{m}}}{\mi{m}}_{lxl}} > 0 \qquad \forall l \in \mi{L}
    \end{equation}
    This is positive definite due to the positive variance clause in the definition \cref{def:MQU:y} of an MQU.

    The closed Sobol' index of input axes $\mi{m}$ with respect to scalar output $\tte[l]{y(\tte[\mi{M+1}]{\rv{u}})}$ is defined as an RV
    \begin{equation}\label{def:GSAI:S_m}
            \te[l]{\rv{S}_\mi{m}} \deq \frac{\cov{\rv{y}_{\mi{m}}}{\mi{m}}_{l\x l}}{\cov{\rv{y}_{\mi{M}}}{\mi{M}}_{l\x l}}
    \end{equation}
    The definition originated \cite{Sobol1993} for fully determined functions ($y$ independent of $\tte[M]{\rv{u}})$. It was later extended to random inputs \cite{Sobol.Kucherenko2005} and Gaussian processes \cite{Oakley.OHagan2004,Marrel.etal2009}. The latter work introduced the Sobol' index as an RV constructed from an important category of SP.

    The complement of a closed index is called a total index \cite{Homma1996}
    \begin{equation}\label{def:GSAI:ST_m}
        \te[l]{\rv{S}^{T}_\mi{M-m}} \deq 1 - \te[l]{\rv{S}_\mi{m}}
    \end{equation}

    Sobol' indices are readily intepreted, given that codependent inputs (reviewed in \cite{PWiederkehrThesis}) are beyond the scope of this work. By Definition \cref{def:GSAI:S_m} a closed index $S_{\mi{m}} \in \uniti$ measures the proportion of output variance captured by the reduced model $\rv{y}_{\mi{m}}$. This, in turn, indicates the influence or relevance of input axes $\mi{m}$. The full model $\rv{y}_\mi{M}$ explains everything explicable, so its Sobol' index is $\tte[l]{\rv{S}_{\mi{M}}}=\tte[l]{\rv{S}^{T}_{\mi{M}}}=1$. The void model $\rv{y}_\mi{0}$ is just the mean output -- an RV depending only on undetermined noise $\te[M]{\rv{u}}$ -- which explains nothing, so its Sobol' index is $\tte[l]{\rv{S}_{\mi{0}}}=\tte[l]{\rv{S}^{T}_{\mi{0}}}=0$.

    The closed index of a single input axis $m\in \mi{M}$ is called a first-order index
    \begin{equation}\label{def:GSAI:S_mp}
        \te[l]{\rv{S}_{m}} \deq \te[l]{\rv{S}_\mi{m+1-m}}
    \end{equation}
    Because inputs may cooperate to affect the output, a closed index often exceeds the sum of its first-order contributions, obeying (for any realization $\tte[M]{\rv{u}}=\tte[M]{u}$)
    \begin{equation}\label{eq:GSAI:order}
        \sum_{\dot{m} \in \mi{m}} \te[l]{\rv{S}_{\dot{m}}} \leq \te[l]{\rv{S}_{\mi{m}}} \leq  \te[l]{\rv{S}^{T}_{\mi{m}}}
    \end{equation}
    The final inequality observes that a closed index only includes cooperation of input axes $\mi{m}$ with each other, whereas a total index also includes cooperation between $\mi{m}$ and $\mi{M-m}$ (but excludes cooperation of input axes $\mi{M-m}$ with each other. This is the only difference between closed and total indices, but it is an important one.

    Regarding the covariance between any reduced model and its full model, Fubini's theorem safeguards the intuitive identity
    \begin{equation} \label{def:GSAI:V_mM}
        \begin{aligned}
            \cov[\rv{y}_{\mi{M}}]{\rv{y}_{\mi{m}}}{\mi{M}}_{\mi{L\x L}} &\deq \ev{\te[\mi{L}]{\rv{y}_{\mi{m}}} \otimes \te[\mi{L}]{\rv{y}_{\mi{M}}}}{\mi{M}} - \evt{\te[\mi{L}]{\rv{y}_{\mi{m}}}}{\mi{M}} \otimes \evt{\te[\mi{L}]{\rv{y}_{\mi{M}}}}{\mi{M}} \\
            &\phantom{:}= \ev{\te[\mi{L}]{\rv{y}_{\mi{m}}} \otimes \te[\mi{L}]{\rv{y}_{\mi{M}}}}{\mi{m}} - \te[\mi{L}]{\rv{y}_{\mi{0}}} \otimes \te[\mi{L}]{\rv{y}_{\mi{0}}} \\
            &\deqr \cov{\rv{y}_{\mi{m}}}{\mi{m}}_{\mi{L\x L}}
        \end{aligned}
    \end{equation}
    Using this, the correlation between (the scalar output predicted by) an mQU and its parent MQU is an RV
    \begin{equation} \label{def:GSAI:R_m}
        \te[l]{\rv{R}_{\mi{m}}} \deq \frac{\cov[\rv{y}_{\mi{M}}]{\rv{y}_{\mi{m}}}{\mi{M}}_{l\x l}}{\dev{\rv{y}_{\mi{m}}}{\mi{m}}_{l} \dev{\rv{y}_{\mi{M}}}{\mi{M}}_{l}}
        = \frac{\dev{\rv{y}_{\mi{m}}}{\mi{m}}_{l}}{\dev{\rv{y}_{\mi{M}}}{\mi{M}}_{l}}
    \end{equation}
    The square of the correlation is a discerning measure of the quality of regression \cite{Chicco2021} called the coefficient of determination. Which is now clearly identical to the closed Sobol' index
    \begin{equation} \label{def:GSAI:R2}
        \te[l]{\rv{R}_{\mi{m}}}^{2} = \frac{\cov{\rv{y}_{\mi{m}}}{\mi{m}}_{l}}{\dev{\rv{y}_{\mi{M}}}{\mi{M}}_{l} \dev{\rv{y}_{\mi{M}}}{\mi{M}}_{l}} \deqr \te[l]{\rv{S}_{\mi{m}}}
    \end{equation}
    Sobol' indices are used to identify reduced models $\tte[l]{\rv{y}_{\mi{m}}}$ which adequately mimic the full model $\tte[l]{\rv{y}_{\mi{M}}}$.
    A closed index close to 1 confirms that the two models make nearly identical predictions. Simplicity and economy (not least of calculation) motivate the adoption of a reduced model, a closed Sobol' index close to 1 is what justifies it. This is precisely equivalent to screening out (obscuring) $\mi{M-m}$ input axes on the grounds that their influence on $\tte[l]{\rv{y}_{\mi{M}}}$ -- measured by their total index $\te[l]{S^{T}_\mi{M-m}}$ -- is close to 0.


\section{GSA Matrices}\label{sec:GSAM}
    This Section introduces a tensor RV called the Sobol' matrix of an mQU. This is defined by tensorizing the definitions of the previous Section, replacing $l$ for a scalar output with $\mi{L\x L}$ for a vector output. All division is performed element-wise, inverting Hadamard multiplication $\circ$
    \begin{equation}\label{def:GSAM:div}
        \te[\#]{*} = \frac{\te[\#]{\bullet}}{\te[\#]{\star}} \quad \Longleftrightarrow
        \quad \te[\#]{*} \circ \te[\#]{\star} = \te[\#]{\bullet}
    \end{equation}

    The tensorization of definition \cref{def:GSAI:S_m} is problematic, as it may invoke division by zero. Instead, we shall tensorize the alternative definition \cref{def:GSAI:R2} of a closed Sobol' index as a coefficient of determination. We shall then compare the two definitions in the light of a toy example.

    The closed Sobol' matrix of input axes $\mi{m}$ with respect to vector output $\tte[\mi{L}]{y(\tte[\mi{M+1}]{\rv{u}})}$ is defined as the tensor RV
    \begin{equation}\label{def:GSAM:S_m}
            \te[\mi{L\x L}]{\rv{S}_\mi{m}} 
            \deq \frac{\cov{\rv{y}_{\mi{m}}}{\mi{m}}_{\mi{L\x L}}}{\dev{\rv{y}_{\mi{M}}}{\mi{M}}_{\mi{L}} \otimes \dev{\rv{y}_{\mi{M}}}{\mi{M}}_{\mi{L}}}
    \end{equation}
    By this definition, the closed Sobol' matrix of the full model is just the ordinary correlation matrix of the components of output vector $y$
    \begin{equation}\label{def:GSAM:R_m}
        \te[\mi{L\x L}]{\rv{R}} 
        \deq \frac{\cov{\rv{y}_{\mi{M}}}{\mi{M}}_{\mi{L\x L}}}{\dev{\rv{y}_{\mi{M}}}{\mi{M}}_{\mi{L}} \otimes \dev{\rv{y}_{\mi{M}}}{\mi{M}}_{\mi{L}}}
        \deqr \te[\mi{L\x L}]{\rv{S}_\mi{M}}
    \end{equation}
    The complement of a closed Sobol' matrix is called a total Sobol' matrix
    \begin{equation}\label{def:GSAM:ST_m}
        \te[\mi{L\x L}]{\rv{S}^{T}_\mi{M-m}} \deq \te[\mi{L\x L}]{\rv{S}_{\mi{M}}} - \te[\mi{L\x L}]{\rv{S}_\mi{m}}
    \end{equation}
    Let us emphasise that these definitions are equivalent to the Sobol' index definitions \cref{def:GSAI:S_m,def:GSAI:ST_m} on the Sobol' matrix diagonal
    \begin{equation*}
        \te[l\x l]{\rv{S}_\mi{m}} =\te[l]{\rv{S}_\mi{m}} \QT{and} \te[l\x l]{\rv{S}^{T}_\mi{m}} =\te[l]{\rv{S}^{T}_\mi{m}}
        \qquad \forall l\in\mi{L}
    \end{equation*}

    Just as a Sobol' index is a proportion of output variance, each element of a Sobol' matrix is a component of output correlation. Unlike the former, the latter can be negative.
    It is still sometimes useful to examine the first-order matrix for a single input axis $m$
    \begin{equation}\label{def:GSAM:S_mp}
        \te[\mi{L\x L}]{\rv{S}_{m}} \deq \te[\mi{L\x L}]{\rv{S}_\mi{m+1-m}}
    \end{equation}
    However, there is no ordering analagous to \cref{eq:GSAI:order} as contributions of opposing sign may offset each other. All one can rely on -- elementwise for any realization $\tte[M]{\rv{u}}=\tte[M]{u}$ -- is
    \begin{subequations}\label{eq:GSAM:order}
        \begin{gather}
            \te[\mi{L\x L}]{\rv{S}_\mi{0}}=\te[\mi{L\x L}]{\rv{S}^{T}_\mi{0}} = \te[\mi{L\x L}]{0} \\
            \te[\mi{L\x L}]{\rv{S}_\mi{M}}=\te[\mi{L\x L}]{\rv{S}^{T}_\mi{M}} =\te[\mi{L\x L}]{\rv{R}} \\
            \te[\mi{L\x L}]{0} \leq
            \modulus{\te[\mi{L\x L}]{\rv{S}_\mi{m}}} , \modulus{\te[\mi{L\x L}]{\rv{S}^{T}_\mi{m}}} \leq
            \modulus{\te[\mi{L\x L}]{\rv{R}}}
        \end{gather}
    \end{subequations}
    So the void model $\tte[\mi{L}]{\rv{y}_{\mi{0}}}$ still explains nothing, the full model $\tte[\mi{L}]{\rv{y}_{\mi{0}}}$ still explains everything explicable, and every other mQU lies somewhere in between.

    Let us pursue an informative toy example to illustrate this. Take some even $M$ and define
    \begin{equation*}
            w \colon \uniti^{M/2} \to \st{R} \QT{such that} \ev{w}{\mi{M/2}} = 0 \QT{and} \cov{w}{\mi{M/2}} = 1
    \end{equation*}
    to construct the ($L=2$) MQU
    \begin{equation*}
        y(\te[\mi{M+1}]{\rv{u}}) = 
        \begin{bmatrix}
            w(\te[\mi{M/2}]{\rv{u}}) + w(\te[\mi{M-M/2}]{\rv{u}}) \\
            w(\te[\mi{M/2}]{\rv{u}}) - w(\te[\mi{M-M/2}]{\rv{u}})
        \end{bmatrix}
    \end{equation*}
    The Sobol' matrix of the full model is easily calculated
    \begin{equation*}
        {\rv{S}_\mi{M}} = 
        \begin{bmatrix}
            1 & 0 \\
            0 & 1
        \end{bmatrix}
    \end{equation*}
    Two important reduced models have the Sobol' matrices
    \begin{equation*}
        {\rv{S}_\mi{M/2}} = {\rv{S}^{T}_\mi{M/2}} = 
        \begin{bmatrix}
            1/2 & 1/2 \\
            1/2 & 1/2
        \end{bmatrix}
        \QT{;}
        {\rv{S}_\mi{M-M/2}} = {\rv{S}^{T}_\mi{M-M/2}} = 
        \begin{bmatrix}
            \phantom{-}1/2 & -1/2 \\
            -1/2 & \phantom{-}1/2
        \end{bmatrix}
    \end{equation*}
    The Sobol' matrices reveal that input axes $\mi{M/2}$ influence both outputs in the same sense, while input axes $\mi{M-M/2}$ influence the two outputs in opposite sense.
    In real-world examples, this information is often valuable. Perhaps the two components of $y$ are the yield and the purity of a pharamaceutical product, or the efficacy and the cost of CO$_{2}$ capture at a power plant.
    We can also ascertain that axes in $\mi{M/2}$ do not cooperate with axes in $\mi{M-M/2}$, as witnessed by total Sobol' matrices equal to closed Sobol' matrices.

    The direct tensorization of Sobol' index definition \cref{def:GSAI:S_m} would define the matrix
    \begin{equation*}
            \te[\mi{L\x L}]{\rv{\hat{S}}_\mi{m}} \deq \frac{\te[\mi{L\x L}]{\rv{S}_\mi{m}}}{\te[\mi{L\x L}]{\rv{S}_\mi{M}}} \QT{which is undefined whenever} \te[l\x l^{\prime}]{\rv{S}_\mi{M}}=0 \T{ for some } l,l^{\prime} \in \mi{L}
    \end{equation*}
    In our toy example, the most interesting information -- the off-diagonal indices for two important reduced models -- is undefined and completely lost by this definition.

    We close this section by considering how a Sobol' matrix may be summarised in a single number. This is often desired to present ``the'' sensitivity of the vector output to a reduced set of inputs $\mi{m}$. The simple answer is to define a seminorm \cite[pp.314]{Schechter1997} (like a norm, only positive semidefinite intead of positive definite) on the Sobol' matrix. The seminorm should be chosen according to one's interest in the outputs. For example, one could use the determinant $\modulus{\tte[\mi{l\x l}]{\rv{S}_{\mi{m}}}}$ of a chosen submatrix $\mi{l}\subseteq\mi{L}$, or the modulus in case $\mi{l}$ is singleton. The Sobol' matrix provides a platform for investigating a variety of such measures. However, all matrices and measures remain RVs at this point, and the next two sections are devoted to extracting statistics -- i.e. determined quantities -- from these.


\section{GSA Statistics}\label{sec:GSAS}
    At this stage, Sobol' matrices have been robustly defined as tensor RVs -- i.e. functions of the undetermined input $\tte[M]{\rv{u}}$. This section extracts two determined matrix statistics from each RV: its expected value and its standard deviation or error.

    Expected values are written as the italic version of the underlying RV, starting with
    \begin{subequations}\label{def:GSAS:VD}
        \begin{align}
            \QT{marginal variance} \te[\mi{L\x L}]{V_\mi{m}} &\deq \evt{\,\cov{\rv{y}_{\mi{m}}}{\mi{m}}}{M}_{\mi{L\x L}}  \label{def:GSAS:V} \\
            \QT{marginal deviation} \te[\mi{L}]{D_\mi{m}} &\deq \evt{\,\dev{\rv{y}_{\mi{m}}}{\mi{m}}}{M}_{\mi{L}}
            \quad \Longrightarrow \quad
            \te[l]{D_\mi{m}} = \sqrt{\te[l\x l]{V_\mi{m}}} \quad \forall l\in\mi{L} \label{def:GSAS:D}
        \end{align}
    \end{subequations}
    The GSA statistics used to assess input relevance are the expected Sobol' matrices
    \begin{subequations}\label{def:GSAS:SST}
        \begin{align}
            \QT{closed Sobol' matrix}\te[\mi{L}^{2}]{S_\mi{m}} &\deq \evt{\te[\mi{L}^{2}]{\rv{S}_\mi{m}}}{M} 
            = \frac{\te[\mi{L}^{2}]{V_\mi{m}}}{\te[\mi{L}]{D_\mi{m}} \otimes \te[\mi{L}]{D_\mi{m}}} \\    
            \QT{total Sobol' matrix}\te[\mi{L}^{2}]{S^{T}_\mi{M-m}} &\deq \evt{\te[\mi{L}^{2}]{\rv{S}^{T}_\mi{M-m}}}{M} = \te[\mi{L}^{2}]{S_\mi{M}} - \te[\mi{L}^{2}]{S_\mi{m}}
        \end{align}
    \end{subequations}

    In order to express standard errors -- which are due to undetermined input $\tte[M]{\rv{u}}$ -- the notation introduced in \cref{def:GSAI:D_m} is extended to define the matrix standard deviation $\devt{\te[\mi{L\x L}]{\bullet}}{M}$ of a matrix RV $\tte[\mi{L\x L}]{\bullet}$
    \begin{equation}\label{def:GSAS:D_m}
        \devt{\te[l\x l^{\prime}]{\bullet}}{M} \deq \sqrt{\cov{\te[l\x l^{\prime}]{\bullet}}{M}} \qquad \forall l,l^{\prime} \in \mi{L}
    \end{equation}
    The GSA statistics used to assess the standard error of Sobol' matrices are
    \begin{subequations}\label{def:GSAS:TTT}
        \begin{align}
            \QT{closed Sobol' matrix error}\te[\mi{L}^{2}]{T_\mi{m}} &\deq \devt{\te[\mi{L}^{2}]{\rv{S}_\mi{m}}}{M} \label{def:GSAS:T}\\
            \QT{total Sobol' matrix error}
            \te[\mi{L}^{2}]{T_{\mi{M-m}}^{T}} &\deq 
            \te[\mi{L}^{2}]{T_{\mi{M}}} + \te[\mi{L}^{2}]{T_{\mi{m}}}
            \geq \devt{\,\te[\mi{L}^{2}]{\rv{S}_{\mi{M-m}}^{T}}}{M}
        \end{align}
    \end{subequations}
    The total Sobol' matrix error is a conservative statistic which achieves equality on the diagonal, and is robust and sufficiently precise for most practical purposes off the diagonal.
    Attempts at greater precision are apt to produce wildly unreliable estimates when implemented on a computer.
    This is because the total Sobol' matrix \cref{def:GSAM:ST_m} is the difference between two terms which are often highly correlated with each other. Tiny differences between correlated terms are swamped by numerical error in implementation, so the resulting computation is apt to yield garbage in practice.

    To calculate the closed Sobol' matrix error $\tte[\mi{L\x L}]{T_\mi{m}}$ we use the Taylor series method \cite[pp.353]{Kendall1994}, which is valid provided $\covt{\,\te[l\x l]{\rv{y}_{\mi{M}}}}{\mi{M}}$ is well approximated by its mean
    \begin{equation*}
        \te[l\x l]{V_{\mi{M}}} \gg \big\vert\covt{\,\te[l\x l]{\rv{y}_{\mi{M}}}}{\mi{M}} - \te[l\x l]{V_{\mi{M}}}\big\vert
    \end{equation*}
    This is essentially the positive variance clause in the definition \cref{def:MQU:y} of an MQU, designed to prohibit constant or pure noise outputs which do not depend on any of the determined input axes $\mi{M}$.

    The covariance between two marginal covariance matrix RVs is the determined 4-tensor
    \begin{equation}\label{def:GSAS:W}
        \te[\mi{L}^{4}]{W_{\mi{mm^{\prime}}}} \deq \cov[\cov{\rv{y}_{\mi{m^{\prime}}}}{\mi{m^{\prime}}}_{\mi{L\x L}}]{\cov{\rv{y}_{\mi{m}}}{\mi{m}}_{\mi{L\x L}}}{M} \qquad \forall \mi{m},\mi{m^{\prime}} \in \mi{M}
    \end{equation}
    used to define the determined matrix $\tte[\mi{L\x L}]{Q_{\mi{m}}}$ elementwise
    \begin{equation}\label{def:GSAS:Q}
        \begin{aligned}
            \te[l\x l^{\prime}]{Q_{\mi{m}}} &\deq 
            \te[(l\x l^{\prime})^{2}]{W_{\mi{mm}}} \\
            &\phantom{\deq}- \te[l\x l^{\prime}]{V_{\mi{m}}} \sum_{l^{\ddag} \in \set{l, l^{\prime}}} \frac{\te[l^{\ddag}\x l^{\ddag}\x l\x l^{\prime}]{W_{\mi{mM}}}} {\te[l^{\ddag}]{D_{\mi{M}}}^{2}} \\                
            &\phantom{\deq}+ \frac{\te[l\x l^{\prime}]{V_{\mi{m}}}^{2}}{4} \sum_{l^{\ddag} \in \set{l, l^{\prime}}} 
            \frac{\te[l^{\ddag}\x l^{\ddag}\x l\x l]{W_{\mi{MM}}}} {\te[l^{\ddag}]{D_{\mi{M}}}^{2} \te[l]{D_{\mi{M}}}^{2}}
            + \frac{\te[l^{\ddag}\x l^{\ddag}\x l^{\prime}\x l^{\prime}]{W_{\mi{MM}}}} {\te[l^{\ddag}]{D_{\mi{M}}}^{2} \otimes \te[l^{\prime}]{D_{\mi{M}}}^{2}} \\
        \end{aligned}
    \end{equation}
    The Taylor series estimate of the closed Sobol matrix error \cref{def:GSAS:T} is finally calculated elementwise as
    \begin{equation} \label{eq:GSAS:Tm}
        \te[l\x l^{\prime}]{T_{\mi{m}}} \deq 
        \devt{\,\te[l\x l^{\prime}]{\rv{S}_{\mi{m}}}}{M} = \frac{\sqrt{\te[l\x l^{\prime}]{Q_{\mi{m}}}}}{\te[l]{D_{\mi{M}}} \te[\mi{l^{\prime}}]{D_{\mi{M}}}}
    \end{equation}
    It is satisfying to note that these equations enforce
    \begin{equation*} \label{eq:GSAS:Tm:diag}
        \te[l\x l]{T_{\mi{M}}}=0 \QT{on the diagonal} \te[l\x l]{\rv{S}_{\mi{M}}}=1        
    \end{equation*}


\section{GSA via mQU moments}\label{sec:CK}
    The central problem in calculating errors on Sobol' matrices is that they involve ineluctable covariances over ungoverned noise between differently marginalized SPs. But marginalization and covariance determination are both a matter of taking expectations. Using Fubini's theorem \cite[pp.77]{Williams1991} the ineluctable can be avoided by reversing the order of expectations -- taking moments over ungoverned noise, then marginalizing.

    The covariance function or kernel \cite{Rasmussen.Williams2005} of an MQU measures the similarity between the output by two determined inputs $\tte[\mi{M}]{u},\tte[\mi{M^{\prime}}]{u^{\prime}}$ as
    \begin{equation} \label{eq:CK:k}
        \te[\mi{L\x L^{\prime}}]{k(\tte[\mi{M}]{u},\tte[\mi{M^{\prime}}]{u^{\prime}})} \deq 
    \cov[{y(\te[\mi{M^{\prime}}]{u^{\prime}})}]{y(\te[\mi{M}]{u})}{M}
    \end{equation}
    Throughout this work, primes on uppercase constants are for bookkeeping only, they do not affect the value $\mi{M}^{n\prime}\deq \mi{M};\ \mi{L}^{n\prime}\deq \mi{L}$. Primes on lowercase variables do affect the value, so it may or may not be the case that $\mi{m}^{n\prime}=\mi{m};\  \mi{l}^{n\prime}=\mi{l}$. All one needs to secure a covariance function is the ability to calculate the finite-dimensional distribution
    \begin{equation*}
        \prob{\left(y(\te[\mi{M+1\x 2}]{\rv{u}})- \begin{bmatrix} \tte[\mi{L}]{Y} \\ \tte[\mi{L^{\prime}}]{Y^{\prime}} \end{bmatrix}\right)^{2} < \sigma^{2}}{\te[\mi{M\x 2}]{\rv{u}}= \begin{bmatrix} \tte[\mi{M}]{u} \\ \tte[\mi{M^{\prime}}]{u^{\prime}} \end{bmatrix}}
    \end{equation*}
    for the design matrix
    \begin{equation*}
        \begin{array}{lllll}
            \te[0]{u} & \cdots & \te[M-1]{u} \\
            \te[0]{u^{\prime}} & \cdots & \te[M^{\prime}-1]{u^{\prime}}
        \end{array}
        \begin{array}{lllll}
            \te[0]{Y} & \cdots & \te[L]{Y} \\
            \te[0]{Y^{\prime}} & \cdots & \te[L^{\prime}]{Y^{\prime}}
        \end{array}
    \end{equation*}
    This is the primary function of an MQU, as discussed in \cref{sec:MQU}. 
    
    Using this finite-dimensional distribution one can always calculate the non-centralized mQU moments
    \begin{equation}\label{eq:CK:mum}
                \te[\mi{L}]{\mu_{\mi{m}}} = \evt{\evt{\te[\mi{L}]{Y}}{M}}{\mi{M-m}}                
                = \evt{\evt{\te[\mi{L}]{y(\te[\mi{M}]{\rv{u}})}}{M}}{\mi{M-m}}                
    \end{equation}
    \begin{equation}\label{eq:CK:mumm}
        \begin{aligned}
            \te[\mi{L\x L^{\prime}}]{\mu_{\mi{mm^{\prime}}}} &= \evt{\evt{{\cov[{\te[\mi{L^{\prime}}]{Y^{\prime}}}]{\te[\mi{L}]{Y}}{M}}}{\mi{M^{\prime}-m^{\prime}}}}{\mi{M-m}} \\
            &= \evt{\evt{{\cov{\te[\mi{L\x L^{\prime}}]{k(\te[\mi{M}]{\rv{u}},\te[\mi{M^{\prime}}]{\rv{u}^{\prime}})}}{M}}}{\mi{M^{\prime}-m^{\prime}}}}{\mi{M-m}} + \te[\mi{L}]{\mu_{\mi{m}}} \otimes \te[\mi{L^{\prime}}]{\mu_{\mi{m^{\prime}}}}                
        \end{aligned}
    \end{equation}
    This is how to calculate mQU moments, it is not how to define them.
    
    In order to calculate the Sobol' matrix error, let us define the mQU moments
    \begin{equation}
        \te[\mi{L}^{n}]{\mu_{\mi{m \ldots m}^{(n-1)\prime}}} \deq \ev{\tte[\mi{L}]{\rv{y}_{\mi{m}}}\otimes\cdots\otimes\tte[\mi{L}]{\rv{y}_{\mi{m}^{(n-1)\prime}}}}{M}
    \end{equation}
    Simply reversing the order in which expectations are taken -- using Fubini's theorem -- establishes equations \cref{eq:CK:mum,eq:CK:mumm}. The law of iterated expectations further entails
    \begin{equation}\label{eq:CK:reduction}
        \te[\mi{L}^{n}]{\mu_{\mi{0\ldots 0}\mi{m}^{j\prime}\mi{\ldots m}^{(n-1)\prime}}} 
        = \evt{\te[\mi{L}^{n}]{\mu_{\mi{m\ldots m}\mi{m}^{j\prime}\mi{\ldots m}^{(n-1)\prime}}}}{\mi{m}} 
        = \evt{\te[\mi{L}^{n}]{\mu_{\mi{M\ldots M}\mi{m}^{j\prime}\mi{\ldots m}^{(n-1)\prime}}}}{\mi{M}} \quad \forall j< n
    \end{equation}
    This reduction will be used repeatedly in the remainder of this section. Definitions \cref{def:MQU:y,def:GSAI:V_m,def:GSAS:VD} conveniently zero the fully marginalized moments
    \begin{equation} \label{eq:CK:V0}
        \covt{\te[\mi{L}^{2}]{\rv{y}_{\mi{0}}}}{\mi{0}} = \te[\mi{L}^{2}]{V_{\mi{0}}} = \te[\mi{L}^{2}]{V_{\mi{0}}} = \te[\mi{L}]{\mu_{\mi{0}}}^{2} = \te[\mi{L}^{2}]{\mu_{\mi{00}}} = 
        \te[\mi{L}^2]{0}
    \end{equation}
    Centralizing the mQUs
    \begin{equation}\label{def:CK:e}
        \te[\mi{L}]{\rv{e}_{\mi{m}}} \deq \te[\mi{L}]{\rv{y}_{\mi{m}}} - \te[\mi{L}]{\mu_{\mi{m}}} \quad \forall \mi{m}\subseteq\mi{M}
    \end{equation}
    the expected conditional variance in \cref{def:GSAS:VD} amounts to
    \begin{equation}\label{eq:CK:V}
        \begin{aligned}
            \te[\mi{L}^{2}]{V_{\mi{m}}} 
            &= \evt{\ev{\te[\mi{L}]{\rv{e}_{\mi{m}} + \mu_{\mi{m}}}^{2}}{M}}{\mi{m}}
            - \ev{\te[\mi{L}]{\rv{e}_{\mi{0}} + \mu_{\mi{0}}}^{2}}{M} \\
            &= \ev{\te[\mi{L}]{\mu_{\mi{m}}}^{2}}{\mi{m}} - \te[\mi{L}]{\mu_{\mi{0}}}^{2} + 
            \evt{\te[\mi{L}^2]{\mu_{\mi{mm}}}}{\mi{m}} - \te[\mi{L}^2]{\mu_{\mi{00}}} \\
            &= \ev{\te[\mi{L}]{\mu_{\mi{m}}}^{2}}{\mi{m}}
        \end{aligned}
    \end{equation}
    Substituting this formula in \cref{def:GSAS:V,def:GSAS:SST} determines the closed and total Sobol' matrices in terms of mQU moments.

    The covariance between conditional variances in \cref{def:GSAS:W} is
    \begin{equation*}
        \begin{aligned}
            \te[\mi{L}^{2}\x \mi{L^{\prime}}^{2}]{W_{\mi{mm^{\prime}}}} &\deq \cov[\covt{\te[\mi{L^{\prime}}^{2}]{\rv{y}_{\mi{m^{\prime}}}}}{\mi{m^{\prime}}}]{\covt{\te[\mi{L}^{2}]{\rv{y}_{\mi{m}}}}{\mi{m}}}{M} \\
            &\phantom{:}=
            \cov[\ev{\te[\mi{L^{\prime}}]{\rv{y}_{\mi{m^{\prime}}}}^{2} - \te[\mi{L^{\prime}}]{\rv{y}_{\mi{0}}}^{2}}{\mi{m^{\prime}}}]{\ev{\te[\mi{L}]{\rv{y}_{\mi{m}}}^{2} - \te[\mi{L}]{\rv{y}_{\mi{0}}}^{2}}{\mi{m}}}{M} \\
            &\phantom{:}=
            \ev{\ev{\te[\mi{L}]{\rv{y}_{\mi{m}}}^{2} - \te[\mi{L}]{\rv{y}_{\mi{0}}}^{2}}{\mi{m}} \otimes\ev{\te[\mi{L^{\prime}}]{\rv{y}_{\mi{m^{\prime}}}}^{2} - \te[\mi{L^{\prime}}]{\rv{y}_{\mi{0}}}^{2}}{\mi{m^{\prime}}}}{M} - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L^{\prime}}^2]{V_{\mi{m^{\prime}}}} \\       
            &\phantom{:}= \te[\mi{L}^2\x \mi{L^\prime}^2]{A_{\mi{mm^{\prime}}}-A_{\mi{0m^{\prime}}}-A_{\mi{m0}}+A_{\mi{00}}}
        \end{aligned}
    \end{equation*}
    where
    \begin{equation*}
        \begin{aligned}
            \te[\mi{L}^2\x \mi{L^\prime}^2]{A_{\mi{mm^{\prime}}}}
            &\deq \evt{\evt{\ev{\te[\mi{L}]{\rv{y}_{\mi{m}}}^{2} \otimes \te[\mi{L^{\prime}}]{\rv{y}_{\mi{m^{\prime}}}}^{2}}{\mi{m}}}{\mi{m^{\prime}}}}{M} - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L^{\prime}}^2]{V_{\mi{m^{\prime}}}} \\
            &\phantom{:}= \evt{\evt{\ev{
                \te[\mi{L}]{\rv{e}_{\mi{m}}+\mu_{\mi{m}}}^{2} \otimes \te[\mi{L^{\prime}}]{\rv{e}_{\mi{m^{\prime}}}+ \mu_{\mi{m^{\prime}}}}^{2} - 
                \te[\mi{L}]{\mu_{\mi{m}}}^{2} \otimes \te[\mi{L^{\prime}}]{\mu_{\mi{m^{\prime}}}}^{2}
            }{M}}{\mi{m^{\prime}}}}{\mi{m}}
        \end{aligned}
    \end{equation*}
    exploiting the fact that $V_{\mi{0}} = \te[\mi{L}^2]{0}$. \Cref{eq:CK:reduction} cancels all terms beginning with $\te[\mi{L}]{\rv{e}_{\mi{m}}}^{2}$, first across $A_{\mi{mm^{\prime}}}-A_{\mi{0m^{\prime}}}$ then across $A_{\mi{m0}}-A_{\mi{00}}$. All remaining terms ending in $\te[\mi{L^{\prime}}]{\mu_{\mi{m^{\prime}}}}^{2}$ are eliminated by centralization $\evt{\,\tte[]{\rv{e}_{\mi{m}}}}{M} = 0$.
    Similar arguments eliminate $\te[\mi{L^{\prime}}]{\rv{e}_{\mi{m^{\prime}}}}^{2}$ and $\te[\mi{L}]{\mu_{\mi{m}}}^{2}$.
    Effectively then
    \begin{equation*}
        \te[\mi{L}^4]{A_{\mi{mm^{\prime}}}} = \sum_{\pi(\mi{L}^{2})} \sum_{\pi(\mi{L^{\prime}}^{2})}
        \evt{\evt{\te[\mi{L}^{2} \x \mi{L^{\prime}}^{2}]{\mu_{\mi{m}} \otimes \mu_{\mi{mm^{\prime}}} \otimes \mu_{\mi{m^{\prime}}}}}{\mi{m^{\prime}}}}{\mi{m}}
    \end{equation*}
    so \cref{eq:CK:V0} entails
    \begin{equation}\label{eq:CK:W}
        \te[\mi{L}^4]{W_{\mi{mm^{\prime}}}} = \sum_{\pi(\mi{L}^{2})} \sum_{\pi(\mi{L^{\prime}}^{2})}
        \evt{\evt{\te[\mi{L}^{2} \x \mi{L^{\prime}}^{2}]{\mu_{\mi{m}} \otimes \mu_{\mi{mm^{\prime}}} \otimes \mu_{\mi{m^{\prime}}}}}{\mi{m^{\prime}}}}{\mi{m}}
    \end{equation}
    where each summation is over permutations of tensor axes
    \begin{equation*}
        \pi(\mi{L}^{2}) \deq \set{(\mi{L}\x\mi{L^{\prime\prime}}), (\mi{L^{\prime\prime}}\x\mi{L})} \QT{;} \pi(\mi{L^{\prime}}^{2}) \deq \set{(\mi{L^{\prime}}\x\mi{L^{\prime\prime\prime}}), (\mi{L^{\prime\prime\prime}}\x\mi{L^{\prime}})}
    \end{equation*}
    Substituting \cref{eq:CK:W} in \cref{def:GSAS:Q,def:GSAS:T,def:GSAS:TTT} determines the closed and total Sobol' matrixe errors in terms of mQU moments.
    
\section{Old}\label{sec:Old}
    The correlation between (the vector predictions of) two mQUs is the tensor RV
    \begin{equation} \label{def:GSAM:R_mmp}
        \te[\mi{L\x L}]{\rv{R}_{\mi{mm^{\prime}}}} \deq \frac{\cov[\rv{y}_{\mi{m}^{\prime}}]{\rv{y}_{\mi{m}}}{\mi{M}}_{\mi{L\x L}}}{\dev{\rv{y}_{\mi{m}}}{\mi{m}}_{\mi{L}} \otimes \dev{\rv{y}_{\mi{m^{\prime}}}}{\mi{m}^{\prime}}_{\mi{L}}} \qquad \forall \mi{m}, \mi{m^{\prime}} \subseteq \mi{M}
    \end{equation}

    The square of the correlation is a discerning judge of regression models \cite{Chicco2021} called the coefficient of determination, which is clearly identical to the closed Sobol' index
    \begin{equation} \label{def:GSAM:R2}
        \te[l\x l]{\rv{R}_{\mi{mM}}^{2}} \deq \te[l\x l]{\rv{R}_{\mi{mM}}}^{2} = \te[l\x l]{\rv{S}_{\mi{m}}}
    \end{equation}
    Perhaps the most significant use of closed Sobol' indices is to identify a representative reduced model of $m\leq M$ inputs within the full model $\mi{M}$.
    A closed index close to 1 confirms that the two models make nearly identical predictions. Simplicity and economy (not least of calculation) motivate the adoption of a reduced model, a closed Sobol' index close to 1 is what permits it. This is precisely equivalent to ignoring $\mi{M-m}$ inputs on the grounds that their influence on $y$ -- measured by their total index $\te[l]{S^{T}_\mi{M-m}}$ -- is close to 0.

    Our aim is to compare predictions from a reduced regression model $\rv{y}_{\mi{m}}$ with those from the full regression model $\rv{y}_{\mi{M}}$. Correlation between these predictions is squared -- using element-wise (Hadamard) multiplication $\circ$ and division $/$ -- to form an RV called the coefficient of determination
    \begin{equation}
        \te[\mi{L}^2]{\rv{R}_{\mi{mM}}^{2}} \deq 
        \frac{\cov[\rv{y}_{\mi{M}}]{\rv{y}_{\mi{m}}}{\mi{M}} \circ \cov[\rv{y}_{\mi{M}}]{\rv{y}_{\mi{m}}}{\mi{M}}}
        {\cov{\rv{y}_{\mi{m}}}{\mi{m}} \circ \cov{\rv{y}_{\mi{M}}}{\mi{M}}} =
        \frac{\cov{\rv{y}_{\mi{m}}}{\mi{m}}}{\cov{\rv{y}_{\mi{M}}}{\mi{M}}}
    \end{equation}
    However, this is undefined whenever $\covt{\tte[l\x l^{\prime}]{\rv{y}_{\mi{M}}}}{\mi{M}} = 0$, obscuring potentially useful information about $\covt{\tte[l\x l^{\prime}]{\rv{y}_{\mi{m}}}}{\mi{m}}$. Introducing 1-tensors representing the square root diagonal of a covariance matrix
    \begin{equation}
        \te[l]{\sqrt{\cov[\cdot]{\cdot}{}_{\mi{L}^{2}}}} \deq \cov[\cdot]{\cdot}{}_{l^2}^{1/2}
    \end{equation}
    the correlation coefficient between output dimensions is
    \begin{equation}
        \te[\mi{L\x L^{\prime}}]{\rv{R}_{\mi{m}}} \deq \frac{\cov{\rv{y}_{\mi{m}}}{\mi{m}}_{\mi{L\x L^{\prime}}}} {\sqrt{\covt{\,\tte[\mi{L}^2]{\rv{y}_{\mi{m}}}}{\mi{m}}}\otimes\sqrt{\covt{\,\tte[\mi{L^{\prime}}^2]{\rv{y}_{\mi{m}}}}{\mi{m}}}}
        \ \ \forall \mi{m}\subseteq \mi{M}
    \end{equation}
    Let us define the multi-output closed Sobol' index as the product of the full correlation between output dimensions and the coefficient of determination
    \begin{equation}
        \te[\mi{L\x L^{\prime}}]{\rv{S}_{\mi{m}}} \deq \te[\mi{L\x L^{\prime}}]{\rv{R}_{\mi{M}}} \circ \te[\mi{L\x L^{\prime}}]{\rv{R}_{\mi{mM}}^{2}} 
    \end{equation}
    and the multi-output total Sobol' index as its complement
    \begin{equation}
        \te[\mi{L\x L^{\prime}}]{\rv{S^{T}_{M-m}}} \deq \te[\mi{L\x L^{\prime}}]{\rv{S}_{\mi{M}}} - \te[\mi{L\x L^{\prime}}]{\rv{S}_{\mi{m}}}
    \end{equation}
    These definitions coincide precisely with the traditional Sobol' index along the diagonal $\sqrt{\tte[\mi{L}^{2}]{\rv{S}_{\mi{m}}}} \circ \sqrt{\tte[\mi{L}^{2}]{\rv{S}_{\mi{m}}}}$, which has been very much the focus of prior literature \cite{Gamboa.etal2013,Xiao2017,GarciaCabrejo2014}. The off-diagonal elements are bound by the diagonal as
    \begin{equation}
        -\te[l^2]{\rv{S}_{\mi{m}}}^{1/2}\te[l^{\prime 2}]{\rv{S}_{\mi{m}}}^{1/2} \leq
            \te[l\x l^{\prime}]{\rv{S}_{\mi{m}}} = 
            \te[l\x l^{\prime}]{\rv{R}_{\mi{m}}}\te[l^2]{\rv{S}_{\mi{m}}}^{1/2}\te[l^{\prime 2}]{\rv{S}_{\mi{m}}}^{1/2} \leq \te[l^2]{\rv{S}_{\mi{m}}}^{1/2}\te[l^{\prime 2}]{\rv{S}_{\mi{m}}}^{1/2}
    \end{equation}
    To calculate moments over ungoverned noise we use the Taylor series method \cite[pp.353]{Kendall1994}, which is valid provided $\covt{\,\te[l^{2}]{\rv{y}_{\mi{M}}}}{\mi{M}}$ is well approximated by its mean
    \begin{equation}
        \te[l^{2}]{V_{\mi{M}}} \deq \evt{\,\covt{\,\te[l^2]{\rv{y}_{\mi{M}}}}{\mi{M}}}{M} \gg \big\vert\covt{\,\te[l^{2}]{\rv{y}_{\mi{M}}}}{\mi{M}} - \te[l^{2}]{V_{\mi{M}}}\big\vert
    \end{equation}
    This provides the mean Sobol' index
    \begin{align}\label{def:GSI:mean}
        \te[\mi{L\x L^{\prime}}]{S_{\mi{m}}} &\deq \evt{\te[\mi{L\x L^{\prime}}]{\rv{S}_{\mi{m}}}}{M}
        = \frac{\tte[\mi{L\x L^{\prime}}]{V_{\mi{m}}}}{\sqrt{\tte[\mi{L}^{2}]{V_{\mi{M}}}} \otimes \sqrt{\tte[\mi{L^{\prime}}^2]{V_{\mi{M}}}}} \\            
        \T{where }\te[\mi{L\x L^{\prime}}]{V_{\mi{m}}} &\deq \evt{ \cov{\rv{y}_{\mi{m}}}{\mi{m}}}{M}_{\mi{L\x L^{\prime}}} \quad \forall \mi{m}\subseteq \mi{M}
    \end{align}
    with standard deviation due to ungoverned noise of
    \begin{equation} \label{def:GSI:variance}
        \te[\mi{L\x L^{\prime}}]{T_{\mi{m}}}^{2} \deq 
        \covt{\,\te[(\mi{L\x L^{\prime}})^2]{\rv{S}_{\mi{m}}}}{M} = \frac{\te[(\mi{L\x L^{\prime}})^2]{Q_{\mi{m}}}}{\te[\mi{L}^{2}]{V_{\mi{M}}}^{2/2} \otimes \te[\mi{L^{\prime}}^{2}]{V_{\mi{M}}}^{2/2}}
    \end{equation}
    where improper fractions exponentiate a square root diagonal of $V_{\mi{M}}$, and
    \begin{multline}\label{def:GSI:Q}
        \te[(\mi{L\x L^{\prime}})^{2}]{Q_{\mi{m}}} \deq \te[(\mi{L\x L^{\prime}})^{2}]{W_{\mi{mm}}}
        - \te[\mi{L\x L^{\prime}}]{V_{\mi{m}}} \circ \sum_{\mi{L}^{\circ} \in \set{\mi{L, L^{\prime}}}} \frac{\te[\mi{L}^{\circ 2}\x\mi{L\x L^{\prime}}]{W_{\mi{Mm}}}} {\te[\mi{L}^{\circ 2}]{V_{\mi{M}}}^{2/2}} \\                
         + \frac{\te[\mi{L\x L^{\prime}}]{V_{\mi{m}}}^{2}}{4} \circ \sum_{\mi{L}^{\circ} \in \set{\mi{L, L^{\prime}}}} \frac{\te[\mi{L}^{\circ 2}\x\mi{L}^{2}]{W_{\mi{MM}}}} {\te[\mi{L}^{\circ 2}]{V_{\mi{M}}}^{2/2} \otimes \te[\mi{L}^{2}]{V_{\mi{M}}}^{2/2}}
        + \frac{\te[\mi{L}^{\circ 2}\x\mi{L}^{\prime 2}]{W_{\mi{MM}}}} {\te[\mi{L}^{\circ 2}]{V_{\mi{M}}}^{2/2} \otimes \te[\mi{L}^{\prime 2}]{V_{\mi{M}}}^{2/2}}
    \end{multline}
    \begin{equation}\label{def:GSI:W}
        \te[\mi{L}^4]{W_{\mi{mm^{\prime}}}} \deq \cov[\cov{\rv{y}_{\mi{m^{\prime}}}}{\mi{m^{\prime}}}]{\cov{\rv{y}_{\mi{m}}}{\mi{m}}}{M}_{\mi{L}^4}
    \end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{siamplain}
\bibliography{../main}

\end{document}
