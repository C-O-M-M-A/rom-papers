% SIAM Article Template
\documentclass[review,onefignum,onetabnum]{siamonline220329}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.


% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={An Example Article},
  pdfauthor={D. Doe, P. T. Frank, and J. E. Smith}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.


\usepackage{amsfonts} 
\usepackage{xspace} 
\usepackage{ifthen} 
\usepackage{csvsimple}
\usepackage{todonotes}
\usepackage{float}
\newcommand*{\M}[1]{\ensuremath{#1}\xspace} 
\newcommand*{\tr}[1]{\M{#1}}
\newcommand*{\x}{\times}
\newcommand*{\mi}[1]{\mathbf{#1}} 
\newcommand*{\st}[1]{\mathbb{#1}} 
\newcommand*{\rv}[1]{\mathsf{#1}} 
\newcommand*{\te}[2][]{\left\lbrack{#2}\right\rbrack_{#1}}
\newcommand*{\tte}[2][]{\lbrack{#2}\rbrack_{#1}}
\newcommand*{\tse}[2][]{\mi{\lbrack#2\rbrack}_{#1}}
\newcommand*{\tme}[3][]{\lbrack{#3}\rbrack_{\tse[#1]{#2}}}
\newcommand*{\diag}[2][]{\left\langle{#2}\right\rangle_{#1}}
\newcommand*{\prob}[3]{\M{\mathrm{p}\!\left(\left.{#1}\right\vert{#2,#3}\right)}} 
\newcommand*{\deq}{\M{\mathrel{\mathop:}=}} 
\newcommand*{\deqr}{\M{=\mathrel{\mathop:}}} 
\newcommand{\T}[1]{\text{#1}} 
\newcommand*{\QT}[2][]{\M{\quad\T{#2}\ifthenelse{\equal{#1}{}}{\quad}{#1}}} 
\newcommand*{\ev}[3][]{\mathbb{E}_{#3}^{#1}\!\left\lbrack{#2}\right\rbrack}
\newcommand*{\evt}[3][]{\mathbb{E}_{#3}^{#1}\!#2}
\newcommand*{\cov}[3][]{\ifthenelse{\equal{#1}{}}{\mathbb{V}_{#3}\!\left\lbrack{#2}\right\rbrack}{\mathbb{V}_{#3}\!\left\lbrack{#2,#1}\right\rbrack}}
\newcommand*{\covt}[2]{\mathbb{V}_{#2}\!{#1}}
\newcommand*{\gauss}[2]{\mathsf{N}\!\left({#1,#2}\right)}
\newcommand*{\uni}[2]{\mathsf{U}\!\left({#1,#2}\right)}
\newcommand*{\tgauss}[2]{\mathsf{N}({#1,#2})}
\newcommand*{\gaussd}[2]{\mathsf{N}^{\dagger}\!\left({#1,#2}\right)}
\newcommand*{\modulus}[1]{\M{\left\lvert{#1}\right\rvert}} 
\newcommand*{\norm}[1]{\M{\left\lVert{#1}\right\rVert}} 
\newcommand*{\ceil}[1]{\M{\left\lceil{#1}\right\rceil}} 
\newcommand*{\set}[1]{\M{\left\lbrace{#1}\right\rbrace}} 
\newcommand*{\setbuilder}[2]{\M{\left\lbrace{#1}\: \big\vert \:{#2}\right\rbrace}}
\newcommand*{\uniti}{\lbrack 0,1\rbrack}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\trace}{tr\!}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

% Sets running headers as well as PDF title and authors
\headers{Generalized Sobol' Indices for Multi-Output Regression Models}{R. A. Milton, S. F. Brown, and A. S. Yeardley}

% Title. If the supplement option is on, then "Supplementary Material"
% is automatically inserted before the title.
\title{Generalized Sobol' Indices for Multi-Output Regression Models\thanks{Submitted to the editors DATE.
\funding{This work was funded by the Fog Research Institute under contract no.~FRI-454.}}}

% Authors: full names plus addresses.
\author{Robert A. Milton\thanks{Department of Chemical and Biological Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom
  (\email{r.a.milton@sheffield.ac.uk}, \url{https://www.browngroupsheffield.com/}).}
\and Solomon F. Brown\thanks{Department of Chemical and Biological Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom
(\email{s.f.brown@sheffield.ac.uk}, \url{https://www.browngroupsheffield.com/}).}
\and Aaron S. Yeardley\thanks{Department of Chemical and Biological Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom
(\email{asyeardley1@sheffield.ac.uk}, \url{https://www.browngroupsheffield.com/}).}}

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
  Variance based global sensitivity usually measures the relevance of inputs to a single output using Sobol' indices. This paper extends the definition in a natural way to multiple outputs, directly measuring the relevance of inputs to the linkages between outputs in a correlation-like matrix of indices. The usual Sobol' indices constitute the diagonal of this matrix. Existence, uniqueness and uncertainty quantification are established by developing the indices from a putative regression model. Analytic expressions for generalized Sobol' indices and their standard deviations are given, and tested computationally against test functions whose ANOVA can be performed.
\end{abstract}

% REQUIRED
\begin{keywords}
  Global Sensitivity Analysis, Sobol' Index, Surrogate Model, Multi-Output, Gaussian Process, Uncertainty Quantification
\end{keywords}

% REQUIRED
\begin{MSCcodes}
  60G07,60G15,62J10, 62H99
\end{MSCcodes}

\section{Introduction}\label{sec:Intro}
    This paper is concerned with analysing the results of experiments or computer simulations in a design matrix of $M\geq 1$ input and $L\geq 1$ output columns, over $N$ rows (datapoints). Global Sensitivity Analysis (GSA) \cite{Razavi2021} examines the relevance of the various inputs to the various outputs. When pursued via ANOVA decomposition of a single output $L=1$, this leads naturally to the well known Sobol' indices, which have by now been applied across most fields of science and engineering \cite{Saltelli2019,Ghanem2017}. This paper extends the definition in a natural way to multiple outputs $L\geq 1$.

    The Sobol' decomposition apportions the variance of a single output to sets of one or more inputs \cite{Sobol2001}. We shall use ordinals of inputs, tuples which are conveniently also naive sets. 
    \begin{equation} \label{def:intro:m}
        \mi{m}\deq (0,\ldots ,m-1) \subseteq (0,\ldots ,M-1) \deqr \mi{M}    
    \end{equation}
    Obviously this restricts the subsets of inputs being studied, but this is a loss of convenience not generality, as any desired subset may be studied by ordering the inputs appropriately in advance. The maximal ordinal $\mi{M}$ of all $M$ inputs explains everything explicable, so its Sobol' index is 1 by definition. The void ordinal $\mi{0}$ explains nothing, so its Sobol' index is 0 by definition. 
    The influence of an isolated ordinal of inputs $\mi{m}$ is measured by its closed Sobol' index $S_{\mi{m}} \in \lbrack 0,1\rbrack$. A first-order Sobol' index $S_{m^{\prime}}$ is simply the closed Sobol' index of a single input $m^{\prime}$.
    Because inputs in an isolated ordinal may act in concert with each other, the influence of the ordinal often exceeds the sum of first-order contributions from its members, always obeying 
    \begin{equation*}
        S_{\mi{m}} \geq \sum_{m^{\prime} \in \mi{m}} S_{m^{\prime}}
    \end{equation*}

    Using the set-theoretic difference
    \begin{equation} \label{def:intro:complement}
        \mi{M-m} \deq \setbuilder{m^{\prime} \in \mi{M}}{m^{\prime} \notin \mi{m}}
    \end{equation}
    the complement of a closed index $S_{\mi{m}}$ is the total Sobol index of its complement
    \begin{equation} \label{def:intro:ST}
        S^{T}_{\mi{M-m}} \deq 1-S_{\mi{m}} \in \lbrack 0,1\rbrack
    \end{equation}
    This expresses the influence of non-isolated inputs $\mi{M-m}$ allowed to act in concert with each other \emph{and} isolated inputs $\mi{m}$. When speaking of irrelevant inputs $\mi{M-m}$, we mean that $S^{T}_{\mi{M-m}} \approx 0$. This is synonymous with the isolated ordinal of inputs $\mi{m}$ explaining everything explicable $S_{\mi{m}}\approx 1$.

    Perhaps the most significant use of closed Sobol' indices is to identify a representative reduced model of $m\leq M$ inputs within the full model $\mi{M}$.
    Apportioning variance is mathematically equivalent to squaring a correlation coefficient to produce a coefficient of determination $R^{2}$ \cite{Chicco2021}. A closed Sobol' index is thus a coefficient of determination between the predictions from the reduced model $\mi{m}$ and predictions from the full model $\mi{M}$. A closed Sobol' index close to 1 confirms that the two models make nearly identical predictions. Simplicity and economy (not least of calculation) motivate the adoption of a reduced model, a closed Sobol' index close to 1 is what permits it.

    There is no generally accepted extension of Sobol' indices to multiple outputs $L>1$. 
    The discussion thus far, and almost all prior GSA, has dealt with a single (i.e scalar) output. With multiple (i.e vector) outputs, the Sobol' decomposition apportions the covariance matrix of outputs rather than the variance of a single output. With $L$ outputs, the closed Sobol' index $S_{\mi{m}}$ is generally a symmetric $\mi{L}\x \mi{L}$ matrix. The diagonal elements express the relevance of inputs to the output variables themselves. The off-diagonal elements express relevance to the linkages between outputs. This may be of considerable interest when outputs are, for example, yield and purity of a product, or perhaps a single output measured at various times. The Sobol indices reveal (amongst other things) which inputs it is worthwhile varying in an effort to alter the linkages between outputs. Prior work on Sobol' indices with multiple outputs \cite{Gamboa.etal2013,Xiao2017,GarciaCabrejo2014} has settled ultimately on just the diagonal elements of the covariance matrix, so this linkage remains unexamined. Although output covariance has been incoroporated indirectly in prior studies by performing principal component analysis (PCA) on ouputs prior to GSA on the (diagonal) variances of the resulting output basis \cite{Campbell2006}. This has been used in particular to study synthetic ``multi-outputs'' which are actually the dynamic response of a single output over time \cite{Lamboni2011, Zhang2020}.

    Accurate calculation of Sobol' indices even for a single output is computationally expensive and requires 10,000+ datapoints \cite{Lamoureux.etal2014}. A (sometimes) more efficient approach is calculation via a surrogate model, such as Polynomial Chaos Expansion (PCE) \cite{Ghanem.Spanos1997,Xiu.Karniadakis2002,Xiu2010}, low-rank tensor approximation \cite{Chevreuil.etal2015,Konakli.Sudret2016}, and support vector regression \cite{Cortes.Vapnik1995}. As well as being efficient, surrogate models also smooth out noise in the output, which is often highly desirable in practice. This paper employs one of the most popular surrogates, the Gaussian Processes (GP) \cite{Sacks.etal1989, Rasmussen.Williams2005} as it is highly tractable. We shall follow the multi-output form (MOGP) described in \cite{Alvarez.etal2011}, in order to examine the linkages between outputs.
    This paper deals exclusively with the anisotropic Radial Basis Function kernel, known as RBF/ARD, which is widely accepted as the kernel of choice for smooth outputs \cite{Rasmussen2016}. This uses the classic Gaussian bell curve to express the proximity of two input points, described in detail in \cref{sub:GPR:Tensor,sub:GPR:Prior}.

    Semi-analytic expressions for Sobol' indices are available for scalar PCEs \cite{Sudret2008}, and the diagonal elements of multi-output PCEs \cite{GarciaCabrejo2014}.
    Semi-analytic expressions for Sobol' indices of GPs have been provided in integral form by \cite{Oakley.OHagan2004} and alternatively by \cite{Chen.etal2005}. These approaches are implemented, examined and compared in \cite{Marrel.etal2009,Srivastava.etal2017}. Both \cite{Oakley.OHagan2004,Marrel.etal2009} estimate the errors on Sobol' indices in semi-analytic, integral form. Fully analytic, closed form expressions have been derived without error estimates for uniformly distributed inputs \cite{Wu.etal2016a} with an RBF kernel. There are currently no closed form expressions for MOGPs, or the errors on Sobol' indices, or any GPs for which inputs are not uniformly distributed. 

    In this paper we provide explicit, closed-form analytic formulae for the $\mi{L}\x \mi{L}$ matrices of closed Sobol' indices and their errors, for a class of MOGP with an RBF/ARD kernel applicable to smoothly varying outputs. 
    We transform uniformly distributed inputs $\rv{u}$ to normally distributed inputs $\rv{z}$ prior to fitting a GP and performing analytic calculation of closed Sobol' indices. This leads to relatively concise expressions in terms of exponentials, and enables ready calculation of the errors (variances) of these expressions. It also allows for an arbitrary rotation $\Theta$ of inputs, as normal variables are additive, whereas summing uniform inputs does not produce uniform inputs. If the goal is reducing inputs, rotating their basis first boosts the possibilities immensely \cite{Constantine2015}. It presents the possibilty of choosing $\Theta$ to maximise the closed Sobol' index of the first few inputs.

    The quantities to be calculated and their formal context are introduced in \cref{sec:GSI}, assuming only that the output is an integrable function of the input. Our approach effectively regards a regression model which quantifies uncertainty with each prediction as just another name for a stochastic process. A great deal of progress is made in \cref{sec:SPEst} using general stochastic (not necessarily Gaussian) processes. This approach is analytically cleaner, as it is not obfuscated by the GP details. Furthermore, it turns out that the desirable properties of the Gaussian (lack of skew, simple kurtosis) are not actually helpful, as these terms cancel of their own accord. This development leaves just two terms to be calculated, which require the stochastic process to be specified. MOGPs with an RBF/ARD kernel are tersely developed and described in \cref{sec:GPR}, then used to calculate the two unknown terms in \cref{sec:GPMom,sec:GPEst}. Methods to reduce computational complexity are discussed in \cref{sec:Complexity}.Conclusions are drawn in \cref{sec:Conc}.


\section{Generalized Sobol' indices}\label{sec:GSI}
    Apply a constant offset to a Lebesgue integrable model so that
    \begin{equation} \label{def:GSI:y}
        y \colon \uniti^{M+1} \rightarrow \st{R}^{L} \QT{obeys} \int y(u) \; \mathrm{d}u = \te[\mi{L}]{0}
    \end{equation}
    taking as input a uniformly distributed random variable (RV)
    \begin{equation} \label{def:GSI:u}
        \rv{u} \sim \uni{\te[\mi{M+1}]{0}}{\te[\mi{M+1}]{1}} \deq \uni{0}{1}^{M+1}
    \end{equation}
    Throughout this paper exponentiation is categorical -- repeated cartesian $\x$ or tensor $\otimes$ -- unless otherwise specified. Square bracketed quantities are tensors, carrying their axes as a subscript tuple. In this case the subscript tuple is the von Neumann ordinal
    \begin{equation*}
        \mi{M+1} \deq (0,\ldots,M) \supset \mi{m} \deq (0,\ldots,m-1 \leq M-1)
    \end{equation*}
    with void $\mi{0}\deq ()$ voiding any tensor it subscripts. Ordinals are concatenated into tuples by Cartesian $\times$ and will be subtracted like sets, as in $\mi{M-m} \deq (m,\ldots,M-1)$. 
    Subscripts label the tensor prior to any superscript operation, so $\te[\mi{L}]{y(\rv{u})}^{2}$ is an $\mi{L}^{2} \deq \mi{L\x L}$ tensor, for example.
    The preference throughout this work is for uppercase constants and lowercase variables, in case of ordinals the lowercase ranging over the uppercase. We prefer $o$ for an unbounded positive integer, avoiding O.

    Expectations and variances will be subscripted by the dimensions of $\rv{u}$ marginalized. Conditioning on the remaining inputs is left implicit after \cref{def:Theory:y_m}, to lighten notation.
    Now, construct $M+1$ stochastic processes (SPs)
    \begin{equation}\label{def:Theory:y_m}
        \te[\mi{L}]{\rv{y}_{\mi{m}}} \deq \ev{y(\rv{u})}{\mi{M-m}} \deq \ev{y(\rv{u}) \big\vert \te[\mi{m}]{u}}{\mi{M-m}}
    \end{equation}
    ranging from $\tte[\mi{L}]{\rv{y}_{\mi{0}}}$ to $\tte[\mi{L}]{\rv{y}_{\mi{M}}}$. Every SP depends stochastically on the ungoverned noise dimension $\tte[M]{\rv{u}} \perp \tte[\mi{M}]{\rv{u}}$ and deterministically on the first $m$ governed inputs $\te[\mi{m}]{u}$, marginalizing the remaining inputs $\tte[\mi{M-m}]{\rv{u}}$. 
    Sans serif symbols such as $\rv{u,y}$ generally refer to RVs and SPs, italic $u,y$ being reserved for (tensor) functions and variables. Each SP is simply a regression model for $y$ on the first $m$ dimensions of $u$.
    
    Following the Kolmogorov extension theorem \cite[pp.124]{Rogers.Williams2000} we may regard an SP as a random function, from which we shall freely extract finite dimensional distributions generated by a design matrix $\tte[\mi{M\x o}]{u}$ of $o \in \st{Z}^{+}$ input samples.
    The Kolmogorov extension theorem incidentally secures $\rv{u}$. 
    Because $y$ is (Lebesgue) integrable it must be measurable, guaranteeing $\tte[\mi{L}]{\rv{y}_{\mi{0}}}$.
    Because all probability measures are finite, integrability of $y$ implies integrability of $y^n$ for all $n \in \st{Z}^{+}$ \cite{Villani1985}. 
    So Fubini's theorem \cite[pp.77]{Williams1991} allows all expectations to be taken in any order. These observations suffice to secure every object appearing in this paper. Changing the order of expectations, as permitted by Fubini's theorem, is the vital tool used throughout to construct this work. 

    Our aim is to compare predictions from a reduced regression model $\rv{y}_{\mi{m}}$ with those from the full regression model $\rv{y}_{\mi{M}}$. Correlation between these predictions is squared -- using element-wise (Hadamard) multiplication $\circ$ and division $/$ -- to form an RV called the coefficient of determination
    \begin{equation}
        \te[\mi{L}^2]{\rv{R}_{\mi{mM}}^{2}} \deq 
        \frac{\cov[\rv{y}_{\mi{M}}]{\rv{y}_{\mi{m}}}{\mi{M}} \circ \cov[\rv{y}_{\mi{M}}]{\rv{y}_{\mi{m}}}{\mi{M}}}
        {\cov{\rv{y}_{\mi{m}}}{\mi{m}} \circ \cov{\rv{y}_{\mi{M}}}{\mi{M}}} =
        \frac{\cov{\rv{y}_{\mi{m}}}{\mi{m}}}{\cov{\rv{y}_{\mi{M}}}{\mi{M}}}
    \end{equation}
    However, this is undefined whenever $\covt{\;\tte[l\x l^{\prime}]{\rv{y}_{\mi{M}}}}{\mi{M}} = 0$, obscuring potentially useful information about $\covt{\;\tte[l\x l^{\prime}]{\rv{y}_{\mi{m}}}}{\mi{m}}$. Introducing 1-tensors representing the square root diagonal of a covariance matrix
    \begin{equation}
        \te[l]{\sqrt{\cov[\cdot]{\cdot}{}_{\mi{L}^{2}}}} \deq \cov[\cdot]{\cdot}{}_{l^2}^{1/2}
    \end{equation}
    the correlation coefficient between output dimensions is
    \begin{equation}
        \te[\mi{L\x L^{\prime}}]{\rv{R}_{\mi{m}}} \deq \frac{\cov{\rv{y}_{\mi{m}}}{\mi{m}}_{\mi{L\x L^{\prime}}}} {\sqrt{\covt{\,\tte[\mi{L}^2]{\rv{y}_{\mi{m}}}}{\mi{m}}}\otimes\sqrt{\covt{\,\tte[\mi{L^{\prime}}^2]{\rv{y}_{\mi{m}}}}{\mi{m}}}}
        \ \ \forall \mi{m}\subseteq \mi{M}
    \end{equation}
    Let us define the multi-output closed Sobol' index as the product of the full correlation between output dimensions and the coefficient of determination
    \begin{equation}
            \te[\mi{L\x L^{\prime}}]{\rv{S}_{\mi{m}}} \deq \te[\mi{L\x L^{\prime}}]{\rv{R}_{\mi{M}}} \circ \te[\mi{L\x L^{\prime}}]{\rv{R}_{\mi{mM}}^{2}} 
    \end{equation}
    and the multi-output total Sobol' index as its complement
    \begin{equation}
        \te[\mi{L\x L^{\prime}}]{\rv{S^{T}_{M-m}}} \deq \te[\mi{L\x L^{\prime}}]{\rv{S}_{\mi{M}}} - \te[\mi{L\x L^{\prime}}]{\rv{S}_{\mi{m}}}
    \end{equation}
    These definitions coincide precisely with the traditional Sobol' index along the diagonal $\sqrt{\tte[\mi{L}^{2}]{\rv{S}_{\mi{m}}}} \circ \sqrt{\tte[\mi{L}^{2}]{\rv{S}_{\mi{m}}}}$, which has been very much the focus of prior literature \cite{Gamboa.etal2013,Xiao2017,GarciaCabrejo2014}. The off-diagonal elements are bound by the diagonal as
    \begin{equation}
        -\te[l^2]{\rv{S}_{\mi{m}}}^{1/2}\te[l^{\prime 2}]{\rv{S}_{\mi{m}}}^{1/2} \leq
            \te[l\x l^{\prime}]{\rv{S}_{\mi{m}}} = 
            \te[l\x l^{\prime}]{\rv{R}_{\mi{m}}}\te[l^2]{\rv{S}_{\mi{m}}}^{1/2}\te[l^{\prime 2}]{\rv{S}_{\mi{m}}}^{1/2} \leq \te[l^2]{\rv{S}_{\mi{m}}}^{1/2}\te[l^{\prime 2}]{\rv{S}_{\mi{m}}}^{1/2}
    \end{equation}
    To calculate moments over ungoverned noise we use the Taylor series method \cite[pp.353]{Kendall1994}, which is valid provided $\covt{\,\te[l^{2}]{\rv{y}_{\mi{M}}}}{\mi{M}}$ is well approximated by its mean
    \begin{equation}
        \te[l^{2}]{V_{\mi{M}}} \deq \evt{\,\covt{\,\te[l^2]{\rv{y}_{\mi{M}}}}{\mi{M}}}{M} \gg \big\vert\covt{\,\te[l^{2}]{\rv{y}_{\mi{M}}}}{\mi{M}} - \te[l^{2}]{V_{\mi{M}}}\big\vert
    \end{equation}
    This provides the mean Sobol' index
    \begin{align}\label{def:GSI:mean}
        \te[\mi{L\x L^{\prime}}]{S_{\mi{m}}} &\deq \evt{\te[\mi{L\x L^{\prime}}]{\rv{S}_{\mi{m}}}}{M}
        = \frac{\tte[\mi{L\x L^{\prime}}]{V_{\mi{m}}}}{\sqrt{\tte[\mi{L}^{2}]{V_{\mi{M}}}} \otimes \sqrt{\tte[\mi{L^{\prime}}^2]{V_{\mi{M}}}}} \\            
        \T{where }\te[\mi{L\x L^{\prime}}]{V_{\mi{m}}} &\deq \evt{\; \cov{\rv{y}_{\mi{m}}}{\mi{m}}}{M}_{\mi{L\x L^{\prime}}} \quad \forall \mi{m}\subseteq \mi{M}
    \end{align}
    with standard deviation due to ungoverned noise of
    \begin{equation} \label{def:GSI:variance}
        \te[\mi{L\x L^{\prime}}]{T_{\mi{m}}}^{2} \deq 
        \covt{\,\te[(\mi{L\x L^{\prime}})^2]{\rv{S}_{\mi{m}}}}{M} = \frac{\te[(\mi{L\x L^{\prime}})^2]{Q_{\mi{m}}}}{\te[\mi{L}^{2}]{V_{\mi{M}}}^{2/2} \otimes \te[\mi{L^{\prime}}^{2}]{V_{\mi{M}}}^{2/2}}
    \end{equation}
    where improper fractions exponentiate a square root diagonal of $V_{\mi{M}}$, and
    \begin{multline}\label{def:GSI:Q}
        \te[(\mi{L\x L^{\prime}})^{2}]{Q_{\mi{m}}} \deq \te[(\mi{L\x L^{\prime}})^{2}]{W_{\mi{mm}}}
        - \te[\mi{L\x L^{\prime}}]{V_{\mi{m}}} \circ \sum_{\mi{L}^{\circ} \in \set{\mi{L, L^{\prime}}}} \frac{\te[\mi{L}^{\circ 2}\x\mi{L\x L^{\prime}}]{W_{\mi{Mm}}}} {\te[\mi{L}^{\circ 2}]{V_{\mi{M}}}^{2/2}} \\                
         + \frac{\te[\mi{L\x L^{\prime}}]{V_{\mi{m}}}^{2}}{4} \circ \sum_{\mi{L}^{\circ} \in \set{\mi{L, L^{\prime}}}} \frac{\te[\mi{L}^{\circ 2}\x\mi{L}^{2}]{W_{\mi{MM}}}} {\te[\mi{L}^{\circ 2}]{V_{\mi{M}}}^{2/2} \otimes \te[\mi{L}^{2}]{V_{\mi{M}}}^{2/2}}
        + \frac{\te[\mi{L}^{\circ 2}\x\mi{L}^{\prime 2}]{W_{\mi{MM}}}} {\te[\mi{L}^{\circ 2}]{V_{\mi{M}}}^{2/2} \otimes \te[\mi{L}^{\prime 2}]{V_{\mi{M}}}^{2/2}}
    \end{multline}
    \begin{equation}\label{def:GSI:W}
        \te[\mi{L}^4]{W_{\mi{mm^{\prime}}}} \deq \cov[\cov{\rv{y}_{\mi{m^{\prime}}}}{\mi{m^{\prime}}}]{\cov{\rv{y}_{\mi{m}}}{\mi{m}}}{M}_{\mi{L}^4}
    \end{equation}
    It is satisfying to note that these Equations enforce
    \begin{equation} \label{eq:GSI:variance:diagonal}
        \te[l^{2}]{T_{\mi{M}}}=0 \QT{on the diagonal} \te[l^{2}]{\rv{S}_{\mi{M}}}=1        
    \end{equation}
    In practice it may be best to retain only the term in $W_{\mi{mm}}$, ignoring the uncertainty in $V_{\mi{M}}$ conveyed by $W_{\mi{Mm}},W_{\mi{MM}}$, because these may drastically reduce uncertainty whenever $V_{\mi{m}} \approx V_{\mi{M}}$, which is the circumstance of greatest interest.
    In a similar vein, the total index is defined as the difference between the closed index and $\tte[\mi{L\x L^{\prime}}]{\rv{S}_{\mi{M}}}$, which is exactly 1 on the diagonal and usually highly correlated with $\tte[\mi{L\x L^{\prime}}]{\rv{S}_{\mi{m}}}$ off the diagonal. Tiny differences between terms in the correlated case are swamped by numerical errors in practice. Extending square root diagonal notation naturally to 2 dimensions, the conservative standard deviation
    \begin{equation} \label{def:GSI:total:variance}
        \te[\mi{L\x L^{\prime}}]{T_{\mi{M-m}}^{T}} \deq 
        \te[\mi{L\x L^{\prime}}]{T_{\mi{M}}} + \te[\mi{L\x L^{\prime}}]{T_{\mi{m}}}
        \geq \sqrt{\covt{\,\te[(\mi{L\x L^{\prime}})^2]{\rv{S}_{\mi{M-m}}^{T}}}{M}}
    \end{equation}
    achieves equality on the diagonal, and is robust and sufficiently precise for most practical purposes.

    The remainder of this paper is devoted to calculating these two quantities -- the generalized closed Sobol' Index $S_{\mi{m}}$ and its standard deviation due to ungoverned noise $T_{\mi{m}}$.


\section{Stochastic Process estimates}\label{sec:SPEst}
    The central problem in calculating errors on Sobol' indices is that they involve ineluctable covariances between differently marginalized SPs, via their moments over ungoverned noise. But marginalization and moment determination are both a matter of taking expectations. So the ineluctable can be avoided by reversing the order of expectations -- taking moments over ungoverned noise, then marginalizing.
    To this end, adopt as design matrix a triad of inputs to condition $\te[\mi{(M+1)\x 3}]{\rv{u}}$, eliciting the response
    \begin{equation}\label{def:SPEst:y}
        \te[\mi{L\x 3}]{\rv{y}} \deq 
        \evt{\;\evt{\;\ev{y(\tte[\mi{(M+1)\x 3}]{\rv{u}}) 
            \big\vert \te[]{\te[\mi{0}]{u}, \te[\mi{m^{\prime}}]{u}, \te[\mi{M^{\prime\prime}}]{u}}}{\mi{0^{\prime\prime}}}}
        {\mi{M^{\prime}-m^{\prime}}}}{\mi{M}}
    \end{equation}
    Primes mark independent inputs, otherwise expectations are shared by all three members of the triad. It is not always obvious whether inputs are independent or shared by the triad, but this can be mechanically checked against the measure of integration behind an expectation. Repeated expectations over the same axis are rare here, usually indicating that apparent repetitions must be ``primed''. The purpose of the triad is to interrogate its response for moments in respect of ungoverned noise (which is shared by the triad members)
    \begin{equation}\label{def:SPEst:mu}
            \te[(\mi{L\x 3})^{n}]{\mu_{n}} \deq \ev{\tte[\mi{L\x 3}]{\rv{y}}^{n}}{M} \ \ \forall n \in \st{Z}^{+}
    \end{equation}
    for these embody
    \begin{equation*}
        \te[\mi{L}^{n}]{\mu_{\mi{m^{\prime}\ldots m}^{n\prime}}} \deq \te[\prod_{j=1}^{n}(\mi{L\x}i_{j})]{\mu_{n}} = \ev{\tte[\mi{L}]{\rv{y}_{\mi{m}^{\prime}}}\otimes\cdots\otimes\tte[\mi{L}]{\rv{y}_{\mi{m}^{n\prime}}}}{M}
    \end{equation*}
    where $i_{j}\in \mi{3}$ corresponds to $\mi{m}^{j\prime} \in \set{\mi{0},\mi{m},\mi{M}}$. This expression underpins the quantities we seek. The reduction which follows repeatedly realizes the iterated expectation law
    \begin{equation}\label{eq:SPEst:reduction}
        \te[\mi{L}^{n}]{\mu_{\mi{0\ldots 0}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}} \deq 
        \evt{\te[\mi{L}^{n}]{\mu_{\mi{M\ldots M}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}}}{\mi{M}} = 
        \evt{\te[\mi{L}^{n}]{\mu_{\mi{m\ldots m}\mi{m}^{j\prime}\mi{\ldots m}^{n\prime}}}}{\mi{m}}
    \end{equation}
    and that $y$ was offset in \cref{def:GSI:y} to obey
    \begin{equation} \label{eq:SPEst:mu0}
        \te[\mi{L}]{\mu_{\mi{0}}} = \te[\mi{L}]{0}
    \end{equation}
    and in any case
    \begin{equation} \label{eq:SPEst:mu00}
        \te[\mi{L}^{2}]{V_{\mi{0}}} = \te[\mi{L}^{2}]{\mu_{\mi{00}}} - \te[\mi{L}]{\mu_{\mi{0}}}^2 = \te[\mi{L}^2]{0}
    \end{equation}
    Defining
    \begin{equation}\label{def:SPEst:e}
        \te[\mi{L\x 3}]{\rv{e}} \deq \rv{y} - \mu_{1}
    \end{equation}
    the expected conditional variance in \cref{def:GSI:mean} amounts to
    \begin{equation}\label{eq:SPEst:V}
        \begin{aligned}
            \te[\mi{L}^{2}]{V_{\mi{m}}} 
            &= \evt{\;\ev{\te[\mi{L}]{\rv{e}_{\mi{m}} + \mu_{\mi{m}}}^{2}}{M}}{\mi{m}}
            - \ev{\te[\mi{L}]{\rv{e}_{\mi{0}} + \mu_{\mi{0}}}^{2}}{M} \\
            &= \ev{\te[\mi{L}]{\mu_{\mi{m}}}^{2}}{\mi{m}} - \te[\mi{L}]{\mu_{\mi{0}}}^{2} + 
            \evt{\te[\mi{L}^2]{\mu_{\mi{mm}}}}{\mi{m}} - \te[\mi{L}^2]{\mu_{\mi{00}}} \\
            &= \ev{\te[\mi{L}]{\mu_{\mi{m}}}^{2}}{\mi{m}}
        \end{aligned}
    \end{equation}
    and the covariance between conditional variances in \cref{def:GSI:W} is
    \begin{equation*}
        \begin{aligned}
            \te[\mi{L}^4]{W_{\mi{mm^{\prime}}}} &\deq \cov[\cov{\rv{y}_{\mi{m^{\prime}}}}{\mi{m^{\prime}}}]{\cov{\rv{y}_{\mi{m}}}{\mi{m}}}{M} \\
            &\phantom{:}=
            \cov[\ev{\te[\mi{L}]{\rv{y}_{\mi{m^{\prime}}}}^{2} - \te[\mi{L}]{\rv{y}_{\mi{0}}}^{2}}{\mi{m^{\prime}}}]{\ev{\te[\mi{L}]{\rv{y}_{\mi{m}}}^{2} - \te[\mi{L}]{\rv{y}_{\mi{0}}}^{2}}{\mi{m}}}{M} \\
            &\phantom{:}=
            \ev{\ev{\te[\mi{L}]{\rv{y}_{\mi{m}}}^{2} - \te[\mi{L}]{\rv{y}_{\mi{0}}}^{2}}{\mi{m}} \otimes\ev{\te[\mi{L}]{\rv{y}_{\mi{m^{\prime}}}}^{2} - \te[\mi{L}]{\rv{y}_{\mi{0}}}^{2}}{\mi{m^{\prime}}}}{M}\\
            &\phantom{\deq}\  - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}} \\       
            &\phantom{:}= \te[\mi{L}^4]{A_{\mi{mm^{\prime}}}-A_{\mi{0m^{\prime}}}-A_{\mi{m0}}+A_{\mi{00}}}
        \end{aligned}
    \end{equation*}
    Here, the inputs within any $\mi{m},\mi{m^{\prime}}\subseteq\mi{M}$ clearly vary independently, and
    \begin{equation*}
        \begin{aligned}
            \te[\mi{L}^4]{A_{\mi{mm^{\prime}}}}
            &\deq \evt{\;\evt{\;\ev{\te[\mi{L}]{\rv{y}_{\mi{m}}}^{2} \otimes \te[\mi{L}]{\rv{y}_{\mi{m^{\prime}}}}^{2}}{\mi{m}}}{\mi{m^{\prime}}}}{M} - \te[\mi{L}^2]{V_{\mi{m}}}\otimes \te[\mi{L}^2]{V_{\mi{m^{\prime}}}} \\
            &\phantom{:}= \evt{\;\evt{\;\ev{
                \te[\mi{L}]{\rv{e}_{\mi{m}}+\mu_{\mi{m}}}^{2} \otimes \te[\mi{L}]{\rv{e}_{\mi{m^{\prime}}}+ \mu_{\mi{m^{\prime}}}}^{2} - 
                \te[\mi{L}]{\mu_{\mi{m}}}^{2} \otimes \te[\mi{L}]{\mu_{\mi{m^{\prime}}}}^{2}
            }{M}}{\mi{m^{\prime}}}}{\mi{m}}
        \end{aligned}
    \end{equation*}
    exploiting the fact that $V_{\mi{0}} = \te[\mi{L}^2]{0}$. \Cref{eq:SPEst:reduction} cancels all terms beginning with $\te[\mi{L}]{\rv{e}_{\mi{m}}}^{2}$, first across $A_{\mi{mm^{\prime}}}-A_{\mi{0m^{\prime}}}$ then across $A_{\mi{m0}}-A_{\mi{00}}$. All remaining terms ending in $\te[\mi{L}]{\mu_{\mi{m^{\prime}}}}^{2}$ are eliminated by centralization $\evt{\,\tte[]{\rv{e}_{\mi{m}}}}{M} = 0$.
    Similar arguments eliminate $\te[\mi{L}]{\rv{e}_{\mi{m^{\prime}}}}^{2}$ and $\te[\mi{L}]{\mu_{\mi{m}}}^{2}$.
    Effectively then
    \begin{equation*}
        \te[\mi{L}^4]{A_{\mi{mm^{\prime}}}} = \sum_{\pi(\mi{L}^{2})} \sum_{\pi(\mi{L^{\prime}}^{2})}
        \evt{\;\evt{\te[\mi{L}^{2} \x \mi{L^{\prime}}^{2}]{\mu_{\mi{m}} \otimes \mu_{\mi{mm^{\prime}}} \otimes \mu_{\mi{m^{\prime}}}}}{\mi{m^{\prime}}}}{\mi{m}}
    \end{equation*}
    so \cref{eq:SPEst:mu0} entails
    \begin{equation}\label{eq:SPEst:W}
        \te[\mi{L}^4]{W_{\mi{mm^{\prime}}}} = \sum_{\pi(\mi{L}^{2})} \sum_{\pi(\mi{L^{\prime}}^{2})}
        \evt{\;\evt{\te[\mi{L}^{2} \x \mi{L^{\prime}}^{2}]{\mu_{\mi{m}} \otimes \mu_{\mi{mm^{\prime}}} \otimes \mu_{\mi{m^{\prime}}}}}{\mi{m^{\prime}}}}{\mi{m}}
    \end{equation}
    where each summation is over permutations of tensor axes
    \begin{equation*}
        \pi(\mi{L}^{2}) \deq \set{(\mi{L}\x\mi{L^{\prime\prime}}), (\mi{L^{\prime\prime}}\x\mi{L})} \QT{;} \pi(\mi{L^{\prime}}^{2}) \deq \set{(\mi{L^{\prime}}\x\mi{L^{\prime\prime\prime}}), (\mi{L^{\prime\prime\prime}}\x\mi{L^{\prime}})}
    \end{equation*}
    Primes on constants are for bookeeping purposes only ($\mi{L}^{j\prime} = \mi{L}$ always), they do not change the value of the constant -- unlike primes on variables ($\mi{m}^{j\prime}$ need not equal $\mi{m}$ in general). One is mainly interested in variances (errors), constituted by the diagonal $\mi{L^{\prime}}^{2}=\mi{L}^{2}$, for which the summation in \cref{eq:SPEst:W} is over a pair of transposed pairs.

    In order to further elucidate these estimates, we must fill in the details of the underlying SPs, sufficiently identifying the regression $\rv{y}$ by its first two moments $\mu_{1}, \mu_{2}$. Then the Sobol' indices are given by \cref{def:GSI:mean,eq:SPEst:V}, and their standard deviation by \cref{def:GSI:variance,def:GSI:Q,eq:SPEst:W}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{siamplain}
\bibliography{../main}

\end{document}
